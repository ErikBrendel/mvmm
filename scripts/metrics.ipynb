{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parsing.ipynb\n",
    "%run util.ipynb\n",
    "%run LocalRepo.ipynb\n",
    "%run structural_parsing.ipynb\n",
    "%run linguistic_parsing.ipynb\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from typing import List\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import regex  # the cooler \"re\"\n",
    "from nltk import WordNetLemmatizer, FreqDist\n",
    "import string\n",
    "from random import shuffle\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import similarities\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from multiprocessing import Pool, TimeoutError, Process, Manager, Lock\n",
    "from functools import partial\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_SAVE_PATH = \"../metrics/\"\n",
    "EXPORT_SAVE_PATH = \"../export/\"\n",
    "\n",
    "# https://networkx.github.io/documentation/latest/tutorial.html#edges\n",
    "class WeightGraph:\n",
    "    def __init__(self, view_name):\n",
    "        self.g = nx.Graph()\n",
    "        self.view_name = view_name\n",
    "        \n",
    "    def add(self, a, b, delta):\n",
    "        #self.g.add_node(a)\n",
    "        #self.g.add_node(b)\n",
    "        if a == b:\n",
    "            return\n",
    "        new_value = self.get(a, b) + delta\n",
    "        self.g.add_edge(a, b, weight=new_value)\n",
    "        \n",
    "    def get(self, a, b):\n",
    "        if a in self.g and b in self.g.adj[a]:\n",
    "            return self.g.adj[a][b][\"weight\"]\n",
    "        return 0\n",
    "    \n",
    "    def add_support(self, node, delta):\n",
    "        if not node in self.g.nodes:\n",
    "            self.g.add_node(node)\n",
    "        self.g.nodes[node][\"support\"] = self.get_support(node) + delta\n",
    "        \n",
    "    def get_support(self, node):\n",
    "        return self.g.nodes.get(node, {}).get(\"support\", 0)\n",
    "    \n",
    "    def add_and_support(self, a, b, delta):\n",
    "        self.add(a, b, delta)\n",
    "        self.add_support(a, delta)\n",
    "        self.add_support(b, delta)\n",
    "    \n",
    "    def cutoff_edges(self, minimum_weight):\n",
    "        fedges = [(a, b) for a, b, info in self.g.edges.data() if info[\"weight\"] < minimum_weight]\n",
    "        self.g.remove_edges_from(fedges)\n",
    "        \n",
    "    def cleanup(self):\n",
    "        # self.g.remove_nodes_from(list(nx.isolates(self.g)))\n",
    "        for component in list(nx.connected_components(self.g)):\n",
    "            if len(component) < 5:\n",
    "                for node in component:\n",
    "                    self.g.remove_node(node)\n",
    "    \n",
    "    def propagate_down(self, layers = 1, weight_factor = 0.2):\n",
    "        \"\"\"copy the connections of each node (scaled by weight_factor) to its children\"\"\"\n",
    "        children_dict = self._get_children_dict()\n",
    "        child_having_nodes = list(children_dict.keys())\n",
    "        child_having_nodes.sort(key=lambda path: -path.count('/'))\n",
    "        for iteration in range(layers):\n",
    "            changes_to_apply = []\n",
    "            for node in log_progress(child_having_nodes, desc=\"Propagating down coupling information, iteration \" + str(iteration + 1) + \"/\" + str(layers)):\n",
    "                connections_and_weights = [(conn, self.get(node, conn) * weight_factor) for conn in self.g[node] if not conn.startswith(node + \"/\")]\n",
    "                for child in children_dict[node]:\n",
    "                    for conn, val in connections_and_weights:\n",
    "                        for conn_child in children_dict.get(conn, []):\n",
    "                            changes_to_apply.append((child, conn_child, val))\n",
    "            for a, b, delta in log_progress(changes_to_apply, desc=\"Applying changes, iteration \" + str(iteration + 1) + \"/\" + str(layers)):\n",
    "                self.add(a, b, delta)\n",
    "                \n",
    "    def dilate(self, iterations = 1, weight_factor = 0.2):\n",
    "        all_nodes = list(self.g.nodes)\n",
    "        for iteration in range(iterations):\n",
    "            changes_to_apply = []\n",
    "            for node in log_progress(all_nodes, desc=\"Dilating coupling information, iteration \" + str(iteration + 1) + \"/\" + str(iterations)):\n",
    "                connections_and_weights = [(conn, self.get(node, conn) * weight_factor) for conn in self.g[node] if not conn.startswith(node + \"/\")]\n",
    "                for (c1, w1), (c2, w2) in all_pairs(connections_and_weights):\n",
    "                    changes_to_apply.append((c1, c2, min(w1, w2)))\n",
    "            for a, b, delta in log_progress(changes_to_apply, desc=\"Applying changes, iteration \" + str(iteration + 1) + \"/\" + str(iterations)):\n",
    "                self.add(a, b, delta)\n",
    "    \n",
    "    def _get_children_dict(self):\n",
    "        result = {}\n",
    "        all_nodes = list(self.g.nodes)\n",
    "        for node in all_nodes:\n",
    "            result[node] = set()\n",
    "        for node in all_nodes:\n",
    "            if \"/\" in node:\n",
    "                parent = \"/\".join(node.split(\"/\")[0:-1])\n",
    "                if parent in result:\n",
    "                    result[parent].add(node)\n",
    "        for node in all_nodes:\n",
    "            if len(result[node]) == 0:\n",
    "                del result[node]\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    def save(self, repo_name):\n",
    "        os.makedirs(METRICS_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        nx.write_gpickle(self.g, WeightGraph.pickle_path(repo_name, self.view_name))\n",
    "        \n",
    "    def get_max_weight(self):\n",
    "        return max([self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges])\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(repo_name, name):\n",
    "        wg = WeightGraph(name)\n",
    "        wg.g = nx.read_gpickle(WeightGraph.pickle_path(repo_name, name))\n",
    "        return wg\n",
    "        \n",
    "    @staticmethod\n",
    "    def pickle_path(repo_name, name):\n",
    "        # see https://networkx.github.io/documentation/stable/reference/readwrite/gpickle.html\n",
    "        return METRICS_SAVE_PATH + repo_name + \"/\" + name + \".gpickle\"\n",
    "    \n",
    "    def json_save(self, repo_name):\n",
    "        data = json_graph.node_link_data(self.g)\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + self.view_name + \".json\", 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            \n",
    "    def html_save(self, repo_name):\n",
    "        data = json.dumps(json_graph.node_link_data(self.g))\n",
    "        content = '<html><body><script type=\"text/javascript\">const graph = ' + data + ';</script><script src=\"/files/metrics/html_app.js?_xsrf=2%7Ce163cb61%7Cb9245804a283415ecb4c641f0cf1f882%7C1601372106\"></script></body></html>'\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + self.view_name + \".html\", 'w') as outfile:\n",
    "            outfile.write(content)\n",
    "    \n",
    "    def plaintext_save(self, repo_name):\n",
    "        node_list = list(self.g.nodes)\n",
    "        node2index = dict(zip(node_list, range(len(node_list))))\n",
    "        content = \";\".join(node_list) + \"\\n\" + \";\".join([str(node2index[a]) + \",\" + str(node2index[b]) + \",\" + str(d[\"weight\"]) for a, b, d in self.g.edges(data=True)])\n",
    "        os.makedirs(EXPORT_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        with open(EXPORT_SAVE_PATH + repo_name + \"/\" + self.view_name + \".graph.txt\", \"w\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    \n",
    "    def print_most_linked_nodes(self, amount = 10):\n",
    "        print(\"Most linked nodes:\")\n",
    "        debug_list = sorted(list(self.g.edges.data()), key = lambda e: -e[2][\"weight\"])\n",
    "        for a, b, info in debug_list[0:amount]:\n",
    "            print(str(info[\"weight\"]) + \": \" + a + \" <> \" + b)\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        # https://networkx.github.io/documentation/latest/tutorial.html#analyzing-graphs\n",
    "        node_count = len(self.g.nodes)\n",
    "        edge_count = len(self.g.edges)\n",
    "        cc = sorted(list(nx.connected_components(self.g)), key= lambda e: -len(e))\n",
    "        print(\"WeightGraph statistics: \"\n",
    "              + str(node_count) + \" nodes, \"\n",
    "              + str(edge_count) + \" edges, \"\n",
    "              + str(len(cc)) + \" connected component(s), with sizes: [\"\n",
    "              + \", \".join([str(len(c)) for c in cc[0:20]])\n",
    "              + \"]\")\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        edge_weights.sort()\n",
    "        node_supports = [self.get_support(n) for n in self.g.nodes]\n",
    "        node_supports.sort()\n",
    "        print(\"Edge weights:\", edge_weights[0:5], \"...\", edge_weights[-5:], \", mean:\", np.array(edge_weights).mean())\n",
    "        print(\"Node support values:\", node_supports[0:5], \"...\", node_supports[-5:], \", mean:\", np.array(node_supports).mean())\n",
    "        \n",
    "    \n",
    "    def show_weight_histogram(self):\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        show_histogram(edge_weights, 'Histogram of edge weights in coupling graph', 'Coupling Strength', 'Amount', 'b')\n",
    "        \n",
    "        node_weights = [sum([self.g[n][n2][\"weight\"] for n2 in self.g.adj[n]]) for n in self.g.nodes]\n",
    "        show_histogram(node_weights, 'Histogram of node weights', 'Coupling Strength', 'Amount', 'g')\n",
    "        \n",
    "        node_supports = [self.get_support(n) for n in self.g.nodes]\n",
    "        show_histogram(node_supports, 'Histogram of node support values', 'Support', 'Amount', 'g')\n",
    "        \n",
    "    def visualize(self, use_spring = False, with_labels = True):\n",
    "        # https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
    "        for e in self.g.edges:\n",
    "            self.g[e[0]][e[1]][\"distance\"] = 1.000001 - self.g[e[0]][e[1]][\"weight\"]  # the value must not be exactly zero\n",
    "        \n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        max_weight = max(edge_weights)\n",
    "        mean_weight = np.array(edge_weights).mean()\n",
    "        target_max_weight = min(max_weight, mean_weight * 2)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        VIZ_POW = 10\n",
    "        max_w_fact = (1. / target_max_weight) ** VIZ_POW\n",
    "        \n",
    "        layout = nx.drawing.layout.kamada_kawai_layout(self.g, weight=\"distance\") if use_spring else None\n",
    "        \n",
    "        # nx.draw_kamada_kawai(self.g, alpha=0.2, node_size=100)\n",
    "        # nx.draw(self.g, alpha=0.2, node_size=100)\n",
    "        edge_colors = [(0., 0., 0., min(1., (self.g[a][b][\"weight\"] ** VIZ_POW) * max_w_fact)) for a, b in self.g.edges]\n",
    "        nx.draw(self.g, pos=layout, node_size=50, edge_color=edge_colors, node_color=[(0.121, 0.469, 0.703, 0.2)], with_labels=with_labels)\n",
    "        \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COMMIT_FILES = 50\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# needs to be separate so that multiprocessing lib can find it\n",
    "def get_commit_diff(commit_hash, repo):\n",
    "    # repo_tree = repo.get_tree()\n",
    "    \n",
    "    def walk_tree_cursor(cursor, prefix, content_bytes, node_handler):\n",
    "        if not cursor.node.is_named:\n",
    "            return\n",
    "        def node_text(node):\n",
    "            return decode(content_bytes[node.start_byte:node.end_byte])\n",
    "            \n",
    "        # cursor.current_field_name() is the role that this node has in its parent\n",
    "        tree_node_names = []  # TODO keep in sync with structural and linguistic view as well as RepoFile class\n",
    "        if cursor.node.type == \"class_declaration\" or cursor.node.type == \"interface_declaration\" or cursor.node.type == \"enum_declaration\":\n",
    "            tree_node_names.append(node_text(cursor.node.child_by_field_name(\"name\")))\n",
    "        elif cursor.node.type == \"field_declaration\":\n",
    "            declarators = [child for child in cursor.node.children if child.type == \"variable_declarator\"]\n",
    "            tree_node_names += [node_text(d.child_by_field_name(\"name\")) for d in declarators]\n",
    "        elif cursor.node.type == \"method_declaration\":\n",
    "            tree_node_names.append(node_text(cursor.node.child_by_field_name(\"name\")))\n",
    "        elif cursor.node.type == \"constructor_declaration\":\n",
    "            tree_node_names.append(\"constructor\")\n",
    "\n",
    "        for tree_node_name in tree_node_names:\n",
    "            node_handler(prefix + \"/\" + tree_node_name, cursor.node)\n",
    "        if len(tree_node_names) > 0:\n",
    "            prefix = prefix + \"/\" + tree_node_names[0]\n",
    "\n",
    "        if cursor.goto_first_child():\n",
    "            walk_tree_cursor(cursor, prefix, content_bytes, node_handler)\n",
    "            while cursor.goto_next_sibling():\n",
    "                walk_tree_cursor(cursor, prefix, content_bytes, node_handler)\n",
    "            cursor.goto_parent()\n",
    "    \n",
    "    def walk_tree(tree, content_bytes, base_path) -> RepoTree:\n",
    "        \"\"\" node_handler gets the current logic-path and node for each ast node\"\"\"\n",
    "        try:\n",
    "            found_nodes = RepoTree(None, \"\")\n",
    "            def handle(logic_path, ts_node):\n",
    "                found_nodes.register(logic_path, ts_node)\n",
    "            walk_tree_cursor(tree.walk(), base_path, content_bytes, handle)\n",
    "            return found_nodes\n",
    "        except Exception as e:\n",
    "            print(\"Failed to parse file:\", base_path, \"Error:\", e)\n",
    "            pdb.set_trace()\n",
    "            return None\n",
    "    \n",
    "    error_query = JA_LANGUAGE.query(\"(ERROR) @err\")\n",
    "    def _has_error(tree) -> List[str]:\n",
    "        errors = error_query.captures(tree.root_node)\n",
    "        return len(errors) > 1\n",
    "    \n",
    "    def blob_diff(diff) -> List[str]:\n",
    "        # pdb.set_trace()\n",
    "        if diff.a_blob is None:\n",
    "            return [diff.b_path] # newly created\n",
    "        elif diff.b_blob is None:\n",
    "            return [diff.a_path] # deleted\n",
    "        path = diff.a_path\n",
    "        # if not repo_tree.has_node(path):\n",
    "        #     return []  # ignore changed files that are not part of the interesting project structure\n",
    "        if not path.endswith(\".\" + repo.type_extension()):\n",
    "            return [path]\n",
    "        a_content = diff.a_blob.data_stream.read()\n",
    "        if should_skip_file(a_content):\n",
    "            return []\n",
    "        b_content = diff.b_blob.data_stream.read()\n",
    "        if should_skip_file(b_content):\n",
    "            return []\n",
    "        a_tree = java_parser.parse(a_content)\n",
    "        b_tree = java_parser.parse(b_content)\n",
    "        if _has_error(a_tree) or _has_error(b_tree):\n",
    "            return [path] # I guess just the file changed, no more details available\n",
    "        a_repo_tree = walk_tree(a_tree, a_content, path)\n",
    "        if a_repo_tree is None:\n",
    "            return [path]\n",
    "        b_repo_tree = walk_tree(b_tree, b_content, path)\n",
    "        if b_repo_tree is None:\n",
    "            return [path]\n",
    "        return a_repo_tree.calculate_diff_to(b_repo_tree, a_content, b_content)\n",
    "    \n",
    "    c1 = repo.get_commit(commit_hash)\n",
    "    if len(c1.parents) == 1:\n",
    "        c2 = c1.parents[0]\n",
    "        # t4 = timer()\n",
    "        diff = c1.diff(c2)\n",
    "        # t5 = timer()\n",
    "        if len(diff) > MAX_COMMIT_FILES or len(diff) <= 1:  # this is duplicated here for performance\n",
    "            return None\n",
    "        diffs = [result for d in diff for result in blob_diff(d)]  #  if repo_tree.has_node(result)\n",
    "        # t6 = timer()\n",
    "        # print(\"Diff: \" + str(len(diff)) + \" / \" + str(len(diffs)) + \" changes\")\n",
    "        \n",
    "        # print(\"Time taken (ms):\", round((t5-t4)*1000), \"(getting git diff)\", round((t6-t5)*1000), \"(parsing sub-file diffs)\", round((t6-t4)*1000), \"(total)\")\n",
    "    elif len(c1.parents) == 2:\n",
    "        return None  # TODO how to do sub-file diffs for merge commits?\n",
    "        #c2 = c1.parents[0]\n",
    "        #diff_1 = c1.diff(c2)\n",
    "        #c3 = c1.parents[1]\n",
    "        #diff_2 = c1.diff(c3)\n",
    "\n",
    "        #diffs_1 = [ d.a_path for d in diff_1 ]\n",
    "        #diffs_2 = [ d.a_path for d in diff_2 ]\n",
    "        #diffs = list(set(diffs_1).intersection(set(diffs_2)))\n",
    "    else:\n",
    "        return None\n",
    "    if len(diffs) > MAX_COMMIT_FILES or len(diffs) <= 1:\n",
    "        return None\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsGeneration:\n",
    "    # ascii art: http://patorjk.com/software/taag/#p=display&f=Soft&t=STRUCTURAL%0A.%0ALINGUISTIC%0A.%0AEVOLUTIONARY%0A.%0ADYNAMIC\n",
    "    def __init__(self, repo):\n",
    "        self.repo = repo\n",
    "        \n",
    "    def calculate_evolutionary_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",------.,--.   ,--.,-----. ,--.   ,--. ,--.,--------.,--. ,-----. ,--.  ,--.  ,---.  ,------.,--.   ,--. \n",
    "|  .---' \\  `.'  /'  .-.  '|  |   |  | |  |'--.  .--'|  |'  .-.  '|  ,'.|  | /  O  \\ |  .--. '\\  `.'  /  \n",
    "|  `--,   \\     / |  | |  ||  |   |  | |  |   |  |   |  ||  | |  ||  |' '  ||  .-.  ||  '--'.' '.    /   \n",
    "|  `---.   \\   /  '  '-'  '|  '--.'  '-'  '   |  |   |  |'  '-'  '|  | `   ||  | |  ||  |\\  \\    |  |    \n",
    "`------'    `-'    `-----' `-----' `-----'    `--'   `--' `-----' `--'  `--'`--' `--'`--' '--'   `--'    \n",
    "        \"\"\"\n",
    "        # MAX_COMMIT_FILES = 50  # Ignore too large commits. (constant moved)\n",
    "        \n",
    "        coupling_graph = WeightGraph(\"evolutionary\")\n",
    "        \n",
    "        def processDiffs(diffs):\n",
    "            score = 2 / len(diffs)\n",
    "            diffs = [d for d in diffs if self.repo.get_tree().has_node(d)]\n",
    "            for f1, f2 in all_pairs(diffs):\n",
    "                coupling_graph.add(f1, f2, score)\n",
    "            for node in diffs:\n",
    "                coupling_graph.add_support(node, 1)\n",
    "        \n",
    "        print(\"Discovering commits...\")\n",
    "        all_commits = list(self.repo.get_all_commits())\n",
    "        # shuffle(all_commits)\n",
    "        print(\"Done!\")\n",
    "        r.get_tree()\n",
    "        print(\"Commits to analyze: \" + str(len(all_commits)))\n",
    "        \n",
    "        map_parallel(\n",
    "            all_commits,\n",
    "            partial(get_commit_diff, repo=self.repo),\n",
    "            processDiffs,\n",
    "            \"Analyzing commits\",\n",
    "            force_non_parallel=False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        coupling_graph.cutoff_edges(0.005)\n",
    "        return coupling_graph\n",
    "    \n",
    "    def post_evolutionary(self, coupling_graph: WeightGraph):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def calculate_structural_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    " ,---. ,--------.,------. ,--. ,--. ,-----.,--------.,--. ,--.,------.   ,---.  ,--.                     \n",
    "'   .-''--.  .--'|  .--. '|  | |  |'  .--./'--.  .--'|  | |  ||  .--. ' /  O  \\ |  |                     \n",
    "`.  `-.   |  |   |  '--'.'|  | |  ||  |       |  |   |  | |  ||  '--'.'|  .-.  ||  |                     \n",
    ".-'    |  |  |   |  |\\  \\ '  '-'  ''  '--'\\   |  |   '  '-'  '|  |\\  \\ |  | |  ||  '--.                  \n",
    "`-----'   `--'   `--' '--' `-----'  `-----'   `--'    `-----' `--' '--'`--' `--'`-----'   \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        coupling_graph = WeightGraph(\"structural\")\n",
    "\n",
    "        context = StructuralContext(self.repo)\n",
    "        context.couple_files_by_import(coupling_graph)\n",
    "        context.couple_by_ineritance(coupling_graph)\n",
    "        context.couple_members_by_content(coupling_graph)\n",
    "        flush_unresolvable_vars()\n",
    "\n",
    "        return coupling_graph\n",
    "    \n",
    "    def post_structural(self, coupling_graph: WeightGraph):\n",
    "        coupling_graph.propagate_down(2, 0.5)\n",
    "        # coupling_graph.dilate(1, 0.2)\n",
    "    \n",
    "    \n",
    "    def calculate_linguistic_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",--.   ,--.,--.  ,--. ,----.   ,--. ,--.,--. ,---. ,--------.,--. ,-----.                                \n",
    "|  |   |  ||  ,'.|  |'  .-./   |  | |  ||  |'   .-''--.  .--'|  |'  .--./                                \n",
    "|  |   |  ||  |' '  ||  | .---.|  | |  ||  |`.  `-.   |  |   |  ||  |                                    \n",
    "|  '--.|  ||  | `   |'  '--'  |'  '-'  '|  |.-'    |  |  |   |  |'  '--'\\                                \n",
    "`-----'`--'`--'  `--' `------'  `-----' `--'`-----'   `--'   `--' `-----'              \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        coupling_graph = WeightGraph(\"linguistic\")\n",
    "        \n",
    "        node_words = extract_topic_model_documents(self.repo.get_all_interesting_files())\n",
    "        topics = train_topic_model(node_words)\n",
    "        couple_by_topic_similarity(node_words, topics, coupling_graph)\n",
    "        \n",
    "        return coupling_graph\n",
    "    \n",
    "    def post_linguistic(self, coupling_graph: WeightGraph):\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricManager:\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear(repo, name):\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            os.remove(WeightGraph.pickle_path(repo.name, name))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get(repo, name) -> WeightGraph:\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            print(\"Using precalculated \" + name + \" values\")\n",
    "            graph = WeightGraph.load(repo.name, name)\n",
    "            getattr(MetricsGeneration(repo), \"post_\" + name)(graph)\n",
    "            return graph\n",
    "        print(\"No precalculated \" + name + \" values found, starting calculations...\")\n",
    "        graph = getattr(MetricsGeneration(repo), \"calculate_\" + name + \"_connections\")()\n",
    "        graph.cleanup()\n",
    "        print(\"Calculated \" + name + \" values, saving them now...\")\n",
    "        graph.save(repo.name)\n",
    "        getattr(MetricsGeneration(repo), \"post_\" + name)(graph)\n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def _data_present(repo_name, name):\n",
    "        return os.path.isfile(WeightGraph.pickle_path(repo_name, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
