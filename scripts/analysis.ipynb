{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "%matplotlib inline\n",
    "%run LocalRepo.ipynb\n",
    "%run repos.ipynb\n",
    "%run parsing.ipynb\n",
    "%run metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisGraph:\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        self.children = {}\n",
    "        for n in self.g.g.nodes:\n",
    "            while n is not None:\n",
    "                p = self.get_parent(n)\n",
    "                self.children.setdefault(p, set()).add(n)\n",
    "                n = p\n",
    "        self.total_relative_coupling_cache = {}\n",
    "        self.median_maximum_support_cache = None\n",
    "    \n",
    "    def propagate_down(self, layers = 1, weight_factor = 0.2):\n",
    "        self.g.propagate_down(layers, weight_factor)\n",
    "        \n",
    "    def get_node_set(self):\n",
    "        return set(self.g.g.nodes)\n",
    "        \n",
    "    def get_children(self, node):\n",
    "        if node in self.children:\n",
    "            return self.children[node]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    def get_parent(self, node):\n",
    "        if len(node) <= 1:\n",
    "            return None\n",
    "        return \"/\".join(node.split(\"/\")[:-1])\n",
    "    \n",
    "    def get_directly_coupled(self, node):\n",
    "        if node in self.g.g:\n",
    "            return [n for n in self.g.g[node]]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def get_siblings(self, node):\n",
    "        \"\"\"Return all the other children of this.parent (excluding this)\"\"\"\n",
    "        parent = self.get_parent(node)\n",
    "        if parent is None:\n",
    "            return []\n",
    "        return [c for c in self.get_children(parent) if c != node]\n",
    "    \n",
    "    def get_coupling_candidates(self, node, add_predecessors = False):\n",
    "        \"\"\"Return all the nodes with which the given one could have a non-zero relative coupling.\n",
    "         * := The direct coupling nodes of given+descendants, and all their predecessors\"\"\"\n",
    "        this_and_descendants = self.get_self_and_descendants(node)\n",
    "        \n",
    "        direct_coupling_candidates = []\n",
    "        for n in this_and_descendants:\n",
    "            for n2 in self.get_directly_coupled(n):\n",
    "                direct_coupling_candidates.append(n2)\n",
    "        \n",
    "        result = set()\n",
    "        result.add(\"\")  # root node, added to stop the predecessor iteration\n",
    "        for other in direct_coupling_candidates:\n",
    "            while result.add(other): # while not present yet\n",
    "                if not add_predecessors:\n",
    "                    break\n",
    "                other = self.get_parent(other)\n",
    "        result.remove(\"\")  # root node - removed, since not very interesting\n",
    "        return result\n",
    "    \n",
    "        \n",
    "    def get_self_and_descendants(self, node):\n",
    "        \"\"\"return the given node and all its descendants as a list\"\"\"\n",
    "        result = []\n",
    "        self.get_self_and_descendants_rec(node, result)\n",
    "        return result\n",
    "    \n",
    "    def get_self_and_descendants_rec(self, node, result):\n",
    "        \"\"\"add the given node and all its descendants into the provided list\"\"\"\n",
    "        result.append(node)\n",
    "        for child in self.get_children(node):\n",
    "            self.get_self_and_descendants_rec(child, result)\n",
    "            \n",
    "    \n",
    "    def get_direct_coupling(self, a, b):\n",
    "        \"\"\"direct coupling between a and b\"\"\"\n",
    "        return self.g.get(a, b)\n",
    "    \n",
    "    def get_direct_multi_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a and all b\"\"\"\n",
    "        return sum([self.get_direct_coupling(a, b) for b in others])\n",
    "    \n",
    "    def get_relative_coupling(self, a, b):\n",
    "        \"\"\"sum of direct coupling between a+descendants and b+descendants\"\"\"\n",
    "        others = self.get_self_and_descendants(b)\n",
    "        return self.get_relative_multi_direct_coupling(a, others)\n",
    "    \n",
    "    def get_relative_multi_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a+descendants and others+descendants\"\"\"\n",
    "        direct_others = []\n",
    "        for other in others:\n",
    "            self.get_self_and_descendants_rec(other, direct_others)\n",
    "        return self.get_relative_multi_direct_coupling(a, direct_others)\n",
    "    \n",
    "    def get_relative_multi_direct_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a+descendants and others\"\"\"\n",
    "        if len(others) == 0:\n",
    "            return 0\n",
    "        result = self.get_direct_multi_coupling(a, others)\n",
    "        for child in self.get_children(a):\n",
    "            result += self.get_relative_multi_direct_coupling(child, others)\n",
    "        return result\n",
    "    \n",
    "    def get_total_relative_coupling(self, a):\n",
    "        \"\"\"the sum of direct couplings that a has, cached\"\"\"\n",
    "        if a in self.total_relative_coupling_cache:\n",
    "            return self.total_relative_coupling_cache[a]\n",
    "        \n",
    "        a_candidates = self.get_coupling_candidates(a, add_predecessors = False)\n",
    "        total_coupling = self.get_relative_multi_direct_coupling(a, a_candidates)\n",
    "        self.total_relative_coupling_cache[a] = total_coupling\n",
    "        return total_coupling\n",
    "    \n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        \"\"\"relative coupling between a and b, normalized by the sum of couplings that a has, in range [0, 1]\"\"\"\n",
    "        if a not in self.g.g or b not in self.g.g:\n",
    "            return 0\n",
    "        target_coupling = self.get_relative_coupling(a, b)\n",
    "        if target_coupling == 0:\n",
    "            return 0\n",
    "        total_coupling = self.get_total_relative_coupling(a)\n",
    "        return target_coupling / total_coupling\n",
    "    \n",
    "    \n",
    "    def get_normalized_support(self, node):\n",
    "        \"\"\"\n",
    "        on a scale of [0, 1], how much support do we have for coupling values with that node?\n",
    "        This should depend on how much data (including children) we have for this node, relative to how much data is normal in this graph.\n",
    "        It should also be outlier-stable, so that having median-much data maybe results in a support score of 0.5?\n",
    "        \"\"\"\n",
    "        abs_supp = self.get_absolute_support(node)\n",
    "        median, maximum = self.get_absolute_support_median_and_max()\n",
    "        if abs_supp <= median:\n",
    "            return 0.5 * abs_supp / median\n",
    "        else:\n",
    "            return 0.5 + (0.5 * (abs_supp - median) / (maximum - median))\n",
    "    \n",
    "    def get_absolute_support(self, node):\n",
    "        result = self.get_absolute_self_support(node)\n",
    "        for child in self.get_children(node):\n",
    "            result += self.get_absolute_support(child)\n",
    "        return result\n",
    "        \n",
    "    def get_absolute_self_support(self, node):\n",
    "        return self.g.get_support(node)\n",
    "    \n",
    "    def get_absolute_support_median_and_max(self):\n",
    "        if self.median_maximum_support_cache is None:\n",
    "            supports = list([self.get_absolute_support(node) for node in self.g.g.nodes])\n",
    "            mean = np.mean(supports) # TODO mean seems to fit better?\n",
    "            maximum = max(supports)\n",
    "            self.median_maximum_support_cache = (mean, maximum)\n",
    "            #for i in range(10):\n",
    "            #    print(\"Amount of\", i, \"in supports:\", supports.count(i * 1.0))\n",
    "            #show_histogram(supports, \"support values\")\n",
    "        return self.median_maximum_support_cache\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public so other code can use it\n",
    "def path_module_distance(a, b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    steps_a = a.split(\"/\")\n",
    "    steps_b = b.split(\"/\")\n",
    "    min_len = min(len(steps_a), len(steps_b))\n",
    "    for i in range(min_len):\n",
    "        if steps_a[i] == steps_b[i]:\n",
    "            continue\n",
    "        # unequal: calc distance\n",
    "        return len(steps_a) + len(steps_b) - (i * 2)\n",
    "    return len(steps_a) + len(steps_b) - (min_len * 2)\n",
    "\n",
    "\n",
    "class ModuleDistanceAnalysisGraph:\n",
    "    \n",
    "    def get_node_set(self):\n",
    "        return None\n",
    "    \n",
    "    def get_normalized_support(self, node):\n",
    "        return 1\n",
    "    \n",
    "    def propagate_down(self, layers = 1, weight_factor = 0.2):\n",
    "        pass\n",
    "    \n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        dist = path_module_distance(a, b)\n",
    "        base = 1.1  # needs to be bigger than one. Lower values = stronger coupling across bigger distances. Higher values = faster decay of coupling across module distance\n",
    "        return math.pow(base, -dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_graph(r, metric_name):\n",
    "    if metric_name == \"module_distance\":\n",
    "        return ModuleDistanceAnalysisGraph()\n",
    "    else:\n",
    "        return AnalysisGraph(MetricManager.get(r, metric_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(coupling_values, pattern, support_values):\n",
    "    \"\"\"how good does this node-pair fit to the given pattern? Range: [0, 1]\"\"\"\n",
    "    error_sum = 0; values = 0; support = 1\n",
    "    for i, coupling_val in enumerate(coupling_values):\n",
    "        if pattern[i] is not None:\n",
    "            error = abs(pattern[i] - coupling_val)\n",
    "            error_sum += error * error\n",
    "            values += 1\n",
    "            support = min(support, support_values[i])\n",
    "    match_score = 1. - (error_sum / values)\n",
    "    return match_score, support\n",
    "\n",
    "\n",
    "# as seen in:\n",
    "# https://thelaziestprogrammer.com/python/a-multiprocessing-pool-pickle\n",
    "# https://thelaziestprogrammer.com/python/multiprocessing-pool-a-global-solution\n",
    "class StaticStuff:\n",
    "    analysis_graphs = None\n",
    "    target_patterns = None\n",
    "\n",
    "MIN_PATTERN_MATCH = 0  # how close the coupling values need to match the pattern to be a result\n",
    "MIN_SUPPORT = 0.4  # how much relative support a result needs to not be discarded\n",
    "def analyze_pair(pair): #, analysis_graphs, target_patterns):\n",
    "    # pdb.set_trace()\n",
    "    _a, _b = pair\n",
    "    if _a.startswith(_b) or _b.startswith(_a):  # ignore nodes that are in a parent-child relation\n",
    "        return None\n",
    "    # for each view: how much support do we have for this node pair (minimum of both node support values)\n",
    "    support_values = [min(supp_a, supp_b) for supp_a, supp_b in zip(*[\n",
    "        [g.get_normalized_support(node) for g in StaticStuff.analysis_graphs] for node in [_a, _b]\n",
    "    ])]\n",
    "    result = [[] for p in StaticStuff.target_patterns]\n",
    "    for a, b in [(_a, _b), (_b, _a)]:\n",
    "        normalized_coupling_values = tuple([g.get_normalized_coupling(a, b) for g in StaticStuff.analysis_graphs])\n",
    "        for i, pattern in enumerate(StaticStuff.target_patterns):\n",
    "            pattern_match_score_data = tuple(abs(p - v) for p, v in zip(pattern, normalized_coupling_values) if p is not None)\n",
    "            match_score, support = pattern_match(normalized_coupling_values, pattern, support_values)\n",
    "            if match_score >= MIN_PATTERN_MATCH and support >= MIN_SUPPORT:\n",
    "                result[i].append((pattern_match_score_data, (a, b, normalized_coupling_values, support)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestResultsSet:\n",
    "    def __init__(self, dimension_count, result_keep_size):\n",
    "        self.dimension_count = dimension_count\n",
    "        self.result_keep_size = result_keep_size\n",
    "        self.data = []  # pair of ([coordinates per dimension], user-data)\n",
    "        self.total_amount = 0\n",
    "    \n",
    "    def add_all(self, new_data):\n",
    "        self.data += new_data\n",
    "        self.total_amount += len(new_data)\n",
    "        self.trim_maybe()\n",
    "        \n",
    "    def get_best(self, dim_weights):\n",
    "        def sort_key(datum):\n",
    "            return sum(datum[0][i] * weight for i, weight in enumerate(dim_weights))\n",
    "        self.data.sort(key=sort_key)\n",
    "        return self.data[:self.result_keep_size]\n",
    "    \n",
    "    def trim_maybe(self):\n",
    "        if len(self.data) > self.result_keep_size * 100 * self.dimension_count:\n",
    "            self.trim()\n",
    "    \n",
    "    def trim(self):\n",
    "        result_keep_tolerance = 2  # higher = keep more, but better chance at not accidentally removing important stuff\n",
    "        sampling_accuracy = 20  # higher = more runtime, but more acurately detecting required important data\n",
    "        # previous_size = len(self.data)\n",
    "        important_data = set()\n",
    "        prev_result_keep_size = self.result_keep_size\n",
    "        self.result_keep_size *= result_keep_tolerance\n",
    "        for dim_weight in generate_one_distributions(self.dimension_count, sampling_accuracy):\n",
    "            important_data.update(self.get_best(dim_weight))\n",
    "        self.result_keep_size = prev_result_keep_size\n",
    "        self.data = list(important_data)\n",
    "        # print(\"Trimming reduced from\", previous_size, \"to\", len(self.data), \"elements\")\n",
    "    \n",
    "    def export_to_csv(self, name):\n",
    "        \"\"\"only works for 2 dimensions!\"\"\"\n",
    "        with open(name + \".csv.txt\", \"w\") as f:\n",
    "            f.write(\"x,y\\n1,1\\n\" + \"\\n\".join(str(d[1][2][0])+\",\"+str(d[1][2][1]) for d in self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_RESULTS_SIZE = 50\n",
    "def analyze_disagreements(repo, views, target_patterns, node_filter_func = None, node_pair_filter_func = None):\n",
    "    \"\"\"\n",
    "    when views are [struct, evo, ling], the pattern [0, 1, None, \"comment\"] searches for nodes that are\n",
    "    strongly coupled evolutionary, loosely coupled structurally, and the language does not matter\n",
    "    \"\"\"\n",
    "    if len(views) <= 1:\n",
    "        return\n",
    "    if not all([len(p) >= len(views) for p in target_patterns]):\n",
    "        print(\"Patterns need at least one element per graph!\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    analysis_graphs = list([get_analysis_graph(repo, g) for g in views])\n",
    "    #for g in analysis_graphs:\n",
    "    #    g.propagate_down(2, 0.2)\n",
    "    analysis_graph_nodes = [g.get_node_set() for g in analysis_graphs]\n",
    "    all_nodes = list(set.intersection(*[nodes for nodes in analysis_graph_nodes if nodes is not None]))\n",
    "    all_nodes = [n for n in all_nodes if repo.get_tree().has_node(n)]\n",
    "    print(\"Total node count:\", len(all_nodes))\n",
    "    print(\"Methods:\", sum(repo.get_tree().find_node(path).get_type() == \"method\" for path in all_nodes))\n",
    "    print(\"constructors:\", sum(repo.get_tree().find_node(path).get_type() == \"constructor\" for path in all_nodes))\n",
    "    print(\"fields:\", sum(repo.get_tree().find_node(path).get_type() == \"field\" for path in all_nodes))\n",
    "    print(\"classes:\", sum(repo.get_tree().find_node(path).get_type() == \"class\" for path in all_nodes))\n",
    "    print(\"interfaces:\", sum(repo.get_tree().find_node(path).get_type() == \"interface\" for path in all_nodes))\n",
    "    print(\"enums:\", sum(repo.get_tree().find_node(path).get_type() == \"enum\" for path in all_nodes))\n",
    "    print(\"without type:\", sum(repo.get_tree().find_node(path).get_type() is None for path in all_nodes))\n",
    "    if node_filter_func is not None:\n",
    "        all_nodes = [node for node in all_nodes if node_filter_func(node)]\n",
    "    print(\"all filtered nodes:\", len(all_nodes))\n",
    "    \n",
    "    all_node_pairs = list(all_pairs(all_nodes))\n",
    "    if node_pair_filter_func is not None:\n",
    "        prev_len = len(all_node_pairs)\n",
    "        all_node_pairs = [pair for pair in all_node_pairs if node_pair_filter_func(pair[0], pair[1])]\n",
    "        print(\"Amount of node pairs to check:\", len(all_node_pairs), \"of\", prev_len, \"(\" + str(len(all_node_pairs)/prev_len*100) + \"%)\")\n",
    "        \n",
    "        \n",
    "    print(\"Going parallel...\")\n",
    "    pattern_results = [\n",
    "        BestResultsSet(sum(type(x)==int for x in p), SHOW_RESULTS_SIZE)\n",
    "        for p in target_patterns]\n",
    "    def handle_results(pattern_results_part):\n",
    "        for i, part in enumerate(pattern_results_part):\n",
    "            pattern_results[i].add_all(part)\n",
    "    StaticStuff.analysis_graphs = analysis_graphs\n",
    "    StaticStuff.target_patterns = target_patterns\n",
    "    map_parallel(\n",
    "        all_node_pairs,\n",
    "        analyze_pair, # partial(analyze_pair, analysis_graphs=analysis_graphs, target_patterns=target_patterns),\n",
    "        handle_results,\n",
    "        \"Analyzing edges\",\n",
    "        force_non_parallel=True\n",
    "    )\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    for i, (pattern, results) in enumerate(zip(target_patterns, pattern_results)):\n",
    "        print(\"\\nPattern \" + str(i) + \" (\" + str(pattern) + \"):\")\n",
    "            \n",
    "        def nice_path(path):\n",
    "            ending = \".\" + repo.type_extension()\n",
    "            if ending in path:\n",
    "                return path[path.index(ending) + len(ending) + 1:]\n",
    "            return path\n",
    "        \n",
    "        def get_raw_i(i):\n",
    "            def getter(d):\n",
    "                return d[1][2][i]\n",
    "            return getter\n",
    "        def get_i(i):\n",
    "            def getter(d):\n",
    "                return d[0][i]\n",
    "            return getter\n",
    "        name_and_raw_getters = [(name, get_raw_i(i)) for i, name in enumerate(views) if pattern[i] is not None]\n",
    "        sort_val_getters = [get_i(i) for i in range(len([p for p in pattern if p is not None]))]\n",
    "        dimensions = [(name, get, get_raw) for (name, get_raw), get in zip(name_and_raw_getters, sort_val_getters)]\n",
    "        def make_show_data(dim):\n",
    "            def show_data(multi_sorted_results):\n",
    "                print(results.total_amount, \"raw results,\", len(multi_sorted_results), \"final results\")\n",
    "\n",
    "                display_data = multi_sorted_results[:SHOW_RESULTS_SIZE]\n",
    "                #for d in display_data:\n",
    "                #    print(d)\n",
    "                display_data = [\n",
    "                    [\"{:1.4f}\".format(raw_getter(datum)) for name, getter, raw_getter in dim] +\n",
    "                    [\"{:1.4f}\".format(datum[1][3])] +\n",
    "                    ['<a target=\"_blank\" href=\"' + repo.url_for(path) + '\" title=\"' + path + '\">' + nice_path(path) + '</a>' for path in datum[1][0:2]]\n",
    "                    for datum in display_data]\n",
    "                header = [name for name, *_ in dim] + [\"support\", \"method 1\", \"method 2\"]\n",
    "                show_html_table([header] + display_data, len(dim) + 3)\n",
    "            return show_data\n",
    "        #results.export_to_csv(\"full\")\n",
    "        results.trim()\n",
    "        #results.export_to_csv(\"trimmed\")\n",
    "        interactive_multi_sort(results.data, dimensions, make_show_data(dimensions)) \n",
    "        \n",
    "    \n",
    "    \n",
    "    # pdb.set_trace()\n",
    "    # TODO trim node set of nodes to those they have in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    r = LocalRepo(\"ErikBrendel/LudumDare\")\n",
    "    analyze_disagreements([MetricManager.get(r, view) for view in [\"structural\", \"evolutionary\"]], [[0, 1], [None, 1], [1, None]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
