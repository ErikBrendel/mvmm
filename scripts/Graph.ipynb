{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750b9dfc-9af4-461f-a7e9-2d84e3edebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.ipynb\n",
    "from abc import ABC, abstractmethod\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7722a47-872f-4818-8e82-11dabe9a6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_SAVE_PATH = \"../metrics/\"\n",
    "EXPORT_SAVE_PATH = \"../export/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dea9ad-2922-4686-8281-8298d7ccf792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingGraph(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def get_node_set(self):\n",
    "        return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_normalized_support(self, node):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        pass\n",
    "\n",
    "    def save(self, repo_name):\n",
    "        os.makedirs(METRICS_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        with open(CouplingGraph.pickle_path(repo_name, self.name), 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(repo_name, name):\n",
    "        with open(CouplingGraph.pickle_path(repo_name, name), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def pickle_path(repo_name, name):\n",
    "        # see https://networkx.github.io/documentation/stable/reference/readwrite/gpickle.html\n",
    "        return METRICS_SAVE_PATH + repo_name + \"/\" + name + \".gpickle\"\n",
    "    \n",
    "    def plaintext_save(self, repo_name):\n",
    "        content = self.plaintext_content()\n",
    "        os.makedirs(EXPORT_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        with open(EXPORT_SAVE_PATH + repo_name + \"/\" + self.name + \".graph.txt\", \"w\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def plaintext_content(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def print_statistics(self):\n",
    "        pass\n",
    "    \n",
    "    def show_weight_histogram(self):\n",
    "        print(\"No Histogram Data to show for \" + type(self).__name__)\n",
    "        \n",
    "    def visualize(self, use_spring = False, with_labels = True):\n",
    "        print(\"No visualization for \" + type(self).__name__)\n",
    "    \n",
    "    def print_most_linked_nodes(self, amount = 10):\n",
    "        print(\"No Most Linked Nodes Data for \" + type(self).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6e9c4d-7e01-4c64-8a11-06fab3623415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeSetCouplingGraph(CouplingGraph):\n",
    "    def __init__(self, name):\n",
    "        CouplingGraph.__init__(self, name)\n",
    "        self.children = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_node_set(self):\n",
    "        pass\n",
    "    \n",
    "    def create_child_cache(self):\n",
    "        self.children = {}\n",
    "        for n in self.get_node_set():\n",
    "            self.children.setdefault(n, set())\n",
    "            while n is not None:  # TODO no loop needed?\n",
    "                p = self.get_parent(n)\n",
    "                self.children.setdefault(p, set()).add(n)\n",
    "                n = p\n",
    "        \n",
    "    def get_children(self, node):\n",
    "        if self.children is None or node not in self.children:\n",
    "            self.create_child_cache()\n",
    "        return self.children.get(node, [])\n",
    "        \n",
    "    def get_parent(self, node):\n",
    "        if len(node) <= 1:\n",
    "            return None\n",
    "        return \"/\".join(node.split(\"/\")[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a614a9-a500-429b-8ed1-4b77156ae62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalizeSupport(ABC):\n",
    "    def __init__(self):\n",
    "        self.median_maximum_support_cache = None\n",
    "    \n",
    "    \"\"\"@abstractmethod\n",
    "    def get_absolute_support(self, node):\n",
    "        pass\"\"\"\n",
    "\n",
    "    def get_normalized_support(self, node):\n",
    "        \"\"\"\n",
    "        on a scale of [0, 1], how much support do we have for coupling values with that node?\n",
    "        This should depend on how much data (including children) we have for this node, relative to how much data is normal in this graph.\n",
    "        It should also be outlier-stable, so that having median-much data maybe results in a support score of 0.5?\n",
    "        \"\"\"\n",
    "        abs_supp = self.get_absolute_support(node)\n",
    "        median, maximum = self.get_absolute_support_median_and_max()\n",
    "        if abs_supp <= median:\n",
    "            return 0.5 * abs_supp / median\n",
    "        else:\n",
    "            return 0.5 + (0.5 * (abs_supp - median) / (maximum - median))\n",
    "    \n",
    "    def get_absolute_support_median_and_max(self):\n",
    "        if self.median_maximum_support_cache is None:\n",
    "            supports = list([self.get_absolute_support(node) for node in self.get_node_set()])\n",
    "            mean = np.mean(supports) # TODO mean seems to fit better?\n",
    "            maximum = max(supports)\n",
    "            self.median_maximum_support_cache = (mean, maximum)\n",
    "        return self.median_maximum_support_cache\n",
    "\n",
    "class NormalizeSupportWithChildren(NormalizeSupport):\n",
    "    def __init__(self):\n",
    "        NormalizeSupport.__init__(self)\n",
    "        \n",
    "    \"\"\"@abstractmethod\n",
    "    def get_absolute_self_support(self, node):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_children(self, node):\n",
    "        pass\"\"\"\n",
    "    \n",
    "    def get_absolute_support(self, node):\n",
    "        result = self.get_absolute_self_support(node)\n",
    "        for child in self.get_children(node):\n",
    "            result += self.get_absolute_support(child)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52bc3e0e-3d15-4862-85ca-b12ff2e2adf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalizeCouplingWithChildren:\n",
    "    def __init__(self):\n",
    "        self.total_relative_coupling_cache = {}\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def get_children(self, node):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_parent(self, node):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_directly_coupled(self, node):\n",
    "        # list of nodes that are directly coupled with the given one\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_direct_coupling(self, a, b):\n",
    "        pass\"\"\"\n",
    "    \n",
    "    def get_coupling_candidates(self, node, add_predecessors = False):\n",
    "        \"\"\"Return all the nodes with which the given one could have a non-zero relative coupling.\n",
    "         * := The direct coupling nodes of given+descendants, and all their predecessors\"\"\"\n",
    "        this_and_descendants = self.get_self_and_descendants(node)\n",
    "        \n",
    "        direct_coupling_candidates = []\n",
    "        for n in this_and_descendants:\n",
    "            for n2 in self.get_directly_coupled(n):\n",
    "                direct_coupling_candidates.append(n2)\n",
    "        \n",
    "        result = set()\n",
    "        result.add(\"\")  # root node, added to stop the predecessor iteration\n",
    "        for other in direct_coupling_candidates:\n",
    "            while result.add(other): # while not present yet\n",
    "                if not add_predecessors:\n",
    "                    break\n",
    "                other = self.get_parent(other)\n",
    "        result.remove(\"\")  # root node - removed, since not very interesting\n",
    "        return result\n",
    "    \n",
    "    def get_self_and_descendants(self, node):\n",
    "        \"\"\"return the given node and all its descendants as a list\"\"\"\n",
    "        result = []\n",
    "        self.get_self_and_descendants_rec(node, result)\n",
    "        return result\n",
    "    \n",
    "    def get_self_and_descendants_rec(self, node, result):\n",
    "        \"\"\"add the given node and all its descendants into the provided list\"\"\"\n",
    "        result.append(node)\n",
    "        for child in self.get_children(node):\n",
    "            self.get_self_and_descendants_rec(child, result)\n",
    "    \n",
    "    def get_direct_multi_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a and all b\"\"\"\n",
    "        return sum(self.get_direct_coupling(a, b) for b in others)\n",
    "    \n",
    "    def get_relative_coupling(self, a, b):\n",
    "        \"\"\"sum of direct coupling between a+descendants and b+descendants\"\"\"\n",
    "        others = self.get_self_and_descendants(b)\n",
    "        return self.get_relative_multi_direct_coupling(a, others)\n",
    "    \n",
    "    def get_relative_multi_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a+descendants and others+descendants\"\"\"\n",
    "        direct_others = []\n",
    "        for other in others:\n",
    "            self.get_self_and_descendants_rec(other, direct_others)\n",
    "        return self.get_relative_multi_direct_coupling(a, direct_others)\n",
    "    \n",
    "    def get_relative_multi_direct_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a+descendants and others\"\"\"\n",
    "        if len(others) == 0:\n",
    "            return 0\n",
    "        result = self.get_direct_multi_coupling(a, others)\n",
    "        for child in self.get_children(a):\n",
    "            result += self.get_relative_multi_direct_coupling(child, others)\n",
    "        return result\n",
    "    \n",
    "    def get_total_relative_coupling(self, a):\n",
    "        \"\"\"the sum of direct couplings that a has, cached\"\"\"\n",
    "        if a in self.total_relative_coupling_cache:\n",
    "            return self.total_relative_coupling_cache[a]\n",
    "        \n",
    "        a_candidates = self.get_coupling_candidates(a, add_predecessors = False)\n",
    "        total_coupling = self.get_relative_multi_direct_coupling(a, a_candidates)\n",
    "        self.total_relative_coupling_cache[a] = total_coupling\n",
    "        return total_coupling\n",
    "    \n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        \"\"\"relative coupling between a and b, normalized by the sum of couplings that a has, in range [0, 1]\"\"\"\n",
    "        if a not in self.g or b not in self.g:\n",
    "            return 0\n",
    "        target_coupling = self.get_relative_coupling(a, b)\n",
    "        if target_coupling == 0:\n",
    "            return 0\n",
    "        total_coupling = self.get_total_relative_coupling(a)\n",
    "        return target_coupling / total_coupling\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7c907c-a043-40be-9595-17dda4fa43c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExplicitCouplingGraph(NormalizeCouplingWithChildren, NormalizeSupportWithChildren, NodeSetCouplingGraph):\n",
    "    def __init__(self, name):\n",
    "        NodeSetCouplingGraph.__init__(self, name)\n",
    "        NormalizeCouplingWithChildren.__init__(self)\n",
    "        NormalizeSupportWithChildren.__init__(self)\n",
    "        self.g = nx.Graph()\n",
    "        \n",
    "    def get_node_set(self):\n",
    "        return set(self.g.nodes)\n",
    "        \n",
    "    def add(self, a, b, delta):\n",
    "        if a == b:\n",
    "            return\n",
    "        new_value = self.get(a, b) + delta\n",
    "        self.g.add_edge(a, b, weight=new_value)\n",
    "        \n",
    "    def get(self, a, b):\n",
    "        if a in self.g and b in self.g.adj[a]:\n",
    "            return self.g.adj[a][b][\"weight\"]\n",
    "        return 0\n",
    "    \n",
    "    def get_direct_coupling(self, a, b):\n",
    "        return self.get(a, b)\n",
    "    \n",
    "    def get_directly_coupled(self, node):\n",
    "        if node in self.g:\n",
    "            return [n for n in self.g[node]]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def add_support(self, node, delta):\n",
    "        if not node in self.g.nodes:\n",
    "            self.g.add_node(node)\n",
    "        self.g.nodes[node][\"support\"] = self.get_support(node) + delta\n",
    "        \n",
    "    def get_support(self, node):\n",
    "        return self.g.nodes.get(node, {}).get(\"support\", 0)\n",
    "        \n",
    "    def get_absolute_self_support(self, node):\n",
    "        return self.get_support(node)\n",
    "    \n",
    "    def add_and_support(self, a, b, delta):\n",
    "        self.add(a, b, delta)\n",
    "        self.add_support(a, delta)\n",
    "        self.add_support(b, delta)\n",
    "    \n",
    "    def cutoff_edges(self, minimum_weight):\n",
    "        fedges = [(a, b) for a, b, info in self.g.edges.data() if info[\"weight\"] < minimum_weight]\n",
    "        self.g.remove_edges_from(fedges)\n",
    "        \n",
    "    def cleanup(self, min_component_size):  # min_component_size = 5\n",
    "        # self.g.remove_nodes_from(list(nx.isolates(self.g)))\n",
    "        for component in list(nx.connected_components(self.g)):\n",
    "            if len(component) < min_component_size:\n",
    "                for node in component:\n",
    "                    self.g.remove_node(node)\n",
    "    \n",
    "    def propagate_down(self, layers = 1, weight_factor = 0.2):\n",
    "        \"\"\"copy the connections of each node (scaled by weight_factor) to its children\"\"\"\n",
    "        children_dict = self._get_children_dict()\n",
    "        child_having_nodes = list(children_dict.keys())\n",
    "        child_having_nodes.sort(key=lambda path: -path.count('/'))\n",
    "        for iteration in range(layers):\n",
    "            changes_to_apply = []\n",
    "            for node in log_progress(child_having_nodes, desc=\"Propagating down coupling information, iteration \" + str(iteration + 1) + \"/\" + str(layers)):\n",
    "                connections_and_weights = [(conn, self.get(node, conn) * weight_factor) for conn in self.g[node] if not conn.startswith(node + \"/\")]\n",
    "                for child in children_dict[node]:\n",
    "                    for conn, val in connections_and_weights:\n",
    "                        for conn_child in children_dict.get(conn, []):\n",
    "                            changes_to_apply.append((child, conn_child, val))\n",
    "            for a, b, delta in log_progress(changes_to_apply, desc=\"Applying changes, iteration \" + str(iteration + 1) + \"/\" + str(layers)):\n",
    "                self.add(a, b, delta)\n",
    "                \n",
    "    def dilate(self, iterations = 1, weight_factor = 0.2):\n",
    "        all_nodes = list(self.g.nodes)\n",
    "        for iteration in range(iterations):\n",
    "            changes_to_apply = []\n",
    "            for node in log_progress(all_nodes, desc=\"Dilating coupling information, iteration \" + str(iteration + 1) + \"/\" + str(iterations)):\n",
    "                connections_and_weights = [(conn, self.get(node, conn) * weight_factor) for conn in self.g[node] if not conn.startswith(node + \"/\")]\n",
    "                for (c1, w1), (c2, w2) in all_pairs(connections_and_weights):\n",
    "                    changes_to_apply.append((c1, c2, min(w1, w2)))\n",
    "            for a, b, delta in log_progress(changes_to_apply, desc=\"Applying changes, iteration \" + str(iteration + 1) + \"/\" + str(iterations)):\n",
    "                self.add(a, b, delta)\n",
    "    \n",
    "    def _get_children_dict(self):\n",
    "        result = {}\n",
    "        all_nodes = list(self.g.nodes)\n",
    "        for node in all_nodes:\n",
    "            result[node] = set()\n",
    "        for node in all_nodes:\n",
    "            if \"/\" in node:\n",
    "                parent = \"/\".join(node.split(\"/\")[0:-1])\n",
    "                if parent in result:\n",
    "                    result[parent].add(node)\n",
    "        for node in all_nodes:\n",
    "            if len(result[node]) == 0:\n",
    "                del result[node]\n",
    "        return result\n",
    "        \n",
    "    def get_max_weight(self):\n",
    "        return max([self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges])\n",
    "    \n",
    "    def plaintext_content(self):\n",
    "        node_list = list(self.g.nodes)\n",
    "        node2index = dict(zip(node_list, range(len(node_list))))\n",
    "        return \";\".join(node_list) + \"\\n\" + \";\".join([str(node2index[a]) + \",\" + str(node2index[b]) + \",\" + str(d[\"weight\"]) for a, b, d in self.g.edges(data=True)])\n",
    "        \n",
    "    def print_statistics(self):\n",
    "        # https://networkx.github.io/documentation/latest/tutorial.html#analyzing-graphs\n",
    "        node_count = len(self.g.nodes)\n",
    "        edge_count = len(self.g.edges)\n",
    "        cc = sorted(list(nx.connected_components(self.g)), key= lambda e: -len(e))\n",
    "        print(\"ExplicitCouplingGraph statistics: \"\n",
    "              + str(node_count) + \" nodes, \"\n",
    "              + str(edge_count) + \" edges, \"\n",
    "              + str(len(cc)) + \" connected component(s), with sizes: [\"\n",
    "              + \", \".join([str(len(c)) for c in cc[0:20]])\n",
    "              + \"]\")\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        edge_weights.sort()\n",
    "        node_supports = [self.get_support(n) for n in self.g.nodes]\n",
    "        node_supports.sort()\n",
    "        print(\"Edge weights:\", edge_weights[0:5], \"...\", edge_weights[-5:], \", mean:\", np.array(edge_weights).mean())\n",
    "        print(\"Node support values:\", node_supports[0:5], \"...\", node_supports[-5:], \", mean:\", np.array(node_supports).mean())\n",
    "        \n",
    "    def show_weight_histogram(self):\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        show_histogram(edge_weights, 'Histogram of edge weights in coupling graph', 'Coupling Strength', 'Amount', 'b')\n",
    "        \n",
    "        node_weights = [sum([self.g[n][n2][\"weight\"] for n2 in self.g.adj[n]]) for n in self.g.nodes]\n",
    "        show_histogram(node_weights, 'Histogram of node weights', 'Coupling Strength', 'Amount', 'g')\n",
    "        \n",
    "        node_supports = [self.get_support(n) for n in self.g.nodes]\n",
    "        show_histogram(node_supports, 'Histogram of node support values', 'Support', 'Amount', 'g')\n",
    "        \n",
    "    def visualize(self, use_spring = False, with_labels = True):\n",
    "        # https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
    "        for e in self.g.edges:\n",
    "            self.g[e[0]][e[1]][\"distance\"] = 1.000001 - self.g[e[0]][e[1]][\"weight\"]  # the value must not be exactly zero\n",
    "        \n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        max_weight = max(edge_weights)\n",
    "        mean_weight = np.array(edge_weights).mean()\n",
    "        target_max_weight = min(max_weight, mean_weight * 2)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        VIZ_POW = 10\n",
    "        max_w_fact = (1. / target_max_weight) ** VIZ_POW\n",
    "        \n",
    "        layout = nx.drawing.layout.kamada_kawai_layout(self.g, weight=\"distance\") if use_spring else None\n",
    "        \n",
    "        # nx.draw_kamada_kawai(self.g, alpha=0.2, node_size=100)\n",
    "        # nx.draw(self.g, alpha=0.2, node_size=100)\n",
    "        edge_colors = [(0., 0., 0., min(1., (self.g[a][b][\"weight\"] ** VIZ_POW) * max_w_fact)) for a, b in self.g.edges]\n",
    "        nx.draw(self.g, pos=layout, node_size=50, edge_color=edge_colors, node_color=[(0.121, 0.469, 0.703, 0.2)], with_labels=with_labels)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def print_most_linked_nodes(self, amount = 10):\n",
    "        print(\"Most linked nodes:\")\n",
    "        debug_list = sorted(list(self.g.edges.data()), key = lambda e: -e[2][\"weight\"])\n",
    "        for a, b, info in debug_list[0:amount]:\n",
    "            print(str(info[\"weight\"]) + \": \" + a + \" <> \" + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecbf6020-7b5e-4c52-ada5-1552655b41f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOCUMENT_SIMILARITY_EXP = 8 # higher = lower equality values, lower = equality values are all closer to 1\n",
    "class SimilarityCouplingGraph(NormalizeSupport, NodeSetCouplingGraph):\n",
    "    \"\"\"assigns d-dimensional coordinates to nodes, coupling is defined by their closeness.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        NodeSetCouplingGraph.__init__(self, name)\n",
    "        NormalizeSupport.__init__(self)\n",
    "        self.support = {}\n",
    "        self.coords = {}\n",
    "        \n",
    "    def get_node_set(self):\n",
    "        return set(self.coords.keys())\n",
    "        \n",
    "    def add_node(self, node, coordinates, support):\n",
    "        self.coords[node] = coordinates\n",
    "        self.support[node] = support\n",
    "        \n",
    "    def get_support(self, node):\n",
    "        return self.support.get(node, 0)\n",
    "    \n",
    "    def get_absolute_support(self, node):\n",
    "        return self.get_support(node)\n",
    "    \n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        def array_similarity(a, b):\n",
    "            \"\"\"given two arrays of numbers, how equal are they?\"\"\"\n",
    "            dist = distance.jensenshannon(a, b, 2)  # TODO check for alternative distance metrics?\n",
    "            if np.isnan(dist):\n",
    "                pdb.set_trace()\n",
    "                return 0\n",
    "            return math.pow(1. - dist, DOCUMENT_SIMILARITY_EXP)\n",
    "        coords_a = self.coords.get(a)\n",
    "        coords_b = self.coords.get(b)\n",
    "        if coords_a is None or coords_b is None:\n",
    "            return 0\n",
    "        return array_similarity(coords_a, coords_b)\n",
    "    \n",
    "    def plaintext_content(self):\n",
    "        return \"\\n\".join(\n",
    "            \",\".join([node, str(self.support[node])] + [str(c) for c in self.coords[node]])\n",
    "            for node in self.get_node_set()\n",
    "        )\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        node_count = len(self.coords)\n",
    "        if node_count == 0:\n",
    "            print(\"Empty SimilarityCouplingGraph!\")\n",
    "            return\n",
    "        min_coord = next(iter(self.coords.values()))\n",
    "        max_coord = list(min_coord)\n",
    "        for val in self.coords.values():\n",
    "            for d in range(len(min_coord)):\n",
    "                min_coord[d] = min(min_coord[d], val[d])\n",
    "                max_coord[d] = max(max_coord[d], val[d])\n",
    "        print(\"SimilarityCouplingGraph statistics: \" + str(node_count) + \" nodes\")\n",
    "        print(\"Min coordinates: \" + str(min_coord))\n",
    "        print(\"Max coordinates: \" + str(max_coord))\n",
    "        node_supports = [self.get_support(n) for n in self.get_node_set()]\n",
    "        node_supports.sort()\n",
    "        print(\"Node support values:\", node_supports[0:5], \"...\", node_supports[-5:], \", mean:\", np.array(node_supports).mean())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d601e4f8-f45d-47b0-9287-5a69570b872c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModuleDistanceCouplingGraph(CouplingGraph):\n",
    "    def __init__(self):\n",
    "        CouplingGraph.__init__(self, \"module_distance\")\n",
    "        \n",
    "    def get_normalized_support(self, node):\n",
    "        return 1\n",
    "    \n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        dist = path_module_distance(a, b)\n",
    "        base = 1.1  # needs to be bigger than one. Lower values = stronger coupling across bigger distances. Higher values = faster decay of coupling across module distance\n",
    "        return math.pow(base, -dist)\n",
    "    \n",
    "    def plaintext_content(self):\n",
    "        return \"\"\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        return \"Module Distance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba59e46-52eb-451c-86eb-41a706293389",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # Local debugging code\n",
    "    g1 = ModuleDistanceCouplingGraph()\n",
    "    print(g1.name)\n",
    "    g1.print_statistics()\n",
    "\n",
    "    g2 = ExplicitCouplingGraph(\"structural\")\n",
    "    g2.add_and_support(\"test1\", \"test2\", 2)\n",
    "    g2.add_and_support(\"test3\", \"test2\", 1)\n",
    "    print(g2.name)\n",
    "    print(g2.get_normalized_support(\"test1\"))\n",
    "    print(g2.get_normalized_support(\"test2\"))\n",
    "    print(g2.get_normalized_support(\"test3\"))\n",
    "    print(g2.get_normalized_support(\"test4\"))\n",
    "    print(g2.get_normalized_coupling(\"test1\", \"test2\"))\n",
    "    print(g2.get_normalized_coupling(\"test2\", \"test1\"))\n",
    "    print(g2.get_normalized_coupling(\"test3\", \"test2\"))\n",
    "    print(g2.get_normalized_coupling(\"test2\", \"test3\"))\n",
    "    print(g2.get_normalized_coupling(\"test3\", \"test1\"))\n",
    "    print(g2.get_normalized_coupling(\"test1\", \"test3\"))\n",
    "    print(g2.get_normalized_coupling(\"test4\", \"test3\"))\n",
    "    g2.print_statistics()\n",
    "\n",
    "    g3 = SimilarityCouplingGraph(\"linguistic\")\n",
    "    g3.add_node(\"test1\", [0.8, 0.2, 0], 10)\n",
    "    g3.add_node(\"test2\", [0.4, 0.4, 0.2], 3)\n",
    "    g3.add_node(\"test3\", [0.2, 0.6, 0.2], 30)\n",
    "    print(g3.name)\n",
    "    print(g3.get_normalized_support(\"test1\"))\n",
    "    print(g3.get_normalized_support(\"test2\"))\n",
    "    print(g3.get_normalized_support(\"test3\"))\n",
    "    print(g3.get_normalized_support(\"test4\"))\n",
    "    print(g3.get_normalized_coupling(\"test1\", \"test2\"))\n",
    "    print(g3.get_normalized_coupling(\"test2\", \"test1\"))\n",
    "    print(g3.get_normalized_coupling(\"test3\", \"test2\"))\n",
    "    print(g3.get_normalized_coupling(\"test2\", \"test3\"))\n",
    "    print(g3.get_normalized_coupling(\"test3\", \"test1\"))\n",
    "    print(g3.get_normalized_coupling(\"test1\", \"test3\"))\n",
    "    print(g3.get_normalized_coupling(\"test4\", \"test3\"))\n",
    "    g3.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77430708-82c2-408f-b56d-1ec700c2af03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPy3",
   "language": "python",
   "name": "pypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
