{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex  # the cooler \"re\"\n",
    "from typing import *\n",
    "from biterm.btm import oBTM \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary # helper functions\n",
    "from scipy.spatial import distance\n",
    "import pdb\n",
    "import subprocess\n",
    "%run LocalRepo.ipynb\n",
    "%run util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: https://pypi.org/project/biterm/\n",
    "#\n",
    "\n",
    "# constants\n",
    "MIN_WORD_LENGTH = 1\n",
    "MAX_WORD_LENGTH = 50\n",
    "MIN_WORD_USAGES = 2  # any word used less often will be ignored\n",
    "MAX_DF = 0.95  # any terms that appear in a bigger proportion of the documents than this will be ignored (corpus-specific stop-words)\n",
    "MAX_FEATURES = 4000  # the size of the LDA thesaurus - amount of words to consider for topic learning\n",
    "TOPIC_COUNT = 10 # 40  # 100 according to paper\n",
    "BTM_ITERATIONS = 1000  # 100 according to docs?\n",
    "LDA_RANDOM_SEED = 42\n",
    "DOCUMENT_SIMILARITY_EXP = 8 # higher = lower equality values, lower = equality values are all closer to 1\n",
    "DOCUMENT_SIMILARITY_CUTOFF = 0.05  # in range [0 .. 1]: everything below this is dropped\n",
    "\n",
    "\n",
    "CLI_PATH = \"/home/ebrendel/util/btm/btm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-zimbabwe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_topic_model_documents(files) -> List[Tuple[RepoTree,List[str]]]:  # List of (RepoTree-Node,wordList) - tuples\n",
    "    # keywords from python, TS and Java\n",
    "    custom_stop_words = [\"abstract\", \"and\", \"any\", \"as\", \"assert\", \"async\", \"await\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\", \"class\", \"const\", \"constructor\", \"continue\", \"debugger\", \"declare\", \"def\", \"default\", \"del\", \"delete\", \"do\", \"double\", \"elif\", \"else\", \"enum\", \"except\", \"export\", \"extends\", \"false\", \"False\", \"final\", \"finally\", \"float\", \"for\", \"from\", \"function\", \"get\", \"global\", \"goto\", \"if\", \"implements\", \"import\", \"in\", \"instanceof\", \"int\", \"interface\", \"is\", \"lambda\", \"let\", \"long\", \"module\", \"new\", \"None\", \"nonlocal\", \"not\", \"null\", \"number\", \"of\", \"or\", \"package\", \"pass\", \"private\", \"protected\", \"public\", \"raise\", \"require\", \"return\", \"set\", \"short\", \"static\", \"strictfp\", \"string\", \"super\", \"switch\", \"symbol\", \"synchronized\", \"this\", \"throw\", \"throws\", \"transient\", \"true\", \"True\", \"try\", \"type\", \"typeof\", \"var\", \"void\", \"volatile\", \"while\", \"with\", \"yield\"]\n",
    "    stop_words = set(list(get_stop_words('en')) + custom_stop_words)  # TODO ignored \"list(stopwords.words('english'))\" because it had \"y\" and other weird ones\n",
    "    splitter = r\"(?:[\\W_]+|(?<![A-Z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z]))\"\n",
    "    lemma = WordNetLemmatizer()\n",
    "    printable_characters = set(string.printable)\n",
    "\n",
    "    def _normalize_word(word):\n",
    "        return lemma.lemmatize(lemma.lemmatize(word.lower(), pos = \"n\"), pos = \"v\")\n",
    "\n",
    "    def _get_text(content_string):\n",
    "        # https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "        # https://agailloty.rbind.io/en/project/nlp_clean-text/\n",
    "        content_string = ''.join(c for c in content_string if c in printable_characters)\n",
    "        words = regex.split(splitter, content_string, flags=regex.VERSION1)  # regex V1 allows splitting on empty matches\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        words = [_normalize_word(word) for word in words]\n",
    "        words = [word for word in words if len(word) >= MIN_WORD_LENGTH and len(word) <= MAX_WORD_LENGTH]\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        return words\n",
    "\n",
    "\n",
    "    # see https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "    freq_dist = FreqDist()\n",
    "\n",
    "    node_words: List[Tuple[RepoTree,List[str]]]  = []  # List of (RepoTree-Node,wordList) - tuples\n",
    "    for file in log_progress(files, desc=\"Extracting language corpus\"):\n",
    "        node = file.get_repo_tree_node()  # TODO unify with structural view code\n",
    "        if node is None:\n",
    "            continue  # TODO why / when does this happen?\n",
    "\n",
    "        # TODO keep in sync with evolutionary and structural view as well as RepoFile class\n",
    "        classes = node.get_descendants_of_type(\"class\") + node.get_descendants_of_type(\"interface\") + node.get_descendants_of_type(\"enum\")\n",
    "        for class_node in classes:\n",
    "            fields = class_node.get_children_of_type(\"field\")\n",
    "            methods = class_node.get_children_of_type(\"method\") + class_node.get_children_of_type(\"constructor\")\n",
    "            # print(\"Class \" + class_node.name + \": \" + str(len(methods)) + \" methods and \" + str(len(fields)) + \" fields\")\n",
    "\n",
    "            for member in fields + methods:\n",
    "                text = member.get_comment_and_own_text(file)\n",
    "                # words = list(_get_text(class_node.get_path() + \" \" + text))\n",
    "                words = list(_get_text(class_node.name + \" \" + text))\n",
    "                for word in words:\n",
    "                    freq_dist[word] += 1\n",
    "                # TODO: handle the empty list?!?\n",
    "                node_words.append((member, words))\n",
    "                # print(\" \".join(words))\n",
    "\n",
    "    # random.seed(LDA_RANDOM_SEED)\n",
    "    # random.shuffle(node_words)\n",
    "\n",
    "    for word in freq_dist:\n",
    "        if freq_dist[word] < MIN_WORD_USAGES:\n",
    "            del freq_dist[word]\n",
    "\n",
    "    print(\"Amount of documents: \" + str(len(node_words)))\n",
    "    print(\"Total Amount of words: \" + str(sum([len(b) for a, b in node_words])))\n",
    "    print(\"Vocab size: \" + str(len(freq_dist)))\n",
    "    return node_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_topic_model(node_words: List[Tuple[RepoTree,List[str]]]):\n",
    "    \"\"\"returns the document - topic matrix, stating how much of each topic is present in each document\"\"\"\n",
    "\n",
    "    \n",
    "    process_options = [CLI_PATH,\n",
    "                       '--iterations', str(BTM_ITERATIONS),\n",
    "                       '--maxVocabSize', str(MAX_FEATURES),\n",
    "                       '--topicCount', str(TOPIC_COUNT),\n",
    "                      ]\n",
    "    process = subprocess.Popen(process_options, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    for (node, words) in node_words:\n",
    "        process.stdin.write((\" \".join(words) + \"\\n\").encode(\"utf-8\"))\n",
    "    process.stdin.close()\n",
    "    # print(\"\\n\".join(\" \".join(words) for node, words in node_words))\n",
    "    \n",
    "    doctop = []\n",
    "    with log_progress(desc=\"BTM Gibbs Sampling\", total=BTM_ITERATIONS) as pbar:\n",
    "        while True:\n",
    "            from_program = process.stdout.readline().decode(\"utf-8\")\n",
    "            if not from_program:\n",
    "                break\n",
    "            if from_program.startswith(\"#progress \"):\n",
    "                pbar.n = int(from_program[len(\"#progress \"):].split(\" \")[0])\n",
    "                pbar.update(0)\n",
    "            elif from_program.startswith(\"#doctop \"):\n",
    "                data = [float(x) for x in from_program[len(\"#doctop \"):].strip().split(\",\")]\n",
    "                doctop.append(data)\n",
    "            else:\n",
    "                short_line = (from_program[:100] + '...') if len(from_program) > 102 else from_program\n",
    "                if short_line[-1] != \"\\n\":\n",
    "                    short_line += \"\\n\"\n",
    "                print(\"[BTM] \" + short_line, end=\"\")\n",
    "                \n",
    "    if len(doctop) != len(node_words):\n",
    "        print(\"Received wrong number of doctop lines! Exit code: \" + str(process.poll()))\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    return np.array(doctop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def couple_by_topic_similarity(node_words: List[Tuple[RepoTree,List[str]]], doctop, coupling_graph):\n",
    "\n",
    "    for (repo_tree, words), topics in log_progress(zip(node_words, doctop), total=len(node_words), desc=\"Generating coupling graph\"):\n",
    "        if not all(x < 0.001 for x in topics):  # if it is not topic-less\n",
    "            coupling_graph.add_node(repo_tree.get_path(), topics, len(words))\n",
    "\n",
    "    return coupling_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
