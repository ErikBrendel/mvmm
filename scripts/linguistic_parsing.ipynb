{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex  # the cooler \"re\"\n",
    "from typing import *\n",
    "from biterm.btm import oBTM \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary # helper functions\n",
    "from scipy.spatial import distance\n",
    "import pdb\n",
    "%run LocalRepo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: https://pypi.org/project/biterm/\n",
    "#\n",
    "\n",
    "# constants\n",
    "MIN_WORD_LENGTH = 1\n",
    "MAX_WORD_LENGTH = 50\n",
    "MIN_WORD_USAGES = 2  # any word used less often will be ignored\n",
    "MAX_DF = 0.95  # any terms that appear in a bigger proportion of the documents than this will be ignored (corpus-specific stop-words)\n",
    "MAX_FEATURES = 1500  # the size of the LDA thesaurus - amount of words to consider for topic learning\n",
    "TOPIC_COUNT = 10 # 40  # 100 according to paper\n",
    "BTM_ITERATIONS = 20  # 100 according to docs?\n",
    "# LDA_PASSES = 200  # how often to go through the corpus\n",
    "LDA_RANDOM_SEED = 42\n",
    "DOCUMENT_SIMILARITY_EXP = 8 # higher = lower equality values, lower = equality values are all closer to 1\n",
    "DOCUMENT_SIMILARITY_CUTOFF = 0.1  # in range [0 .. 1]: everything below this is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_model_documents(files) -> List[Tuple[RepoTree,List[str]]]:  # List of (RepoTree-Node,wordList) - tuples\n",
    "    # keywords from python, TS and Java\n",
    "    custom_stop_words = [\"abstract\", \"and\", \"any\", \"as\", \"assert\", \"async\", \"await\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\", \"class\", \"const\", \"constructor\", \"continue\", \"debugger\", \"declare\", \"def\", \"default\", \"del\", \"delete\", \"do\", \"double\", \"elif\", \"else\", \"enum\", \"except\", \"export\", \"extends\", \"false\", \"False\", \"final\", \"finally\", \"float\", \"for\", \"from\", \"function\", \"get\", \"global\", \"goto\", \"if\", \"implements\", \"import\", \"in\", \"instanceof\", \"int\", \"interface\", \"is\", \"lambda\", \"let\", \"long\", \"module\", \"new\", \"None\", \"nonlocal\", \"not\", \"null\", \"number\", \"of\", \"or\", \"package\", \"pass\", \"private\", \"protected\", \"public\", \"raise\", \"require\", \"return\", \"set\", \"short\", \"static\", \"strictfp\", \"string\", \"super\", \"switch\", \"symbol\", \"synchronized\", \"this\", \"throw\", \"throws\", \"transient\", \"true\", \"True\", \"try\", \"type\", \"typeof\", \"var\", \"void\", \"volatile\", \"while\", \"with\", \"yield\"]\n",
    "    stop_words = set(list(get_stop_words('en')) + custom_stop_words)  # TODO ignored \"list(stopwords.words('english'))\" because it had \"y\" and other weird ones\n",
    "    splitter = r\"(?:[\\W_]+|(?<![A-Z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z]))\"\n",
    "    lemma = WordNetLemmatizer()\n",
    "    printable_characters = set(string.printable)\n",
    "\n",
    "    def _normalize_word(word):\n",
    "        return lemma.lemmatize(lemma.lemmatize(word.lower(), pos = \"n\"), pos = \"v\")\n",
    "\n",
    "    def _get_text(content_string):\n",
    "        # https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "        # https://agailloty.rbind.io/en/project/nlp_clean-text/\n",
    "        content_string = ''.join(c for c in content_string if c in printable_characters)\n",
    "        words = regex.split(splitter, content_string, flags=regex.VERSION1)  # regex V1 allows splitting on empty matches\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        words = [_normalize_word(word) for word in words]\n",
    "        words = [word for word in words if len(word) >= MIN_WORD_LENGTH and len(word) <= MAX_WORD_LENGTH]\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        return words\n",
    "\n",
    "\n",
    "    # see https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "    freq_dist = FreqDist()\n",
    "\n",
    "    node_words: List[Tuple[RepoTree,List[str]]]  = []  # List of (RepoTree-Node,wordList) - tuples\n",
    "    for file in log_progress(files, desc=\"Extracting language corpus\"):\n",
    "        node = file.get_repo_tree_node()  # TODO unify with structural view code\n",
    "        if node is None:\n",
    "            continue  # TODO why / when does this happen?\n",
    "\n",
    "        # TODO keep in sync with evolutionary and structural view as well as RepoFile class\n",
    "        classes = node.get_descendants_of_type(\"class\") + node.get_descendants_of_type(\"interface\") + node.get_descendants_of_type(\"enum\")\n",
    "        for class_node in classes:\n",
    "            fields = class_node.get_children_of_type(\"field\")\n",
    "            methods = class_node.get_children_of_type(\"method\") + class_node.get_children_of_type(\"constructor\")\n",
    "            # print(\"Class \" + class_node.name + \": \" + str(len(methods)) + \" methods and \" + str(len(fields)) + \" fields\")\n",
    "\n",
    "            for member in fields + methods:\n",
    "                text = member.get_comment_and_own_text(file)\n",
    "                # words = list(_get_text(class_node.get_path() + \" \" + text))\n",
    "                words = list(_get_text(class_node.name + \" \" + text))\n",
    "                for word in words:\n",
    "                    freq_dist[word] += 1\n",
    "                # TODO: handle the empty list?!?\n",
    "                node_words.append((member, words))\n",
    "                # print(\" \".join(words))\n",
    "\n",
    "    # random.seed(LDA_RANDOM_SEED)\n",
    "    # random.shuffle(node_words)\n",
    "\n",
    "    for word in freq_dist:\n",
    "        if freq_dist[word] < MIN_WORD_USAGES:\n",
    "            del freq_dist[word]\n",
    "\n",
    "    print(\"Amount of documents: \" + str(len(node_words)))\n",
    "    print(\"Total Amount of words: \" + str(sum([len(b) for a, b in node_words])))\n",
    "    print(\"Vocab size: \" + str(len(freq_dist)))\n",
    "    return node_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_topic_model(node_words: List[Tuple[RepoTree,List[str]]]):\n",
    "    \"\"\"returns the document - topic matrix, stating how much of each topic is present in each document\"\"\"\n",
    "\n",
    "    def vec_to_sparse_btm(X):\n",
    "        B_d = []\n",
    "        for x in X:\n",
    "            nonzero = np.nonzero(x)[0]\n",
    "            b_i = [(nonzero[i], nonzero[i+1]) for i in range(len(nonzero)-1)]\n",
    "            B_d.append(b_i)\n",
    "        return B_d\n",
    "    \n",
    "    # https://pypi.org/project/biterm/\n",
    "    print(\"Vectorizing words...\")\n",
    "    pdb.set_trace()\n",
    "    texts = [\" \".join(words) for (node, words) in node_words]\n",
    "    print(\"Here are the texts:\")\n",
    "    print(\"\\n\".join(texts))\n",
    "    print(\"------\")\n",
    "    vec = CountVectorizer(max_df=MAX_DF, max_features=MAX_FEATURES, stop_words=None)\n",
    "    X = vec.fit_transform(texts).toarray()\n",
    "    vocab = np.array(vec.get_feature_names())\n",
    "\n",
    "    print(\"Instantiating BTM...\")\n",
    "    btm = oBTM(num_topics=TOPIC_COUNT, V=vocab)\n",
    "    biterms = vec_to_biterms(X)\n",
    "    print(\"Total Amount of biterms: \" + str(sum([len(x) for x in biterms])))\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    pdb.set_trace()\n",
    "    btm.fit(biterms, iterations=BTM_ITERATIONS)\n",
    "    topics = btm.transform(biterms)\n",
    "\n",
    "    print(\"Generating topic output...\")\n",
    "    words_to_show_per_topic = 10\n",
    "    topic_summuary(btm.phi_wz.T, X, vocab, words_to_show_per_topic)\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def couple_by_topic_similarity(node_words: List[Tuple[RepoTree,List[str]]], topics, coupling_graph):\n",
    "\n",
    "    def array_similarity(a, b):\n",
    "        \"\"\"given two arrays of numbers, how equal are they?\"\"\"\n",
    "        return math.pow(1. - distance.cosine(a, b), DOCUMENT_SIMILARITY_EXP)  # TODO check for alternative distance metrics?\n",
    "    \n",
    "    print(\"Generating coupling graph...\")\n",
    "    # paths = [n.get_path() for n, w in node_words]  # TODO speed up by using this pths array?\n",
    "    for n1 in log_progress(range(len(node_words)), desc=\"Generating coupling graph\"):\n",
    "        for n2 in range(len(node_words)):\n",
    "            if n1 >= n2:\n",
    "                continue\n",
    "            t1 = topics[n1]\n",
    "            t2 = topics[n2]\n",
    "            if np.isnan(t1[0]) or np.isnan(t2[0]):\n",
    "                continue  # TODO filter out those earlier (happen when a btm-document is empty)\n",
    "            similarity = array_similarity(t1, t2)\n",
    "            coupling_graph.add(node_words[n1][0].get_path(), node_words[n2][0].get_path(), similarity)\n",
    "\n",
    "    for node, words in log_progress(node_words, desc=\"Generating coupling graph step 2\"):\n",
    "        coupling_graph.add_support(node.get_path(), len(words))\n",
    "\n",
    "    print(\"Trimming graph...\")\n",
    "    coupling_graph.cutoff_edges(DOCUMENT_SIMILARITY_CUTOFF)\n",
    "\n",
    "    return coupling_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
