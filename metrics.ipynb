{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parsing.ipynb\n",
    "%run util.ipynb\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from typing import List\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, FreqDist\n",
    "import string\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import similarities\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from multiprocessing import Pool, TimeoutError, Process, Manager, Lock\n",
    "from functools import partial\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_SAVE_PATH = \"../metrics/\"\n",
    "EXPORT_SAVE_PATH = \"../export/\"\n",
    "\n",
    "# https://networkx.github.io/documentation/latest/tutorial.html#edges\n",
    "class WeightGraph:\n",
    "    def __init__(self):\n",
    "        self.g = nx.Graph()\n",
    "        \n",
    "    def add(self, a, b, delta):\n",
    "        self.g.add_node(a)\n",
    "        self.g.add_node(b)\n",
    "        new_value = self.get(a, b) + delta\n",
    "        self.g.add_edge(a, b, weight=new_value)\n",
    "        \n",
    "    def add_sym(self, a, b, delta):\n",
    "        self.add(a, b, delta)\n",
    "        \n",
    "    def get(self, a, b):\n",
    "        if b in self.g.adj[a]:\n",
    "            return self.g.adj[a][b][\"weight\"]\n",
    "        return 0\n",
    "    \n",
    "    def cutoff_edges(self, minimum_weight):\n",
    "        fedges = [(a, b) for a, b, info in self.g.edges.data() if info[\"weight\"] < minimum_weight]\n",
    "        self.g.remove_edges_from(fedges)\n",
    "        \n",
    "    def cleanup(self):\n",
    "        # self.g.remove_nodes_from(list(nx.isolates(self.g)))\n",
    "        for component in list(nx.connected_components(self.g)):\n",
    "            if len(component) < 5:\n",
    "                for node in component:\n",
    "                    self.g.remove_node(node)\n",
    "        \n",
    "    def save(self, repo_name, name):\n",
    "        os.makedirs(METRICS_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        nx.write_gpickle(self.g, WeightGraph.pickle_path(repo_name, name))\n",
    "        \n",
    "    def get_max_weight(self):\n",
    "        return max([self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges])\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(repo_name, name):\n",
    "        wg = WeightGraph()\n",
    "        wg.g = nx.read_gpickle(WeightGraph.pickle_path(repo_name, name))\n",
    "        return wg\n",
    "        \n",
    "    @staticmethod\n",
    "    def pickle_path(repo_name, name):\n",
    "        # see https://networkx.github.io/documentation/stable/reference/readwrite/gpickle.html\n",
    "        return METRICS_SAVE_PATH + repo_name + \"/\" + name + \".gpickle\"\n",
    "    \n",
    "    def json_save(self, repo_name, name):\n",
    "        data = json_graph.node_link_data(self.g)\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + name + \".json\", 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            \n",
    "    def html_save(self, repo_name, name):\n",
    "        data = json.dumps(json_graph.node_link_data(self.g))\n",
    "        content = '<html><body><script type=\"text/javascript\">const graph = ' + data + ';</script><script src=\"/files/metrics/html_app.js?_xsrf=2%7Ce163cb61%7Cb9245804a283415ecb4c641f0cf1f882%7C1601372106\"></script></body></html>'\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + name + \".html\", 'w') as outfile:\n",
    "            outfile.write(content)\n",
    "    \n",
    "    def plaintext_save(self, repo_name, name):\n",
    "        node_list = list(self.g.nodes)\n",
    "        node2index = dict(zip(node_list, range(len(node_list))))\n",
    "        content = \";\".join(node_list) + \"\\n\" + \";\".join([str(node2index[a]) + \",\" + str(node2index[b]) + \",\" + str(d[\"weight\"]) for a, b, d in self.g.edges(data=True)])\n",
    "        os.makedirs(EXPORT_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        with open(EXPORT_SAVE_PATH + repo_name + \"/\" + name + \".graph.txt\", \"w\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    \n",
    "    def print_statistics(self):\n",
    "        # https://networkx.github.io/documentation/latest/tutorial.html#analyzing-graphs\n",
    "        node_count = len(self.g.nodes)\n",
    "        edge_count = len(self.g.edges)\n",
    "        cc = list(nx.connected_components(self.g))\n",
    "        print(\"WeightGraph statistics: \"\n",
    "              + str(node_count) + \" nodes, \"\n",
    "              + str(edge_count) + \" edges, \"\n",
    "              + str(len(cc)) + \" connected component(s), with sizes: [\"\n",
    "              + \", \".join([str(len(c)) for c in cc])\n",
    "              + \"]\")\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        edge_weights.sort()\n",
    "        print(\"Edge weights:\", edge_weights[0:5], \"...\", edge_weights[-5:], \", mean:\", np.array(edge_weights).mean())\n",
    "    \n",
    "    def show_weight_histogram(self):\n",
    "        # https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "        # import pdb; pdb.set_trace()  # debugger\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        plt.hist(edge_weights, \"auto\", facecolor='b', alpha=0.75)\n",
    "        plt.axvline(np.array(edge_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of edge weights in coupling graph')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "        node_weights = [sum([self.g[n][n2][\"weight\"] for n2 in self.g.adj[n]]) for n in self.g.nodes]\n",
    "        plt.hist(node_weights, \"auto\", facecolor='g', alpha=0.75)\n",
    "        plt.axvline(np.array(node_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        # plt.xscale(\"log\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of node weights')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize(self, use_spring = False, with_labels = True):\n",
    "        # https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
    "        \n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        max_weight = max(edge_weights)\n",
    "        mean_weight = np.array(edge_weights).mean()\n",
    "        target_max_weight = min(max_weight, mean_weight * 2)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        VIZ_POW = 1\n",
    "        max_w_fact = (1. / target_max_weight) ** VIZ_POW\n",
    "        \n",
    "        draw_func = nx.draw_kamada_kawai if use_spring else nx.draw\n",
    "        \n",
    "        # nx.draw_kamada_kawai(self.g, alpha=0.2, node_size=100)\n",
    "        # nx.draw(self.g, alpha=0.2, node_size=100)\n",
    "        edge_colors = [(0., 0., 0., min(1., (self.g[a][b][\"weight\"] ** VIZ_POW) * max_w_fact)) for a, b in self.g.edges]\n",
    "        draw_func(self.g, node_size=50, edge_color=edge_colors, node_color=[(0.121, 0.469, 0.703, 0.2)], with_labels = with_labels)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COMMIT_FILES = 50\n",
    "# needs to be separate so that multiprocessing lib can find it\n",
    "def get_commit_diff(commit_hash, repo):\n",
    "    c1 = repo.get_commit(commit_hash)\n",
    "    if len(c1.parents) == 1:\n",
    "        c2 = c1.parents[0]\n",
    "        diff = c1.diff(c2)\n",
    "        diffs = [ d.a_path for d in diff ]\n",
    "    elif len(c1.parents) == 2:\n",
    "        c2 = c1.parents[0]\n",
    "        diff_1 = c1.diff(c2)\n",
    "        c3 = c1.parents[1]\n",
    "        diff_2 = c1.diff(c3)\n",
    "\n",
    "        diffs_1 = [ d.a_path for d in diff_1 ]\n",
    "        diffs_2 = [ d.a_path for d in diff_2 ]\n",
    "        diffs = list(set(diffs_1).intersection(set(diffs_2)))\n",
    "    else:\n",
    "        return None\n",
    "    if len(diffs) > MAX_COMMIT_FILES or len(diffs) <= 1:\n",
    "        return None\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsGeneration:\n",
    "    # ascii art: http://patorjk.com/software/taag/#p=display&f=Soft&t=STRUCTURAL%0A.%0ALINGUISTIC%0A.%0AEVOLUTIONARY%0A.%0ADYNAMIC\n",
    "    def __init__(self, repo):\n",
    "        self.repo = repo\n",
    "        \n",
    "    def calculate_evolutionary_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",------.,--.   ,--.,-----. ,--.   ,--. ,--.,--------.,--. ,-----. ,--.  ,--.  ,---.  ,------.,--.   ,--. \n",
    "|  .---' \\  `.'  /'  .-.  '|  |   |  | |  |'--.  .--'|  |'  .-.  '|  ,'.|  | /  O  \\ |  .--. '\\  `.'  /  \n",
    "|  `--,   \\     / |  | |  ||  |   |  | |  |   |  |   |  ||  | |  ||  |' '  ||  .-.  ||  '--'.' '.    /   \n",
    "|  `---.   \\   /  '  '-'  '|  '--.'  '-'  '   |  |   |  |'  '-'  '|  | `   ||  | |  ||  |\\  \\    |  |    \n",
    "`------'    `-'    `-----' `-----' `-----'    `--'   `--' `-----' `--'  `--'`--' `--'`--' '--'   `--'    \n",
    "        \"\"\"\n",
    "        # MAX_COMMIT_FILES = 50  # Ignore too large commits. (constant moved)\n",
    "        PARALLEL_THREADS = 45  # more seems to make it worse again?\n",
    "        PARALLEL_BATCH_SIZE = 2  # the size of packets delivered to worker processes\n",
    "        \n",
    "        coupling_graph = WeightGraph()\n",
    "        # graph_lock = Lock()\n",
    "        \n",
    "        def processDiffs(diffs):\n",
    "            score = 2 / len(diffs)\n",
    "            # with graph_lock:\n",
    "            for f1, f2 in all_pairs(diffs):\n",
    "                coupling_graph.add_sym(f1, f2, score)\n",
    "        \n",
    "        all_commits = list(self.repo.get_all_commits())\n",
    "        print(\"Commits to analyze: \" + str(len(all_commits)))\n",
    "\n",
    "        with Pool(processes=PARALLEL_THREADS) as pool:\n",
    "            bar = log_progress(total=len(all_commits), desc=\"Analyzing commits\", smoothing=0.1)\n",
    "            diffs = pool.imap_unordered(partial(get_commit_diff, repo=self.repo), all_commits, PARALLEL_BATCH_SIZE)\n",
    "            # single-threaded alternative for debugging:\n",
    "            #diffs = []\n",
    "            #for i, t in enumerate(all_commits):\n",
    "            #    diffs.append(partial(get_commit_diff, repo=self.repo)(t))\n",
    "            #    print(str(i / float(len(all_commits)) * 100.) + \" % done.\")\n",
    "\n",
    "            for i, elem in enumerate(diffs):\n",
    "                if elem is not None:\n",
    "                    processDiffs(elem)\n",
    "                    # print(str(i / float(len(all_commits)) * 100.) + \" % done.\")\n",
    "                bar.update()\n",
    "        \n",
    "            bar.close()\n",
    "        coupling_graph.cutoff_edges(0.005)\n",
    "        return coupling_graph\n",
    "    \n",
    "    \n",
    "    def calculate_structural_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    " ,---. ,--------.,------. ,--. ,--. ,-----.,--------.,--. ,--.,------.   ,---.  ,--.                     \n",
    "'   .-''--.  .--'|  .--. '|  | |  |'  .--./'--.  .--'|  | |  ||  .--. ' /  O  \\ |  |                     \n",
    "`.  `-.   |  |   |  '--'.'|  | |  ||  |       |  |   |  | |  ||  '--'.'|  .-.  ||  |                     \n",
    ".-'    |  |  |   |  |\\  \\ '  '-'  ''  '--'\\   |  |   '  '-'  '|  |\\  \\ |  | |  ||  '--.                  \n",
    "`-----'   `--'   `--' '--' `-----'  `-----'   `--'    `-----' `--' '--'`--' `--'`-----'   \n",
    "        \"\"\"\n",
    "        STRNGTH_FILE_IMPORT = 1\n",
    "        STRENGTH_MEMBER_CLASS = 1\n",
    "        STRENGTH_METHOD_RETURN_CLASS = 1\n",
    "        STRENGTH_METHOD_PARAM_CLASS = 1\n",
    "        \n",
    "        \n",
    "        builtin_types = set(['void', 'String', 'byte', 'short', 'int', 'long', 'float', 'double', 'boolean', 'char', 'Byte', 'Short', 'Integer', 'Long', 'Float', 'Double', 'Boolean', 'Character'])\n",
    "        stl_types = set(['ArrayList', 'List', 'LinkedList', 'Map', 'HashMap', 'Object'])\n",
    "        ignored_types = builtin_types.union(stl_types)\n",
    "        \n",
    "        coupling_graph = WeightGraph()\n",
    "\n",
    "        error_query = JA_LANGUAGE.query(\"(ERROR) @err\")\n",
    "        package_query_1 = JA_LANGUAGE.query(\"(package_declaration (identifier) @decl)\")\n",
    "        package_query_2 = JA_LANGUAGE.query(\"(package_declaration (scoped_identifier) @decl)\")\n",
    "        import_query = JA_LANGUAGE.query(\"(import_declaration (scoped_identifier) @decl)\")\n",
    "        class_query = JA_LANGUAGE.query(\"(class_declaration name: (identifier) @decl)\")\n",
    "\n",
    "\n",
    "        def _has_error(file) -> List[str]:\n",
    "            errors = error_query.captures(file.get_tree().root_node)\n",
    "            return len(errors) > 1\n",
    "\n",
    "\n",
    "        def _get_package(file) -> List[str]:\n",
    "            packages = package_query_1.captures(file.get_tree().root_node) + package_query_2.captures(file.get_tree().root_node)\n",
    "            # assert len(packages) <= 1\n",
    "            if len(packages) > 1:\n",
    "                import pdb; pdb.set_trace()\n",
    "            if len(packages) == 1:\n",
    "                return file.node_text(packages[0][0]).split(\".\")\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def _get_imports(file) -> List[str]:\n",
    "            imports = import_query.captures(file.get_tree().root_node)\n",
    "            result = []\n",
    "            for import_statement in imports:\n",
    "                import_string = file.node_text(import_statement[0])\n",
    "                if not import_string.startswith(\"java\"):\n",
    "                    result.append(import_string)\n",
    "            return result\n",
    "\n",
    "        def _get_main_class_name(file) -> List[str]:\n",
    "            classes = class_query.captures(file.get_tree().root_node)\n",
    "            if len(classes) >= 1:\n",
    "                return file.node_text(classes[0][0])\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        #######\n",
    "\n",
    "        full_class_name_to_id = {}\n",
    "\n",
    "        files = self._get_all_files()\n",
    "        for file in log_progress(files, desc=\"Building Import Graph\", smoothing=0.1):\n",
    "            if _has_error(file):\n",
    "                continue\n",
    "            class_name = _get_main_class_name(file)\n",
    "            if class_name is not None:\n",
    "                full_class_name = \".\".join(_get_package(file) + [class_name])\n",
    "                full_class_name_to_id[full_class_name] = file.get_path()\n",
    "                class_node = file.get_repo_tree_node().find_node(class_name)\n",
    "                if class_node is not None:\n",
    "                    full_class_name_to_id[full_class_name] = class_node.get_path()\n",
    "                \n",
    "                \n",
    "        def _resolve_type(type_name, imports, file_tree_node: RepoTree) -> str:\n",
    "            \"\"\"find the node name of the node representing this type, if any, or None\"\"\"\n",
    "            # in imports\n",
    "            matching_imports = [i for i in imports if i.endswith(\".\" + type_name) or i == type_name]\n",
    "            if len(matching_imports) > 1:\n",
    "                raise Exception(\"Ambiguous import!\")\n",
    "            elif len(matching_imports) == 1:\n",
    "                import_path = matching_imports[0]\n",
    "                if import_path in full_class_name_to_id:\n",
    "                    return full_class_name_to_id[import_path]\n",
    "            # in same file\n",
    "            classes = node.get_descendants_of_type(\"class\") + node.get_descendants_of_type(\"interface\")\n",
    "            matching_classes = [c for c in classes if c.name == type_name]\n",
    "            if len(matching_classes) > 1:\n",
    "                pdb.set_trace()\n",
    "                raise Exception(\"Ambiguous class name!\")\n",
    "            elif len(matching_classes) == 1:\n",
    "                return matching_classes[0].get_path()\n",
    "            # in same package\n",
    "            type_file_name = type_name + \".java\"\n",
    "            if file_tree_node.parent.has_child(type_file_name):\n",
    "                class_file = file_tree_node.parent.find_node(type_file_name)\n",
    "                class_node = class_file.find_node(type_name)\n",
    "                if class_node is None:\n",
    "                    return class_file.get_path()\n",
    "                else:\n",
    "                    return class_node.get_path()\n",
    "            return None\n",
    "        \n",
    "        def _couple_type(type_text, coupling_path, coupling_strength):\n",
    "            data_types = re.split(\"[^\\w]+\", type_text)\n",
    "            data_types = [dt.strip() for dt in data_types]\n",
    "            data_types = [dt for dt in data_types if len(dt) > 0 and not dt[0].isdigit()]\n",
    "            data_types = [dt for dt in data_types if dt not in ignored_types]\n",
    "            for data_type in data_types:\n",
    "                resolved_type = _resolve_type(data_type, imports, node)\n",
    "                if resolved_type is not None:\n",
    "                    print(\"  Coupling \" + coupling_path + \" to \" + resolved_type)\n",
    "                    coupling_graph.add(coupling_path, resolved_type, coupling_strength)\n",
    "                else:\n",
    "                    print(\"  Cannot resolve type: \" + data_type)\n",
    "        \n",
    "\n",
    "        for file in log_progress(files, desc=\"Extraction connections\", smoothing=0.1):\n",
    "            imports = _get_imports(file)\n",
    "            for i in imports:\n",
    "                if i in full_class_name_to_id:\n",
    "                    pass# print(\"import RESOLVED: \" + i)\n",
    "                    coupling_graph.add(file.get_path(), full_class_name_to_id[i], STRNGTH_FILE_IMPORT)\n",
    "                else:\n",
    "                    pass # print(\"cannot resolve import: \" + i)\n",
    "        \n",
    "            node = file.get_repo_tree_node()\n",
    "            if node is None:\n",
    "                continue  # TODO why / when does this happen?\n",
    "            classes = node.get_descendants_of_type(\"class\") + node.get_descendants_of_type(\"interface\")\n",
    "            for class_node in classes:\n",
    "                fields = class_node.get_children_of_type(\"field\")\n",
    "                methods = class_node.get_children_of_type(\"method\")\n",
    "                print(\"Class \" + class_node.name + \": \" + str(len(methods)) + \" methods and \" + str(len(fields)) + \" fields\")\n",
    "                \n",
    "                for field in fields:\n",
    "                    type_node = field.ts_node.child_by_field_name(\"type\")\n",
    "                    _couple_type(file.node_text(type_node), field.get_path(), STRENGTH_MEMBER_CLASS)\n",
    "                            \n",
    "                for method in methods:\n",
    "                    type_node = method.ts_node.child_by_field_name(\"type\")\n",
    "                    _couple_type(file.node_text(type_node), method.get_path(), STRENGTH_METHOD_RETURN_CLASS)\n",
    "                    parameters_node = method.ts_node.child_by_field_name(\"parameters\")\n",
    "                    for parameter in [p for p in parameters_node.children if p.type == 'formal_parameter']:\n",
    "                        type_node = parameter.child_by_field_name(\"type\")\n",
    "                        _couple_type(file.node_text(type_node), method.get_path(), STRENGTH_METHOD_PARAM_CLASS)\n",
    "                        \n",
    "\n",
    "        return coupling_graph\n",
    "    \n",
    "    \n",
    "    def calculate_linguistic_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",--.   ,--.,--.  ,--. ,----.   ,--. ,--.,--. ,---. ,--------.,--. ,-----.                                \n",
    "|  |   |  ||  ,'.|  |'  .-./   |  | |  ||  |'   .-''--.  .--'|  |'  .--./                                \n",
    "|  |   |  ||  |' '  ||  | .---.|  | |  ||  |`.  `-.   |  |   |  ||  |                                    \n",
    "|  '--.|  ||  | `   |'  '--'  |'  '-'  '|  |.-'    |  |  |   |  |'  '--'\\                                \n",
    "`-----'`--'`--'  `--' `------'  `-----' `--'`-----'   `--'   `--' `-----'              \n",
    "        \"\"\"\n",
    "        # constants\n",
    "        MIN_WORD_LENGTH = 3\n",
    "        MAX_WORD_LENGTH = 50\n",
    "        MIN_WORD_USAGES = 2  # any word used less often will be ignored\n",
    "        MAX_DF = 0.95  # any terms that appear in a bigger proportion of the documents than this will be ignored (corpus-specific stop-words)\n",
    "        MAX_FEATURES = 1500  # the size of the LDA thesaurus - amount of words to consider for topic learning\n",
    "        TOPIC_COUNT = 10 # 40  # 100 according to paper\n",
    "        LDA_ITERATIONS = 1000  # 3.000 according to paper, but at least 500, but we are using online learning, they did not\n",
    "        LDA_PASSES = 200  # how often to go through the corpus\n",
    "        LDA_RANDOM_SEED = 42\n",
    "        DOCUMENT_SIMILARITY_EXP = 4 # higher = lower equality values, lower = equality values are all closer to 1\n",
    "        DOCUMENT_SIMILARITY_CUTOFF = 0.01  # in range [0 .. 1]: everything below this is dropped\n",
    "        \n",
    "        \n",
    "        # keywords from python, TS and Java\n",
    "        custom_stop_words = [\"abstract\", \"and\", \"any\", \"as\", \"assert\", \"async\", \"await\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\", \"class\", \"const\", \"constructor\", \"continue\", \"debugger\", \"declare\", \"def\", \"default\", \"del\", \"delete\", \"do\", \"double\", \"elif\", \"else\", \"enum\", \"except\", \"export\", \"extends\", \"false\", \"False\", \"final\", \"finally\", \"float\", \"for\", \"from\", \"function\", \"get\", \"global\", \"goto\", \"if\", \"implements\", \"import\", \"in\", \"instanceof\", \"int\", \"interface\", \"is\", \"lambda\", \"let\", \"long\", \"module\", \"new\", \"None\", \"nonlocal\", \"not\", \"null\", \"number\", \"of\", \"or\", \"package\", \"pass\", \"private\", \"protected\", \"public\", \"raise\", \"require\", \"return\", \"set\", \"short\", \"static\", \"strictfp\", \"string\", \"super\", \"switch\", \"symbol\", \"synchronized\", \"this\", \"throw\", \"throws\", \"transient\", \"true\", \"True\", \"try\", \"type\", \"typeof\", \"var\", \"void\", \"volatile\", \"while\", \"with\", \"yield\"]\n",
    "        stop_words = set(list(get_stop_words('en')) + list(stopwords.words('english')) + custom_stop_words)\n",
    "        splitter = r\"(?:[\\W_]+|(?<![A-Z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z]))\"\n",
    "        lemma = WordNetLemmatizer()\n",
    "        printable_characters = set(string.printable)\n",
    "        \n",
    "        def _normalize_word(word):\n",
    "            return lemma.lemmatize(lemma.lemmatize(word.lower(), pos = \"n\"), pos = \"v\")\n",
    "        \n",
    "        def _get_text(content_string):\n",
    "            # https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "            # https://agailloty.rbind.io/en/project/nlp_clean-text/\n",
    "            content_string = ''.join(c for c in content_string if c in string.printable)\n",
    "            words = re.split(splitter, content_string)\n",
    "            words = [_normalize_word(word) for word in words]\n",
    "            words = [word for word in words if len(word) >= MIN_WORD_LENGTH and len(word) <= MAX_WORD_LENGTH]\n",
    "            words = [word for word in words if not word in stop_words]\n",
    "            return words\n",
    "        \n",
    "        def document_similarity(doc_a, doc_b):\n",
    "            \"\"\"given two sparse arrays of tuples (index in range [0, TOPIC_COUNT], weight in range [0., 1.]) , how equal are they?\"\"\"\n",
    "            dist = 0. # sum of squared distances\n",
    "            # pointers in sparse documents\n",
    "            pa = 0\n",
    "            pb = 0\n",
    "            while pa < len(doc_a) and pb < len(doc_b):\n",
    "                ia, a = doc_a[pa]\n",
    "                ib, b = doc_b[pb]\n",
    "                if ia == ib:\n",
    "                    diff = a - b\n",
    "                    dist += diff * diff\n",
    "                    pa += 1\n",
    "                    pb += 1\n",
    "                elif ia < ib:\n",
    "                    dist += a * a\n",
    "                    pa += 1\n",
    "                elif ib < ia:\n",
    "                    dist += b * b\n",
    "                    pb += 1\n",
    "            while pa < len(doc_a):\n",
    "                a = doc_a[pa][1]\n",
    "                dist += a * a\n",
    "                pa += 1\n",
    "            while pb < len(doc_b):\n",
    "                b = doc_b[pb][1]\n",
    "                dist += b * b\n",
    "                pb += 1\n",
    "            return math.pow(1. - (dist / 2), DOCUMENT_SIMILARITY_EXP)\n",
    "        \n",
    "        \n",
    "        print(\"Extracting words...\")\n",
    "        \n",
    "        # see https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "        freq_dist = FreqDist()\n",
    "        \n",
    "        files = self._get_all_files()\n",
    "        file_words = []  # List of (file,wordList) - tuples\n",
    "        for file in files:\n",
    "            words = _get_text(file.get_content_without_copyright())\n",
    "            file_words.append((file, words))\n",
    "            for word in words:\n",
    "                freq_dist[word] += 1\n",
    "                \n",
    "        for word in freq_dist:\n",
    "            if freq_dist[word] < MIN_WORD_USAGES:\n",
    "                del freq_dist[word]\n",
    "        print(\"Found words:\", len(freq_dist))\n",
    "        print(\"Creating vectorizer...\")\n",
    "        # print([(w, a) for w, a in freq_dist.most_common()][0:MAX_FEATURES])\n",
    "        # [(word, amount) for word, amount in freq_dist.most_common() if word.isdigit()]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        \n",
    "        texts = [words for (file, words) in file_words]\n",
    "        vectorizer = Dictionary(texts)\n",
    "        corpus = [vectorizer.doc2bow(text) for text in texts]\n",
    "        \n",
    "        print(\"Instantiating LDA...\")\n",
    "        \n",
    "        lda = LdaMulticore(num_topics=TOPIC_COUNT,\n",
    "                     id2word=vectorizer,\n",
    "                     iterations=LDA_ITERATIONS,\n",
    "                     passes=LDA_PASSES,\n",
    "                     chunksize=32,\n",
    "                     alpha='asymmetric',\n",
    "                     eta='auto',\n",
    "                     workers=4,\n",
    "                     random_state=LDA_RANDOM_SEED)\n",
    "        \n",
    "        print(\"Training LDA...\")\n",
    "\n",
    "        for _ in log_progress(range(10), desc=\"Updating LDA Model\", smoothing=0.1):\n",
    "            lda.update(corpus)\n",
    "        \n",
    "        print(\"Generating topic output...\")\n",
    "        \n",
    "        for i, topic in lda.show_topics(num_topics=TOPIC_COUNT, formatted=False):\n",
    "            print(f\"Topic #{i}: {' '.join([k for k, v in topic])}\")\n",
    "        \n",
    "        \n",
    "        print(\"Generating doc_top...\")\n",
    "        \n",
    "        doc_top = [lda[doc] for doc in corpus]\n",
    "        \n",
    "        \n",
    "        print(\"Generating coupling graph...\")\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        # debug_list = [] # (sim, f1, f2)\n",
    "        coupling_graph = WeightGraph()\n",
    "        for f1 in range(len(files)):\n",
    "            for f2 in range(len(files)):\n",
    "                if f1 >= f2:\n",
    "                    continue\n",
    "                similarity = document_similarity(doc_top[f1], doc_top[f2])\n",
    "                coupling_graph.add_sym(files[f1].get_path(), files[f2].get_path(), similarity)\n",
    "                # debug_list.append((similarity, f1, f2))\n",
    "                \n",
    "        print(\"Trimming graph...\")\n",
    "        \n",
    "        coupling_graph.cutoff_edges(DOCUMENT_SIMILARITY_CUTOFF)\n",
    "                \n",
    "        \n",
    "        # print(\"Most similar files:\")\n",
    "        # debug_list = sorted(debug_list, key = lambda x: -x[0])\n",
    "        # print([str(sim) + \": \" + files[f1].get_path() + \" <> \" + files[f2].get_path() for sim, f1, f2 in debug_list[0:10]])\n",
    "        \n",
    "        # print(\"Most dissimilar files:\")\n",
    "        # debug_list = sorted(debug_list, key = lambda x: x[0])\n",
    "        # print([str(sim) + \": \" + files[f1].get_path() + \" <> \" + files[f2].get_path() for sim, f1, f2 in debug_list[0:10]])\n",
    "        \n",
    "        return coupling_graph\n",
    "            \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def _get_all_files(self) -> List[RepoFile]:\n",
    "        return [RepoFile(self.repo, o) for o in self.repo.get_file_objects()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricManager:\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear(repo, name):\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            os.remove(WeightGraph.pickle_path(repo.name, name))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get(repo, name) -> WeightGraph:\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            print(\"Using precalculated \" + name + \" values\")\n",
    "            return WeightGraph.load(repo.name, name)\n",
    "        print(\"No precalculated \" + name + \" values found, starting calculations...\")\n",
    "        graph = getattr(MetricsGeneration(repo), \"calculate_\" + name + \"_connections\")()\n",
    "        graph.cleanup()\n",
    "        print(\"Calculated \" + name + \" values, saving them now...\")\n",
    "        graph.save(repo.name, name)\n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def _data_present(repo_name, name):\n",
    "        return os.path.isfile(WeightGraph.pickle_path(repo_name, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
