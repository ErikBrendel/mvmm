{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parsing.ipynb\n",
    "%run util.ipynb\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from typing import List\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, FreqDist\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from multiprocessing import Pool, TimeoutError, Process, Manager, Lock\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_SAVE_PATH = \"../metrics/\"\n",
    "\n",
    "# https://networkx.github.io/documentation/latest/tutorial.html#edges\n",
    "class WeightGraph:\n",
    "    def __init__(self):\n",
    "        self.g = nx.Graph()\n",
    "        \n",
    "    def add(self, a, b, delta):\n",
    "        self.g.add_node(a)\n",
    "        self.g.add_node(b)\n",
    "        new_value = self.get(a, b) + delta\n",
    "        self.g.add_edge(a, b, weight=new_value)\n",
    "        \n",
    "    def add_sym(self, a, b, delta):\n",
    "        self.add(a, b, delta)\n",
    "        self.add(b, a, delta)\n",
    "        \n",
    "    def get(self, a, b):\n",
    "        if b in self.g.adj[a]:\n",
    "            return self.g.adj[a][b][\"weight\"]\n",
    "        return 0\n",
    "    \n",
    "    def cutoff_edges(self, minimum_weight):\n",
    "        fedges = [(a, b) for a, b, info in self.g.edges.data() if info[\"weight\"] < minimum_weight]\n",
    "        self.g.remove_edges_from(fedges)\n",
    "        \n",
    "    def cleanup(self):\n",
    "        # self.g.remove_nodes_from(list(nx.isolates(self.g)))\n",
    "        for component in list(nx.connected_components(self.g)):\n",
    "            if len(component) < 5:\n",
    "                for node in component:\n",
    "                    self.g.remove_node(node)\n",
    "        \n",
    "    def save(self, repo_name, name):\n",
    "        os.makedirs(METRICS_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        nx.write_gpickle(self.g, WeightGraph.pickle_path(repo_name, name))\n",
    "        \n",
    "    def get_max_weight(self):\n",
    "        return max([self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges])\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(repo_name, name):\n",
    "        wg = WeightGraph()\n",
    "        wg.g = nx.read_gpickle(WeightGraph.pickle_path(repo_name, name))\n",
    "        return wg\n",
    "        \n",
    "    @staticmethod\n",
    "    def pickle_path(repo_name, name):\n",
    "        # see https://networkx.github.io/documentation/stable/reference/readwrite/gpickle.html\n",
    "        return METRICS_SAVE_PATH + repo_name + \"/\" + name + \".gpickle\"\n",
    "    \n",
    "    def json_save(self, repo_name, name):\n",
    "        data = json_graph.node_link_data(self.g)\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + name + \".json\", 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            \n",
    "    def html_save(self, repo_name, name):\n",
    "        data = json.dumps(json_graph.node_link_data(self.g))\n",
    "        content = '<html><body><script type=\"text/javascript\">const graph = ' + data + ';</script><script src=\"/files/metrics/html_app.js?_xsrf=2%7Ce163cb61%7Cb9245804a283415ecb4c641f0cf1f882%7C1601372106\"></script></body></html>'\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + name + \".html\", 'w') as outfile:\n",
    "            outfile.write(content)\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        # https://networkx.github.io/documentation/latest/tutorial.html#analyzing-graphs\n",
    "        node_count = len(self.g.nodes)\n",
    "        edge_count = len(self.g.edges)\n",
    "        cc = list(nx.connected_components(self.g))\n",
    "        print(\"WeightGraph statistics: \"\n",
    "              + str(node_count) + \" nodes, \"\n",
    "              + str(edge_count) + \" edges, \"\n",
    "              + str(len(cc)) + \" connected component(s), with sizes: [\"\n",
    "              + \", \".join([str(len(c)) for c in cc])\n",
    "              + \"]\")\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        edge_weights.sort()\n",
    "        print(\"Edge weights:\", edge_weights[0:5], \"...\", edge_weights[-5:], \", mean:\", np.array(edge_weights).mean())\n",
    "    \n",
    "    def show_weight_histogram(self):\n",
    "        # https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "        # import pdb; pdb.set_trace()  # debugger\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        plt.hist(edge_weights, \"auto\", facecolor='b', alpha=0.75)\n",
    "        plt.axvline(np.array(edge_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of edge weights in coupling graph')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "        node_weights = [sum([self.g[n][n2][\"weight\"] for n2 in self.g.adj[n]]) for n in self.g.nodes]\n",
    "        plt.hist(node_weights, \"auto\", facecolor='g', alpha=0.75)\n",
    "        plt.axvline(np.array(node_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        # plt.xscale(\"log\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of node weights')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize(self, use_spring = False):\n",
    "        # https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
    "        \n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        max_weight = max(edge_weights)\n",
    "        mean_weight = np.array(edge_weights).mean()\n",
    "        target_max_weight = min(max_weight, mean_weight * 2)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        VIZ_POW = 1\n",
    "        max_w_fact = (1. / target_max_weight) ** VIZ_POW\n",
    "        \n",
    "        draw_func = nx.draw_kamada_kawai if use_spring else nx.draw\n",
    "        \n",
    "        # nx.draw_kamada_kawai(self.g, alpha=0.2, node_size=100)\n",
    "        # nx.draw(self.g, alpha=0.2, node_size=100)\n",
    "        edge_colors = [(0., 0., 0., min(1., (self.g[a][b][\"weight\"] ** VIZ_POW) * max_w_fact)) for a, b in self.g.edges]\n",
    "        draw_func(self.g, node_size=50, edge_color=edge_colors, node_color=[(0.121, 0.469, 0.703, 0.2)])\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COMMIT_FILES = 50\n",
    "# needs to be separate so that multiprocessing lib can find it\n",
    "def get_commit_diff(commit_hash, repo):\n",
    "    c1 = repo.get_commit(commit_hash)\n",
    "    if len(c1.parents) == 1:\n",
    "        c2 = c1.parents[0]\n",
    "        diff = c1.diff(c2)\n",
    "        diffs = [ d.a_path for d in diff ]\n",
    "    elif len(c1.parents) == 2:\n",
    "        c2 = c1.parents[0]\n",
    "        diff_1 = c1.diff(c2)\n",
    "        c3 = c1.parents[1]\n",
    "        diff_2 = c1.diff(c3)\n",
    "\n",
    "        diffs_1 = [ d.a_path for d in diff_1 ]\n",
    "        diffs_2 = [ d.a_path for d in diff_2 ]\n",
    "        diffs = set(diffs_1).intersection(set(diffs_2))\n",
    "    else:\n",
    "        return None\n",
    "    if len(diffs) > MAX_COMMIT_FILES or len(diffs) <= 1:\n",
    "        return None\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsGeneration:\n",
    "    # ascii art: http://patorjk.com/software/taag/#p=display&f=Soft&t=STRUCTURAL%0A.%0ALINGUISTIC%0A.%0AEVOLUTIONARY%0A.%0ADYNAMIC\n",
    "    def __init__(self, repo):\n",
    "        self.repo = repo\n",
    "    \n",
    "    def calculate_structural_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    " ,---. ,--------.,------. ,--. ,--. ,-----.,--------.,--. ,--.,------.   ,---.  ,--.                     \n",
    "'   .-''--.  .--'|  .--. '|  | |  |'  .--./'--.  .--'|  | |  ||  .--. ' /  O  \\ |  |                     \n",
    "`.  `-.   |  |   |  '--'.'|  | |  ||  |       |  |   |  | |  ||  '--'.'|  .-.  ||  |                     \n",
    ".-'    |  |  |   |  |\\  \\ '  '-'  ''  '--'\\   |  |   '  '-'  '|  |\\  \\ |  | |  ||  '--.                  \n",
    "`-----'   `--'   `--' '--' `-----'  `-----'   `--'    `-----' `--' '--'`--' `--'`-----'   \n",
    "        \"\"\"\n",
    "        coupling_graph = WeightGraph()\n",
    "\n",
    "        error_query = JA_LANGUAGE.query(\"(ERROR) @err\")\n",
    "        package_query_1 = JA_LANGUAGE.query(\"(package_declaration (identifier) @decl)\")\n",
    "        package_query_2 = JA_LANGUAGE.query(\"(package_declaration (scoped_identifier) @decl)\")\n",
    "        import_query = JA_LANGUAGE.query(\"(import_declaration (scoped_identifier) @decl)\")\n",
    "        class_query = JA_LANGUAGE.query(\"(class_declaration name: (identifier) @decl)\")\n",
    "\n",
    "\n",
    "        def _has_error(file) -> List[str]:\n",
    "            errors = error_query.captures(file.get_tree().root_node)\n",
    "            return len(errors) > 1\n",
    "\n",
    "\n",
    "        def _get_package(file) -> List[str]:\n",
    "            packages = package_query_1.captures(file.get_tree().root_node) + package_query_2.captures(file.get_tree().root_node)\n",
    "            # assert len(packages) <= 1\n",
    "            if len(packages) > 1:\n",
    "                import pdb; pdb.set_trace()\n",
    "            if len(packages) == 1:\n",
    "                return file.node_text(packages[0][0]).split(\".\")\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def _get_imports(file) -> List[str]:\n",
    "            imports = import_query.captures(file.get_tree().root_node)\n",
    "            result = []\n",
    "            for import_statement in imports:\n",
    "                import_string = file.node_text(import_statement[0])\n",
    "                if not import_string.startswith(\"java\"):\n",
    "                    result.append(import_string)\n",
    "            return result\n",
    "\n",
    "        def _get_main_class_name(file) -> List[str]:\n",
    "            classes = class_query.captures(file.get_tree().root_node)\n",
    "            if len(classes) >= 1:\n",
    "                return file.node_text(classes[0][0])\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        def _mark_connected(a, b):\n",
    "            coupling_graph.add(a, b, 1)\n",
    "\n",
    "        #######\n",
    "\n",
    "        full_class_name_to_id = {}\n",
    "\n",
    "        files = self._get_all_files()\n",
    "        for file in files:\n",
    "            if _has_error(file):\n",
    "                continue\n",
    "            class_name = _get_main_class_name(file)\n",
    "            if class_name is not None:\n",
    "                full_class_name = \".\".join(_get_package(file) + [class_name])\n",
    "                full_class_name_to_id[full_class_name] = file.get_path()\n",
    "\n",
    "        for file in files:\n",
    "            imports = _get_imports(file)\n",
    "            for i in imports:\n",
    "                if i in full_class_name_to_id:\n",
    "                    # print(\"import RESOLVED: \" + i)\n",
    "                    _mark_connected(file.get_path(), full_class_name_to_id[i])\n",
    "                else:\n",
    "                    pass # print(\"cannot resolve import: \" + i)\n",
    "\n",
    "        return coupling_graph\n",
    "    \n",
    "    \n",
    "    def calculate_linguistic_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",--.   ,--.,--.  ,--. ,----.   ,--. ,--.,--. ,---. ,--------.,--. ,-----.                                \n",
    "|  |   |  ||  ,'.|  |'  .-./   |  | |  ||  |'   .-''--.  .--'|  |'  .--./                                \n",
    "|  |   |  ||  |' '  ||  | .---.|  | |  ||  |`.  `-.   |  |   |  ||  |                                    \n",
    "|  '--.|  ||  | `   |'  '--'  |'  '-'  '|  |.-'    |  |  |   |  |'  '--'\\                                \n",
    "`-----'`--'`--'  `--' `------'  `-----' `--'`-----'   `--'   `--' `-----'              \n",
    "        \"\"\"\n",
    "        # constants\n",
    "        MIN_WORD_LENGTH = 3\n",
    "        MAX_WORD_LENGTH = 50\n",
    "        MIN_WORD_USAGES = 2  # any word used less often will be ignored\n",
    "        MAX_DF = 0.95  # any terms that appear in a bigger proportion of the documents than this will be ignored (corpus-specific stop-words)\n",
    "        MAX_FEATURES = 1500  # the size of the LDA thesaurus - amount of words to consider for topic learning\n",
    "        TOPIC_COUNT = 10 # 40  # 100 according to paper\n",
    "        LDA_ITERATIONS = 1000  # 3.000 according to paper, but at least 500, but we are using online learning, they did not\n",
    "        LDA_RANDOM_SEED = 42\n",
    "        DOCUMENT_SIMILARITY_EXP = 4 # higher = lower equality values, lower = equality values are all closer to 1\n",
    "        DOCUMENT_SIMILARITY_CUTOFF = 0.8  # in range [0 .. 2]: everything below this is dropped\n",
    "        \n",
    "        \n",
    "        # keywords from python, TS and Java\n",
    "        custom_stop_words = [\"abstract\", \"and\", \"any\", \"as\", \"assert\", \"async\", \"await\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\", \"class\", \"const\", \"constructor\", \"continue\", \"debugger\", \"declare\", \"def\", \"default\", \"del\", \"delete\", \"do\", \"double\", \"elif\", \"else\", \"enum\", \"except\", \"export\", \"extends\", \"false\", \"False\", \"final\", \"finally\", \"float\", \"for\", \"from\", \"function\", \"get\", \"global\", \"goto\", \"if\", \"implements\", \"import\", \"in\", \"instanceof\", \"int\", \"interface\", \"is\", \"lambda\", \"let\", \"long\", \"module\", \"new\", \"None\", \"nonlocal\", \"not\", \"null\", \"number\", \"of\", \"or\", \"package\", \"pass\", \"private\", \"protected\", \"public\", \"raise\", \"require\", \"return\", \"set\", \"short\", \"static\", \"strictfp\", \"string\", \"super\", \"switch\", \"symbol\", \"synchronized\", \"this\", \"throw\", \"throws\", \"transient\", \"true\", \"True\", \"try\", \"type\", \"typeof\", \"var\", \"void\", \"volatile\", \"while\", \"with\", \"yield\"]\n",
    "        stop_words = set(list(get_stop_words('en')) + list(stopwords.words('english')) + custom_stop_words)\n",
    "        splitter = r\"(?:[\\W_]+|(?<![A-Z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z]))\"\n",
    "        lemma = WordNetLemmatizer()\n",
    "        printable_characters = set(string.printable)\n",
    "        \n",
    "        def _normalize_word(word):\n",
    "            return lemma.lemmatize(lemma.lemmatize(word.lower(), pos = \"n\"), pos = \"v\")\n",
    "        \n",
    "        def _get_text(content_string):\n",
    "            # https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "            # https://agailloty.rbind.io/en/project/nlp_clean-text/\n",
    "            content_string = ''.join(c for c in content_string if c in string.printable)\n",
    "            words = re.split(splitter, content_string)\n",
    "            words = [_normalize_word(word) for word in words]\n",
    "            words = [word for word in words if len(word) >= MIN_WORD_LENGTH and len(word) <= MAX_WORD_LENGTH]\n",
    "            words = [word for word in words if not word in stop_words]\n",
    "            return words\n",
    "        \n",
    "        def document_similarity(doc_a, doc_b):\n",
    "            \"\"\"given two arrays of numbers in range [0., 1.] and length TOPIC_COUNT, how equal are they?\"\"\"\n",
    "            dist = 0. # sum of squared distances\n",
    "            for a, b in zip(doc_a, doc_b):\n",
    "                diff = a - b\n",
    "                dist += diff * diff\n",
    "            return math.pow(1. - (dist / 2), DOCUMENT_SIMILARITY_EXP)\n",
    "        \n",
    "        \n",
    "        print(\"Extracting words...\")\n",
    "        \n",
    "        # see https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "        freq_dist = FreqDist()\n",
    "        \n",
    "        files = self._get_all_files()\n",
    "        file_words = []  # List of (file,wordList) - tuples\n",
    "        for file in files:\n",
    "            words = _get_text(file.get_content_without_copyright())\n",
    "            file_words.append((file, words))\n",
    "            for word in words:\n",
    "                freq_dist[word] += 1\n",
    "                \n",
    "        for word in freq_dist:\n",
    "            if freq_dist[word] < MIN_WORD_USAGES:\n",
    "                del freq_dist[word]\n",
    "        print(\"Found words:\", len(freq_dist))\n",
    "        print(\"Creating vectorizer...\")\n",
    "        # print([(w, a) for w, a in freq_dist.most_common()][0:MAX_FEATURES])\n",
    "        # [(word, amount) for word, amount in freq_dist.most_common() if word.isdigit()]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        \n",
    "        # see https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "        tf_vectorizer = CountVectorizer(max_df=MAX_DF, min_df=MIN_WORD_USAGES,\n",
    "                                        max_features=MAX_FEATURES)\n",
    "        \n",
    "        tf = tf_vectorizer.fit_transform([' '.join(words) for file, words in file_words])\n",
    "        \n",
    "        print(\"Training LDA...\")\n",
    "        lda = LatentDirichletAllocation(n_components=TOPIC_COUNT, max_iter=LDA_ITERATIONS,\n",
    "                                        learning_method='online',\n",
    "                                        learning_offset=64.,  # offset and batch size values copied from online learning LDA paper:\n",
    "                                        batch_size=256,  # https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf\n",
    "                                        n_jobs=8,  # seems to be the fastest, empirically tested\n",
    "                                        random_state=LDA_RANDOM_SEED)\n",
    "        doc_top = lda.fit_transform(tf)\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "        # doc_top[documentId][topicId]\n",
    "        # in each document, the sum is 1\n",
    "        # sums of topic across documents are roughly equal magnitude\n",
    "            \n",
    "        \n",
    "        print(\"Generating topic output...\")\n",
    "        \n",
    "        tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "        \n",
    "        def print_top_words(model, feature_names, n_top_words=10):\n",
    "            for topic_idx, topic in enumerate(model.components_):\n",
    "                message = \"Topic #%d: \" % topic_idx\n",
    "                message += \" \".join([feature_names[i]\n",
    "                                     for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "                print(message)\n",
    "            print()\n",
    "        print_top_words(lda, tf_feature_names)\n",
    "        \n",
    "        print(\"Generating coupling graph...\")\n",
    "        \n",
    "        # debug_list = [] # (sim, f1, f2)\n",
    "        coupling_graph = WeightGraph()\n",
    "        for f1 in range(len(files)):\n",
    "            for f2 in range(len(files)):\n",
    "                if f1 >= f2:\n",
    "                    continue\n",
    "                similarity = document_similarity(doc_top[f1], doc_top[f2])\n",
    "                coupling_graph.add_sym(files[f1].get_path(), files[f2].get_path(), similarity)\n",
    "                # debug_list.append((similarity, f1, f2))\n",
    "        \n",
    "        coupling_graph.cutoff_edges(DOCUMENT_SIMILARITY_CUTOFF)\n",
    "                \n",
    "        \n",
    "        # print(\"Most similar files:\")\n",
    "        # debug_list = sorted(debug_list, key = lambda x: -x[0])\n",
    "        # print([str(sim) + \": \" + files[f1].get_path() + \" <> \" + files[f2].get_path() for sim, f1, f2 in debug_list[0:10]])\n",
    "        \n",
    "        # print(\"Most dissimilar files:\")\n",
    "        # debug_list = sorted(debug_list, key = lambda x: x[0])\n",
    "        # print([str(sim) + \": \" + files[f1].get_path() + \" <> \" + files[f2].get_path() for sim, f1, f2 in debug_list[0:10]])\n",
    "        \n",
    "        return coupling_graph\n",
    "            \n",
    "        \n",
    "    def calculate_evolutionary_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",------.,--.   ,--.,-----. ,--.   ,--. ,--.,--------.,--. ,-----. ,--.  ,--.  ,---.  ,------.,--.   ,--. \n",
    "|  .---' \\  `.'  /'  .-.  '|  |   |  | |  |'--.  .--'|  |'  .-.  '|  ,'.|  | /  O  \\ |  .--. '\\  `.'  /  \n",
    "|  `--,   \\     / |  | |  ||  |   |  | |  |   |  |   |  ||  | |  ||  |' '  ||  .-.  ||  '--'.' '.    /   \n",
    "|  `---.   \\   /  '  '-'  '|  '--.'  '-'  '   |  |   |  |'  '-'  '|  | `   ||  | |  ||  |\\  \\    |  |    \n",
    "`------'    `-'    `-----' `-----' `-----'    `--'   `--' `-----' `--'  `--'`--' `--'`--' '--'   `--'    \n",
    "        \"\"\"\n",
    "        # MAX_COMMIT_FILES = 50  # Ignore too large commits. (constant moved)\n",
    "        PARALLEL_THREADS = 45  # more seems to make it worse again?\n",
    "        PARALLEL_BATCH_SIZE = 2  # the size of packets delivered to worker processes\n",
    "        \n",
    "        coupling_graph = WeightGraph()\n",
    "        # graph_lock = Lock()\n",
    "        \n",
    "        def processDiffs(diffs):\n",
    "            score = 2 / len(diffs)\n",
    "            # with graph_lock:\n",
    "            for f1 in diffs:\n",
    "                for f2 in diffs:\n",
    "                    if f1 is not f2:\n",
    "                        coupling_graph.add_sym(f1, f2, score)\n",
    "        \n",
    "        all_commits = list(self.repo.get_all_commits())\n",
    "        print(\"Commits to analyze: \" + str(len(all_commits)))\n",
    "\n",
    "        with Pool(processes=PARALLEL_THREADS) as pool:\n",
    "            bar = log_progress(total=len(all_commits), desc=\"Analyzing commits\", smoothing=0.1)\n",
    "            diffs = pool.imap_unordered(partial(get_commit_diff, repo=self.repo), all_commits, PARALLEL_BATCH_SIZE)\n",
    "            # single-threaded alternative for debugging:\n",
    "            #diffs = []\n",
    "            #for i, t in enumerate(all_commits):\n",
    "            #    diffs.append(partial(get_commit_diff, repo=self.repo)(t))\n",
    "            #    print(str(i / float(len(all_commits)) * 100.) + \" % done.\")\n",
    "\n",
    "            for i, elem in enumerate(diffs):\n",
    "                if elem is not None:\n",
    "                    processDiffs(elem)\n",
    "                    # print(str(i / float(len(all_commits)) * 100.) + \" % done.\")\n",
    "                bar.update()\n",
    "        \n",
    "            bar.close()\n",
    "        coupling_graph.cutoff_edges(3)\n",
    "        return coupling_graph\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def _get_all_files(self) -> List[RepoFile]:\n",
    "        return [RepoFile(self.repo, o) for o in self.repo.get_file_objects()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricManager:\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear(repo, name):\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            os.remove(WeightGraph.pickle_path(repo.name, name))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get(repo, name) -> WeightGraph:\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            print(\"Using precalculated \" + name + \" values\")\n",
    "            return WeightGraph.load(repo.name, name)\n",
    "        print(\"No precalculated \" + name + \" values found, starting calculations...\")\n",
    "        graph = getattr(MetricsGeneration(repo), \"calculate_\" + name + \"_connections\")()\n",
    "        graph.cleanup()\n",
    "        print(\"Calculated \" + name + \" values, saving them now...\")\n",
    "        graph.save(repo.name, name)\n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def _data_present(repo_name, name):\n",
    "        return os.path.isfile(WeightGraph.pickle_path(repo_name, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
