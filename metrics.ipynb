{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parsing.ipynb\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, FreqDist\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.github.io/documentation/latest/tutorial.html#edges\n",
    "class WeightGraph:\n",
    "    def __init__(self):\n",
    "        self.g = nx.Graph()\n",
    "        \n",
    "    def add(self, a, b, delta):\n",
    "        self.g.add_node(a)\n",
    "        self.g.add_node(b)\n",
    "        new_value = self.get(a, b) + delta\n",
    "        self.g.add_edge(a, b, weight=new_value)\n",
    "        \n",
    "    def get(self, a, b):\n",
    "        if b in self.g.adj[a]:\n",
    "            return self.g.adj[a][b][\"weight\"]\n",
    "        return 0\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        # https://networkx.github.io/documentation/latest/tutorial.html#analyzing-graphs\n",
    "        node_count = len(self.g.nodes)\n",
    "        edge_count = len(self.g.edges)\n",
    "        cc = list(nx.connected_components(self.g))\n",
    "        print(\"WeightGraph statistics: \"\n",
    "              + str(node_count) + \" nodes, \"\n",
    "              + str(edge_count) + \" edges, \"\n",
    "              + str(len(cc)) + \" connected component(s), with sizes: [\"\n",
    "              + \", \".join([str(len(c)) for c in cc])\n",
    "              + \"]\")\n",
    "    \n",
    "    def show_weight_histogram(self):\n",
    "        # https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "        # import pdb; pdb.set_trace()  # debugger\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        plt.hist(edge_weights, \"auto\", facecolor='b', alpha=0.75)\n",
    "        plt.axvline(np.array(edge_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of edge weights in coupling graph')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "        node_weights = [sum([self.g[n][n2][\"weight\"] for n2 in self.g.adj[n]]) for n in self.g.nodes]\n",
    "        plt.hist(node_weights, \"auto\", facecolor='g', alpha=0.75)\n",
    "        plt.axvline(np.array(node_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        # plt.xscale(\"log\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of node weights')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize(self):\n",
    "        # https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        \n",
    "        nx.draw_kamada_kawai(self.g, alpha=0.2, node_size=100)\n",
    "        #nx.draw(self.g, alpha=0.2, node_size=100)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepoFile:\n",
    "    def __init__(self, repo, file_obj):\n",
    "        self.repo = repo\n",
    "        self.file_obj = file_obj\n",
    "        self.content = None\n",
    "        self.tree = None\n",
    "        \n",
    "    def get_path(self):\n",
    "        return self.file_obj.path\n",
    "    \n",
    "    def get_content(self):\n",
    "        if self.content is None:\n",
    "            self.content = self.repo.get_file_object_content(self.file_obj)\n",
    "        return self.content\n",
    "    \n",
    "    def get_content_without_copyright(self):\n",
    "        tree = self.get_tree()\n",
    "        first_root_child = tree.root_node.children[0]\n",
    "        if first_root_child.type == \"comment\":\n",
    "            return self.get_content()[first_root_child.end_byte:].decode(\"utf-8\")\n",
    "        else:\n",
    "            return self.get_content().decode(\"utf-8\")\n",
    "    \n",
    "    def get_tree(self):\n",
    "        if self.tree is None:\n",
    "            self.tree = java_parser.parse(self.get_content())\n",
    "        return self.tree\n",
    "    \n",
    "    def node_text(self, node):\n",
    "        return self.content[node.start_byte:node.end_byte].decode(\"utf-8\")\n",
    "    \n",
    "    def walk_tree(self, node_handler):\n",
    "        \"\"\" node_handler gets the current logic-path and node for each ast node\"\"\"\n",
    "        self.walk_tree_cursor(self.get_tree().walk(), self.get_path(), node_handler)\n",
    "    \n",
    "    def walk_tree_cursor(self, cursor, prefix, node_handler):\n",
    "        if not cursor.node.is_named:\n",
    "            return\n",
    "        node_handler(prefix, cursor.node)\n",
    "        # cursor.current_field_name() is the role that this node has in its parent\n",
    "        tree_node_name = None\n",
    "        if cursor.node.type == \"class_declaration\":\n",
    "            idfield = cursor.node.child_by_field_name(\"name\")\n",
    "            tree_node_name = self.node_text(idfield)\n",
    "        elif cursor.node.type == \"field_declaration\":\n",
    "            idfield = cursor.node.child_by_field_name(\"declarator\").child_by_field_name(\"name\")\n",
    "            tree_node_name = self.node_text(idfield)\n",
    "        elif cursor.node.type == \"method_declaration\":\n",
    "            idfield = cursor.node.child_by_field_name(\"name\")\n",
    "            tree_node_name = self.node_text(idfield)\n",
    "\n",
    "        if tree_node_name is not None:\n",
    "            prefix = prefix + \"/\" + tree_node_name\n",
    "            # found_nodes.register(prefix)\n",
    "\n",
    "        if cursor.goto_first_child():\n",
    "            self.walk_tree_cursor(cursor, prefix, node_handler)\n",
    "            while cursor.goto_next_sibling():\n",
    "                self.walk_tree_cursor(cursor, prefix, node_handler)\n",
    "            cursor.goto_parent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsGeneration:\n",
    "    # ascii art: http://patorjk.com/software/taag/#p=display&f=Soft&t=STRUCTURAL%0A.%0ALINGUISTIC%0A.%0AEVOLUTIONARY%0A.%0ADYNAMIC\n",
    "    def __init__(self, repo):\n",
    "        self.repo = repo\n",
    "    \n",
    "    def calculate_structural_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    " ,---. ,--------.,------. ,--. ,--. ,-----.,--------.,--. ,--.,------.   ,---.  ,--.                     \n",
    "'   .-''--.  .--'|  .--. '|  | |  |'  .--./'--.  .--'|  | |  ||  .--. ' /  O  \\ |  |                     \n",
    "`.  `-.   |  |   |  '--'.'|  | |  ||  |       |  |   |  | |  ||  '--'.'|  .-.  ||  |                     \n",
    ".-'    |  |  |   |  |\\  \\ '  '-'  ''  '--'\\   |  |   '  '-'  '|  |\\  \\ |  | |  ||  '--.                  \n",
    "`-----'   `--'   `--' '--' `-----'  `-----'   `--'    `-----' `--' '--'`--' `--'`-----'   \n",
    "        \"\"\"\n",
    "        coupling_graph = WeightGraph()\n",
    "\n",
    "        package_query_1 = JA_LANGUAGE.query(\"(package_declaration (identifier) @decl)\")\n",
    "        package_query_2 = JA_LANGUAGE.query(\"(package_declaration (scoped_identifier) @decl)\")\n",
    "        import_query = JA_LANGUAGE.query(\"(import_declaration (scoped_identifier) @decl)\")\n",
    "        class_query = JA_LANGUAGE.query(\"(class_declaration name: (identifier) @decl)\")\n",
    "\n",
    "\n",
    "        def _get_package(file) -> List[str]:\n",
    "            packages = package_query_1.captures(file.get_tree().root_node) + package_query_2.captures(file.get_tree().root_node)\n",
    "            assert len(packages) <= 1\n",
    "            if len(packages) == 1:\n",
    "                return file.node_text(packages[0][0]).split(\".\")\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def _get_imports(file) -> List[str]:\n",
    "            imports = import_query.captures(file.get_tree().root_node)\n",
    "            result = []\n",
    "            for import_statement in imports:\n",
    "                import_string = file.node_text(import_statement[0])\n",
    "                if not import_string.startswith(\"java\"):\n",
    "                    result.append(import_string)\n",
    "            return result\n",
    "\n",
    "        def _get_main_class_name(file) -> List[str]:\n",
    "            classes = class_query.captures(file.get_tree().root_node)\n",
    "            if len(classes) >= 1:\n",
    "                return file.node_text(classes[0][0])\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        def _mark_connected(a, b):\n",
    "            coupling_graph.add(a, b, 1)\n",
    "\n",
    "        #######\n",
    "\n",
    "        full_class_name_to_id = {}\n",
    "\n",
    "        files = self._get_all_files()\n",
    "        for file in files:\n",
    "            class_name = _get_main_class_name(file)\n",
    "            if class_name is not None:\n",
    "                full_class_name = \".\".join(_get_package(file) + [class_name])\n",
    "                full_class_name_to_id[full_class_name] = file.get_path()\n",
    "\n",
    "        for file in files:\n",
    "            imports = _get_imports(file)\n",
    "            for i in imports:\n",
    "                if i in full_class_name_to_id:\n",
    "                    # print(\"import RESOLVED: \" + i)\n",
    "                    _mark_connected(file.get_path(), full_class_name_to_id[i])\n",
    "                else:\n",
    "                    pass # print(\"cannot resolve import: \" + i)\n",
    "\n",
    "        coupling_graph.print_statistics()\n",
    "        # coupling_graph.show_weight_histogram()\n",
    "        coupling_graph.visualize()\n",
    "        return coupling_graph\n",
    "    \n",
    "    \n",
    "    def calculate_linguistic_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",--.   ,--.,--.  ,--. ,----.   ,--. ,--.,--. ,---. ,--------.,--. ,-----.                                \n",
    "|  |   |  ||  ,'.|  |'  .-./   |  | |  ||  |'   .-''--.  .--'|  |'  .--./                                \n",
    "|  |   |  ||  |' '  ||  | .---.|  | |  ||  |`.  `-.   |  |   |  ||  |                                    \n",
    "|  '--.|  ||  | `   |'  '--'  |'  '-'  '|  |.-'    |  |  |   |  |'  '--'\\                                \n",
    "`-----'`--'`--'  `--' `------'  `-----' `--'`-----'   `--'   `--' `-----'              \n",
    "        \"\"\"\n",
    "        # constants\n",
    "        MIN_WORD_LENGTH = 3\n",
    "        MAX_WORD_LENGTH = 50\n",
    "        MIN_WORD_USAGES = 2  # any word used less often will be ignored\n",
    "        MAX_DF = 0.95  # any terms that appear in a bigger proportion of the documents than this will be ignored (corpus-specific stop-words)\n",
    "        MAX_FEATURES = 1000  # the size of the LDA thesaurus - amount of words to consider for topic learning\n",
    "        TOPIC_COUNT = 20  # 100 according to paper\n",
    "        LDA_ITERATIONS = 10  # 3.000 according to paper, but at least 500\n",
    "        LDA_RANDOM_SEED = 42\n",
    "        \n",
    "        \n",
    "        # keywords from python, TS and Java\n",
    "        custom_stop_words = [\"abstract\", \"and\", \"any\", \"as\", \"assert\", \"async\", \"await\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\", \"class\", \"const\", \"constructor\", \"continue\", \"debugger\", \"declare\", \"def\", \"default\", \"del\", \"delete\", \"do\", \"double\", \"elif\", \"else\", \"enum\", \"except\", \"export\", \"extends\", \"false\", \"False\", \"final\", \"finally\", \"float\", \"for\", \"from\", \"function\", \"get\", \"global\", \"goto\", \"if\", \"implements\", \"import\", \"in\", \"instanceof\", \"int\", \"interface\", \"is\", \"lambda\", \"let\", \"long\", \"module\", \"new\", \"None\", \"nonlocal\", \"not\", \"null\", \"number\", \"of\", \"or\", \"package\", \"pass\", \"private\", \"protected\", \"public\", \"raise\", \"require\", \"return\", \"set\", \"short\", \"static\", \"strictfp\", \"string\", \"super\", \"switch\", \"symbol\", \"synchronized\", \"this\", \"throw\", \"throws\", \"transient\", \"true\", \"True\", \"try\", \"type\", \"typeof\", \"var\", \"void\", \"volatile\", \"while\", \"with\", \"yield\"]\n",
    "        stop_words = set(list(get_stop_words('en')) + list(stopwords.words('english')) + custom_stop_words)\n",
    "        splitter = r\"(?:[\\W_]+|(?<![A-Z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z]))\"\n",
    "        lemma = WordNetLemmatizer()\n",
    "        printable_characters = set(string.printable)\n",
    "        \n",
    "        def _normalize_word(word):\n",
    "            return lemma.lemmatize(lemma.lemmatize(word.lower(), pos = \"n\"), pos = \"v\")\n",
    "        \n",
    "        def _get_text(content_string):\n",
    "            # https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "            # https://agailloty.rbind.io/en/project/nlp_clean-text/\n",
    "            content_string = ''.join(c for c in content_string if c in string.printable)\n",
    "            words = re.split(splitter, content_string)\n",
    "            words = [_normalize_word(word) for word in words]\n",
    "            words = [word for word in words if len(word) >= MIN_WORD_LENGTH and len(word) <= MAX_WORD_LENGTH]\n",
    "            words = [word for word in words if not word in stop_words]\n",
    "            return words\n",
    "        \n",
    "        \n",
    "        print(\"Extracting words...\")\n",
    "        \n",
    "        # see https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "        freq_dist = FreqDist()\n",
    "        \n",
    "        files = self._get_all_files()\n",
    "        file_words = []  # List of (file,wordList) - tuples\n",
    "        for file in files:\n",
    "            words = _get_text(file.get_content_without_copyright())\n",
    "            file_words.append((file, words))\n",
    "            for word in words:\n",
    "                freq_dist[word] += 1\n",
    "                \n",
    "        for word in freq_dist:\n",
    "            if freq_dist[word] < MIN_WORD_USAGES:\n",
    "                del freq_dist[word]\n",
    "        print(\"Found words:\", len(freq_dist))\n",
    "        print(\"Creating vectorizer...\")\n",
    "        # print([(w, a) for w, a in freq_dist.most_common()][0:MAX_FEATURES])\n",
    "        # [(word, amount) for word, amount in freq_dist.most_common() if word.isdigit()]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        \n",
    "        # see https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "        tf_vectorizer = CountVectorizer(max_df=MAX_DF, min_df=MIN_WORD_USAGES,\n",
    "                                        max_features=MAX_FEATURES)\n",
    "        \n",
    "        tf = tf_vectorizer.fit_transform([' '.join(words) for file, words in file_words])\n",
    "        \n",
    "        lda = LatentDirichletAllocation(n_components=TOPIC_COUNT, max_iter=LDA_ITERATIONS,\n",
    "                                        learning_method='online',\n",
    "                                        learning_offset=50.,  # dafuq does this do?\n",
    "                                        random_state=LDA_RANDOM_SEED)\n",
    "        print(\"Training LDA...\")\n",
    "        lda.fit(tf)\n",
    "        print(\"Generating result output...\")\n",
    "        \n",
    "        tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "        \n",
    "        def print_top_words(model, feature_names, n_top_words=10):\n",
    "            for topic_idx, topic in enumerate(model.components_):\n",
    "                message = \"Topic #%d: \" % topic_idx\n",
    "                message += \" \".join([feature_names[i]\n",
    "                                     for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "                print(message)\n",
    "            print()\n",
    "        print_top_words(lda, tf_feature_names)\n",
    "            \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def _get_all_files(self) -> List[RepoFile]:\n",
    "        return [RepoFile(self.repo, o) for o in self.repo.get_file_objects()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
