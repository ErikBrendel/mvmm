{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parsing.ipynb\n",
    "%run util.ipynb\n",
    "%run LocalRepo.ipynb\n",
    "%run structural_parsing.ipynb\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from typing import List\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, FreqDist\n",
    "import string\n",
    "from random import shuffle\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import similarities\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from multiprocessing import Pool, TimeoutError, Process, Manager, Lock\n",
    "from functools import partial\n",
    "import pdb\n",
    "from biterm.cbtm import oBTM \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary # helper functions\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_SAVE_PATH = \"../metrics/\"\n",
    "EXPORT_SAVE_PATH = \"../export/\"\n",
    "\n",
    "# https://networkx.github.io/documentation/latest/tutorial.html#edges\n",
    "class WeightGraph:\n",
    "    def __init__(self, view_name):\n",
    "        self.g = nx.Graph()\n",
    "        self.view_name = view_name\n",
    "        \n",
    "    def add(self, a, b, delta):\n",
    "        #self.g.add_node(a)\n",
    "        #self.g.add_node(b)\n",
    "        new_value = self.get(a, b) + delta\n",
    "        self.g.add_edge(a, b, weight=new_value)\n",
    "        \n",
    "    def get(self, a, b):\n",
    "        if a in self.g and b in self.g.adj[a]:\n",
    "            return self.g.adj[a][b][\"weight\"]\n",
    "        return 0\n",
    "    \n",
    "    def add_support(self, node, delta):\n",
    "        if not node in self.g.nodes:\n",
    "            self.g.add_node(node)\n",
    "        self.g.nodes[node][\"support\"] = self.get_support(node) + delta\n",
    "        \n",
    "    def get_support(self, node):\n",
    "        #if node in self.g.nodes and \"support\" in self.g.nodes[node]:\n",
    "        return self.g.nodes.get(node, {}).get(\"support\", 0);\n",
    "        #else:\n",
    "        #    return 0\n",
    "    \n",
    "    def add_and_support(self, a, b, delta):\n",
    "        self.add(a, b, delta)\n",
    "        self.add_support(a, delta)\n",
    "        self.add_support(b, delta)\n",
    "    \n",
    "    def cutoff_edges(self, minimum_weight):\n",
    "        fedges = [(a, b) for a, b, info in self.g.edges.data() if info[\"weight\"] < minimum_weight]\n",
    "        self.g.remove_edges_from(fedges)\n",
    "        \n",
    "    def cleanup(self):\n",
    "        # self.g.remove_nodes_from(list(nx.isolates(self.g)))\n",
    "        for component in list(nx.connected_components(self.g)):\n",
    "            if len(component) < 5:\n",
    "                for node in component:\n",
    "                    self.g.remove_node(node)\n",
    "        \n",
    "    def save(self, repo_name):\n",
    "        os.makedirs(METRICS_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        nx.write_gpickle(self.g, WeightGraph.pickle_path(repo_name, self.view_name))\n",
    "        \n",
    "    def get_max_weight(self):\n",
    "        return max([self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges])\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(repo_name, name):\n",
    "        wg = WeightGraph(name)\n",
    "        wg.g = nx.read_gpickle(WeightGraph.pickle_path(repo_name, name))\n",
    "        return wg\n",
    "        \n",
    "    @staticmethod\n",
    "    def pickle_path(repo_name, name):\n",
    "        # see https://networkx.github.io/documentation/stable/reference/readwrite/gpickle.html\n",
    "        return METRICS_SAVE_PATH + repo_name + \"/\" + name + \".gpickle\"\n",
    "    \n",
    "    def json_save(self, repo_name):\n",
    "        data = json_graph.node_link_data(self.g)\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + self.view_name + \".json\", 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            \n",
    "    def html_save(self, repo_name):\n",
    "        data = json.dumps(json_graph.node_link_data(self.g))\n",
    "        content = '<html><body><script type=\"text/javascript\">const graph = ' + data + ';</script><script src=\"/files/metrics/html_app.js?_xsrf=2%7Ce163cb61%7Cb9245804a283415ecb4c641f0cf1f882%7C1601372106\"></script></body></html>'\n",
    "        with open(METRICS_SAVE_PATH + repo_name + \"/\" + self.view_name + \".html\", 'w') as outfile:\n",
    "            outfile.write(content)\n",
    "    \n",
    "    def plaintext_save(self, repo_name):\n",
    "        node_list = list(self.g.nodes)\n",
    "        node2index = dict(zip(node_list, range(len(node_list))))\n",
    "        content = \";\".join(node_list) + \"\\n\" + \";\".join([str(node2index[a]) + \",\" + str(node2index[b]) + \",\" + str(d[\"weight\"]) for a, b, d in self.g.edges(data=True)])\n",
    "        os.makedirs(EXPORT_SAVE_PATH + repo_name, exist_ok=True)\n",
    "        with open(EXPORT_SAVE_PATH + repo_name + \"/\" + self.view_name + \".graph.txt\", \"w\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    \n",
    "    def print_most_linked_nodes(self, amount = 10):\n",
    "        print(\"Most linked nodes:\")\n",
    "        debug_list = sorted(list(self.g.edges.data()), key = lambda e: -e[2][\"weight\"])\n",
    "        print([str(info[\"weight\"]) + \": \" + a + \" <> \" + b for a, b, info in debug_list[0:amount]])\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        # https://networkx.github.io/documentation/latest/tutorial.html#analyzing-graphs\n",
    "        node_count = len(self.g.nodes)\n",
    "        edge_count = len(self.g.edges)\n",
    "        cc = sorted(list(nx.connected_components(self.g)), key= lambda e: -len(e))\n",
    "        print(\"WeightGraph statistics: \"\n",
    "              + str(node_count) + \" nodes, \"\n",
    "              + str(edge_count) + \" edges, \"\n",
    "              + str(len(cc)) + \" connected component(s), with sizes: [\"\n",
    "              + \", \".join([str(len(c)) for c in cc[0:20]])\n",
    "              + \"]\")\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        edge_weights.sort()\n",
    "        node_supports = [self.get_support(n) for n in self.g.nodes]\n",
    "        node_supports.sort()\n",
    "        print(\"Edge weights:\", edge_weights[0:5], \"...\", edge_weights[-5:], \", mean:\", np.array(edge_weights).mean())\n",
    "        print(\"Node support values:\", node_supports[0:5], \"...\", node_supports[-5:], \", mean:\", np.array(node_supports).mean())\n",
    "        \n",
    "    \n",
    "    def show_weight_histogram(self):\n",
    "        # https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        plt.hist(edge_weights, \"auto\", facecolor='b', alpha=0.75)\n",
    "        plt.axvline(np.array(edge_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of edge weights in coupling graph')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        node_weights = [sum([self.g[n][n2][\"weight\"] for n2 in self.g.adj[n]]) for n in self.g.nodes]\n",
    "        plt.hist(node_weights, \"auto\", facecolor='g', alpha=0.75)\n",
    "        plt.axvline(np.array(node_weights).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        # plt.xscale(\"log\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.xlabel('Coupling Strength')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of node weights')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        node_supports = [self.get_support(n) for n in self.g.nodes]\n",
    "        plt.hist(node_supports, \"auto\", facecolor='g', alpha=0.75)\n",
    "        plt.axvline(np.array(node_supports).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.xlabel('Support')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Histogram of node support values')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize(self, use_spring = False, with_labels = True):\n",
    "        # https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
    "        for e in self.g.edges:\n",
    "            self.g[e[0]][e[1]][\"distance\"] = 1. - self.g[e[0]][e[1]][\"weight\"] + 0.000001  # the value must not be exactly zero\n",
    "        \n",
    "        edge_weights = [self.g[e[0]][e[1]][\"weight\"] for e in self.g.edges]\n",
    "        max_weight = max(edge_weights)\n",
    "        mean_weight = np.array(edge_weights).mean()\n",
    "        target_max_weight = min(max_weight, mean_weight * 2)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        VIZ_POW = 1\n",
    "        max_w_fact = (1. / target_max_weight) ** VIZ_POW\n",
    "        \n",
    "        layout = nx.drawing.layout.kamada_kawai_layout(self.g, weight=\"distance\") if use_spring else None\n",
    "        \n",
    "        # nx.draw_kamada_kawai(self.g, alpha=0.2, node_size=100)\n",
    "        # nx.draw(self.g, alpha=0.2, node_size=100)\n",
    "        edge_colors = [(0., 0., 0., min(1., (self.g[a][b][\"weight\"] ** VIZ_POW) * max_w_fact)) for a, b in self.g.edges]\n",
    "        nx.draw(self.g, pos=layout, node_size=50, edge_color=edge_colors, node_color=[(0.121, 0.469, 0.703, 0.2)], with_labels=with_labels)\n",
    "        \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COMMIT_FILES = 50\n",
    "# needs to be separate so that multiprocessing lib can find it\n",
    "def get_commit_diff(commit_hash, repo):\n",
    "    # repo_tree = repo.get_tree()\n",
    "    \n",
    "    def walk_tree_cursor(cursor, prefix, content_bytes, node_handler):\n",
    "        if not cursor.node.is_named:\n",
    "            return\n",
    "        def node_text(node):\n",
    "            return decode(content_bytes[node.start_byte:node.end_byte])\n",
    "            \n",
    "        # cursor.current_field_name() is the role that this node has in its parent\n",
    "        tree_node_names = []  # TODO keep in sync with structural and linguistic view as well as RepoFile class\n",
    "        if cursor.node.type == \"class_declaration\" or cursor.node.type == \"interface_declaration\" or cursor.node.type == \"enum_declaration\":\n",
    "            tree_node_names.append(node_text(cursor.node.child_by_field_name(\"name\")))\n",
    "        elif cursor.node.type == \"field_declaration\":\n",
    "            declarators = [child for child in cursor.node.children if child.type == \"variable_declarator\"]\n",
    "            tree_node_names += [node_text(d.child_by_field_name(\"name\")) for d in declarators]\n",
    "        elif cursor.node.type == \"method_declaration\":\n",
    "            tree_node_names.append(node_text(cursor.node.child_by_field_name(\"name\")))\n",
    "        elif cursor.node.type == \"constructor_declaration\":\n",
    "            tree_node_names.append(\"constructor\")\n",
    "\n",
    "        for tree_node_name in tree_node_names:\n",
    "            node_handler(prefix + \"/\" + tree_node_name, cursor.node)\n",
    "        if len(tree_node_names) > 0:\n",
    "            prefix = prefix + \"/\" + tree_node_names[0]\n",
    "\n",
    "        if cursor.goto_first_child():\n",
    "            walk_tree_cursor(cursor, prefix, content_bytes, node_handler)\n",
    "            while cursor.goto_next_sibling():\n",
    "                walk_tree_cursor(cursor, prefix, content_bytes, node_handler)\n",
    "            cursor.goto_parent()\n",
    "    \n",
    "    def walk_tree(tree, content_bytes, base_path) -> RepoTree:\n",
    "        \"\"\" node_handler gets the current logic-path and node for each ast node\"\"\"\n",
    "        try:\n",
    "            found_nodes = RepoTree(None, \"\")\n",
    "            def handle(logic_path, ts_node):\n",
    "                found_nodes.register(logic_path, ts_node)\n",
    "            walk_tree_cursor(tree.walk(), base_path, content_bytes, handle)\n",
    "            return found_nodes\n",
    "        except Exception as e:\n",
    "            print(\"Failed to parse file:\", base_path, \"Error:\", e)\n",
    "            pdb.set_trace()\n",
    "            return None\n",
    "    \n",
    "    error_query = JA_LANGUAGE.query(\"(ERROR) @err\")\n",
    "    def _has_error(tree) -> List[str]:\n",
    "        errors = error_query.captures(tree.root_node)\n",
    "        return len(errors) > 1\n",
    "    \n",
    "    def blob_diff(diff) -> List[str]:\n",
    "        # pdb.set_trace()\n",
    "        if diff.a_blob is None:\n",
    "            return [diff.b_path] # newly created\n",
    "        elif diff.b_blob is None:\n",
    "            return [diff.a_path] # deleted\n",
    "        path = diff.a_path\n",
    "        # if not repo_tree.has_node(path):\n",
    "        #     return []  # ignore changed files that are not part of the interesting project structure\n",
    "        if not path.endswith(\".\" + repo.type_extension()):\n",
    "            return [path]\n",
    "        a_content = diff.a_blob.data_stream.read()\n",
    "        if should_skip_file(a_content):\n",
    "            return []\n",
    "        b_content = diff.b_blob.data_stream.read()\n",
    "        if should_skip_file(b_content):\n",
    "            return []\n",
    "        a_tree = java_parser.parse(a_content)\n",
    "        b_tree = java_parser.parse(b_content)\n",
    "        if _has_error(a_tree) or _has_error(b_tree):\n",
    "            return [path] # I guess just the file changed, no more details available\n",
    "        a_repo_tree = walk_tree(a_tree, a_content, path)\n",
    "        if a_repo_tree is None:\n",
    "            return [path]\n",
    "        b_repo_tree = walk_tree(b_tree, b_content, path)\n",
    "        if b_repo_tree is None:\n",
    "            return [path]\n",
    "        return a_repo_tree.calculate_diff_to(b_repo_tree, a_content, b_content)\n",
    "    \n",
    "    c1 = repo.get_commit(commit_hash)\n",
    "    if len(c1.parents) == 1:\n",
    "        c2 = c1.parents[0]\n",
    "        diff = c1.diff(c2)\n",
    "        if len(diff) > MAX_COMMIT_FILES or len(diff) <= 1:  # this is duplicated here for performance\n",
    "            return None\n",
    "        diffs = [result for d in diff for result in blob_diff(d)]  #  if repo_tree.has_node(result)\n",
    "        # print(\"Diff: \" + str(len(diff)) + \" / \" + str(len(diffs)) + \" changes\")\n",
    "    elif len(c1.parents) == 2:\n",
    "        return None  # TODO how to do sub-file diffs for merge commits?\n",
    "        #c2 = c1.parents[0]\n",
    "        #diff_1 = c1.diff(c2)\n",
    "        #c3 = c1.parents[1]\n",
    "        #diff_2 = c1.diff(c3)\n",
    "\n",
    "        #diffs_1 = [ d.a_path for d in diff_1 ]\n",
    "        #diffs_2 = [ d.a_path for d in diff_2 ]\n",
    "        #diffs = list(set(diffs_1).intersection(set(diffs_2)))\n",
    "    else:\n",
    "        return None\n",
    "    if len(diffs) > MAX_COMMIT_FILES or len(diffs) <= 1:\n",
    "        return None\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsGeneration:\n",
    "    # ascii art: http://patorjk.com/software/taag/#p=display&f=Soft&t=STRUCTURAL%0A.%0ALINGUISTIC%0A.%0AEVOLUTIONARY%0A.%0ADYNAMIC\n",
    "    def __init__(self, repo):\n",
    "        self.repo = repo\n",
    "        \n",
    "    def calculate_evolutionary_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",------.,--.   ,--.,-----. ,--.   ,--. ,--.,--------.,--. ,-----. ,--.  ,--.  ,---.  ,------.,--.   ,--. \n",
    "|  .---' \\  `.'  /'  .-.  '|  |   |  | |  |'--.  .--'|  |'  .-.  '|  ,'.|  | /  O  \\ |  .--. '\\  `.'  /  \n",
    "|  `--,   \\     / |  | |  ||  |   |  | |  |   |  |   |  ||  | |  ||  |' '  ||  .-.  ||  '--'.' '.    /   \n",
    "|  `---.   \\   /  '  '-'  '|  '--.'  '-'  '   |  |   |  |'  '-'  '|  | `   ||  | |  ||  |\\  \\    |  |    \n",
    "`------'    `-'    `-----' `-----' `-----'    `--'   `--' `-----' `--'  `--'`--' `--'`--' '--'   `--'    \n",
    "        \"\"\"\n",
    "        # MAX_COMMIT_FILES = 50  # Ignore too large commits. (constant moved)\n",
    "        PARALLEL_THREADS = 45  # more seems to make it worse again?\n",
    "        PARALLEL_BATCH_SIZE = 50  # the size of packets delivered to worker processes\n",
    "        \n",
    "        coupling_graph = WeightGraph(\"evolutionary\")\n",
    "        \n",
    "        def processDiffs(diffs):\n",
    "            score = 2 / len(diffs)\n",
    "            diffs = [d for d in diffs if self.repo.get_tree().has_node(d)]\n",
    "            for f1, f2 in all_pairs(diffs):\n",
    "                coupling_graph.add(f1, f2, score)\n",
    "            for node in diffs:\n",
    "                coupling_graph.add_support(node, 1)\n",
    "        \n",
    "        print(\"Discovering commits...\")\n",
    "        all_commits = list(self.repo.get_all_commits())\n",
    "        shuffle(all_commits)\n",
    "        print(\"Done!\")\n",
    "        r.get_tree()\n",
    "        print(\"Commits to analyze: \" + str(len(all_commits)))\n",
    "\n",
    "        with Pool(processes=PARALLEL_THREADS) as pool:\n",
    "            bar = log_progress(total=len(all_commits), desc=\"Analyzing commits\", smoothing=0.1)\n",
    "            diffs = pool.imap_unordered(partial(get_commit_diff, repo=self.repo), all_commits, PARALLEL_BATCH_SIZE)\n",
    "            # single-threaded alternative for debugging:\n",
    "            #diffs = []\n",
    "            #for i, t in enumerate(all_commits):\n",
    "            #    diffs.append(partial(get_commit_diff, repo=self.repo)(t))\n",
    "            #    print(str(i / float(len(all_commits)) * 100.) + \" % done.\")\n",
    "\n",
    "            for i, elem in enumerate(diffs):\n",
    "                if elem is not None:\n",
    "                    processDiffs(elem)\n",
    "                    # print(str(i / float(len(all_commits)) * 100.) + \" % done.\")\n",
    "                bar.update()\n",
    "        \n",
    "            bar.close()\n",
    "        coupling_graph.cutoff_edges(0.005)\n",
    "        return coupling_graph\n",
    "    \n",
    "    \n",
    "    def calculate_structural_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    " ,---. ,--------.,------. ,--. ,--. ,-----.,--------.,--. ,--.,------.   ,---.  ,--.                     \n",
    "'   .-''--.  .--'|  .--. '|  | |  |'  .--./'--.  .--'|  | |  ||  .--. ' /  O  \\ |  |                     \n",
    "`.  `-.   |  |   |  '--'.'|  | |  ||  |       |  |   |  | |  ||  '--'.'|  .-.  ||  |                     \n",
    ".-'    |  |  |   |  |\\  \\ '  '-'  ''  '--'\\   |  |   '  '-'  '|  |\\  \\ |  | |  ||  '--.                  \n",
    "`-----'   `--'   `--' '--' `-----'  `-----'   `--'    `-----' `--' '--'`--' `--'`-----'   \n",
    "        \"\"\"\n",
    "        STRENGTH_FILE_IMPORT = 1\n",
    "        STRENGTH_MEMBER_CLASS = 1\n",
    "        STRENGTH_METHOD_RETURN_CLASS = 1\n",
    "        STRENGTH_METHOD_PARAM_CLASS = 1\n",
    "        STRENGTH_FIELD_USAGE = 1\n",
    "        \n",
    "        \n",
    "        builtin_types = set(['void', 'String', 'byte', 'short', 'int', 'long', 'float', 'double', 'boolean', 'char', 'Byte', 'Short', 'Integer', 'Long', 'Float', 'Double', 'Boolean', 'Character'])\n",
    "        stl_types = set(['Override', 'ArrayList', 'List', 'LinkedList', 'Map', 'HashMap', 'Object'])\n",
    "        ignored_types = builtin_types.union(stl_types)\n",
    "        \n",
    "        coupling_graph = WeightGraph(\"structural\")\n",
    "\n",
    "        error_query = JA_LANGUAGE.query(\"(ERROR) @err\")\n",
    "        package_query = JA_LANGUAGE.query(\"(package_declaration (_) @decl)\")\n",
    "        import_query = JA_LANGUAGE.query(\"(import_declaration (scoped_identifier) @decl)\")\n",
    "        class_query = JA_LANGUAGE.query(\"(class_declaration name: (identifier) @decl)\")\n",
    "        assignment_query = JA_LANGUAGE.query(\"(assignment_expression left: (_) @left)\")\n",
    "\n",
    "\n",
    "        def _has_error(file) -> List[str]:\n",
    "            errors = error_query.captures(file.get_tree().root_node)\n",
    "            return len(errors) > 1\n",
    "\n",
    "\n",
    "        def _get_package(file) -> List[str]:\n",
    "            packages = package_query.captures(file.get_tree().root_node)\n",
    "            # assert len(packages) <= 1\n",
    "            if len(packages) > 1:\n",
    "                print(\"Multiple packet declarations found!\")\n",
    "                pdb.set_trace()\n",
    "            if len(packages) == 1:\n",
    "                return file.node_text(packages[0][0]).split(\".\")\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def _get_imports(file) -> List[str]:\n",
    "            imports = import_query.captures(file.get_tree().root_node)\n",
    "            result = []\n",
    "            for import_statement in imports:\n",
    "                import_string = file.node_text(import_statement[0])\n",
    "                if not import_string.startswith(\"java\"):\n",
    "                    result.append(import_string)\n",
    "            return result\n",
    "\n",
    "        def _get_main_class_name(file) -> List[str]:\n",
    "            classes = class_query.captures(file.get_tree().root_node)\n",
    "            if len(classes) >= 1:\n",
    "                return file.node_text(classes[0][0])\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        #######\n",
    "        \n",
    "        pdb.set_trace()\n",
    "        context = StructuralContext(self.repo)\n",
    "\n",
    "        full_class_name_to_id = {}\n",
    "\n",
    "        files = self.repo.get_all_files()\n",
    "        imports = None  # import dict, changed per file\n",
    "        node = None  # node of current file, changed per file\n",
    "        \n",
    "        for file in log_progress(files, desc=\"Building Import Graph\", smoothing=0.1):\n",
    "            if _has_error(file):\n",
    "                continue\n",
    "            class_name = _get_main_class_name(file)\n",
    "            if class_name is not None:\n",
    "                full_class_name = \".\".join(_get_package(file) + [class_name])\n",
    "                full_class_name_to_id[full_class_name] = file.get_path()\n",
    "                class_node = file.get_repo_tree_node().find_node(class_name)\n",
    "                if class_node is not None:\n",
    "                    full_class_name_to_id[full_class_name] = class_node.get_path()\n",
    "                \n",
    "                \n",
    "        def _resolve_type(type_name, file_tree_node: RepoTree, current_path: str) -> str:\n",
    "            \"\"\"find the node name of the node representing this type, if any, or None\"\"\"\n",
    "            # in imports\n",
    "            matching_imports = [i for i in imports if i.endswith(\".\" + type_name) or i == type_name]\n",
    "            if len(matching_imports) > 1:\n",
    "                print(\"Ambiguous import!\")\n",
    "                for import_path in matching_imports[0]:\n",
    "                    if import_path in full_class_name_to_id:\n",
    "                        return full_class_name_to_id[import_path]\n",
    "            elif len(matching_imports) == 1:\n",
    "                import_path = matching_imports[0]\n",
    "                if import_path in full_class_name_to_id:\n",
    "                    return full_class_name_to_id[import_path]\n",
    "            # in same file\n",
    "            classes = node.get_descendants_of_type(\"class\") + node.get_descendants_of_type(\"interface\")\n",
    "            matching_classes = [c for c in classes if c.name == type_name]\n",
    "            if len(matching_classes) > 1:\n",
    "                # find the one hopefully unique match with longest common prefix to current_path\n",
    "                maximum_prefix_length = max([common_prefix_length(current_path, match.get_path()) for match in matching_classes])\n",
    "                max_matches = list([match for match in matching_classes if common_prefix_length(current_path, match.get_path()) == maximum_prefix_length])\n",
    "                if len(max_matches) > 1:\n",
    "                    minimum_step_count = min([len(match.get_path().split(\"/\")) for match in max_matches])\n",
    "                    min_matches = list([match for match in matching_classes if len(match.get_path().split(\"/\")) == minimum_step_count])\n",
    "                    if len(min_matches) > 1:\n",
    "                        print(\"Ambiguous class name!\")\n",
    "                        return None\n",
    "                    return min_matches[0].get_path()\n",
    "                return max_matches[0].get_path()\n",
    "            elif len(matching_classes) == 1:\n",
    "                return matching_classes[0].get_path()\n",
    "            # in same package\n",
    "            type_file_name = type_name + \".java\"\n",
    "            if file_tree_node.parent.has_child(type_file_name):\n",
    "                class_file = file_tree_node.parent.find_node(type_file_name)\n",
    "                class_node = class_file.find_node(type_name)\n",
    "                if class_node is None:\n",
    "                    return class_file.get_path()\n",
    "                else:\n",
    "                    return class_node.get_path()\n",
    "            return None\n",
    "        \n",
    "        def _resolve_compound_type(type_name, file_tree_node: RepoTree, current_path: str) -> str:\n",
    "            result = _resolve_type(type_name, file_tree_node, current_path)\n",
    "            if result is not None or \".\" not in type_name:\n",
    "                return result\n",
    "            parts = type_name.split(\".\")\n",
    "            step = parts[0]\n",
    "            rest = parts[1:].join(\".\")\n",
    "            step_result = _resolve_type(step, file_tree_node, current_path)\n",
    "            if step_result is not None:\n",
    "                return _resolve_compound_type(rest, file_tree_node, current_path)  # TODO those parameters are wrong?\n",
    "            print(\"Cannot resolve compound type\", type_name)\n",
    "            pdb.set_trace()\n",
    "            return None\n",
    "        \n",
    "        unresolvable_types = set()\n",
    "        def _couple_type(type_text, coupling_path, coupling_strength):\n",
    "            data_types = re.split(\"[^\\w.]+\", type_text)\n",
    "            data_types = [dt.strip(\" .\") for dt in data_types]\n",
    "            data_types = [dt for dt in data_types if len(dt) > 0 and not dt[0].isdigit()]\n",
    "            data_types = [dt for dt in data_types if dt not in ignored_types]\n",
    "            for data_type in data_types:\n",
    "                resolved_type = _resolve_compound_type(data_type, node, coupling_path)\n",
    "                if resolved_type is not None:\n",
    "                    # print(\"  Coupling \" + coupling_path + \" to \" + resolved_type)\n",
    "                    coupling_graph.add_and_support(coupling_path, resolved_type, coupling_strength)\n",
    "                else:\n",
    "                    # print(\"  Cannot resolve type: \" + data_type)\n",
    "                    unresolvable_types.add(data_type)\n",
    "        \n",
    "        for file in log_progress(files, desc=\"Extracting connections\", smoothing=0.1):\n",
    "            imports = _get_imports(file)\n",
    "            for i in imports:\n",
    "                if i in full_class_name_to_id:\n",
    "                    pass# print(\"import RESOLVED: \" + i)\n",
    "                    coupling_graph.add_and_support(file.get_path(), full_class_name_to_id[i], STRENGTH_FILE_IMPORT)\n",
    "                else:\n",
    "                    pass # print(\"cannot resolve import: \" + i)\n",
    "        \n",
    "            node = file.get_repo_tree_node()\n",
    "            if node is None:\n",
    "                continue  # TODO why / when does this happen?\n",
    "                \n",
    "            # TODO keep in sync with evolutionary and linguistic view as well as RepoFile class\n",
    "            classes = node.get_descendants_of_type(\"class\") + node.get_descendants_of_type(\"interface\") + node.get_descendants_of_type(\"enum\")\n",
    "            for class_node in classes:\n",
    "                fields = class_node.get_children_of_type(\"field\")\n",
    "                methods = class_node.get_children_of_type(\"method\") + class_node.get_children_of_type(\"constructor\")\n",
    "                # print(\"Class \" + class_node.name + \": \" + str(len(methods)) + \" methods and \" + str(len(fields)) + \" fields\")\n",
    "                \n",
    "                for field in fields:\n",
    "                    type_node = field.ts_node.child_by_field_name(\"type\")\n",
    "                    _couple_type(file.node_text(type_node), field.get_path(), STRENGTH_MEMBER_CLASS)\n",
    "                    \n",
    "                field_by_name = dict((f.name, f.get_path()) for f in fields)\n",
    "                            \n",
    "                for method in methods:\n",
    "                    type_node = method.ts_node.child_by_field_name(\"type\")\n",
    "                    if type_node is not None:  # e.g. constructor\n",
    "                        _couple_type(file.node_text(type_node), method.get_path(), STRENGTH_METHOD_RETURN_CLASS)\n",
    "                    parameters_node = method.ts_node.child_by_field_name(\"parameters\")\n",
    "                    if parameters_node is None:\n",
    "                        parameters_children = list([c for c in method.ts_node.children if c.type == \"formal_parameters\"])\n",
    "                        if len(parameters_children) == 1:\n",
    "                            parameters_node = parameters_children[0]\n",
    "                        else:\n",
    "                            print(\"Can not find parameters for this method!\")\n",
    "                    if parameters_node is not None:\n",
    "                        for parameter in [p for p in parameters_node.children if p.type == 'formal_parameter']:\n",
    "                            type_node = parameter.child_by_field_name(\"type\")\n",
    "                            _couple_type(file.node_text(type_node), method.get_path(), STRENGTH_METHOD_PARAM_CLASS)\n",
    "                            \n",
    "                    # find statements in method to couple structurally based on field access and the call graph:\n",
    "                    def couple_method_to(path, strength):\n",
    "                        if path is None:\n",
    "                            pdb.set_trace(header=\"Cannot couple with nothing!\")\n",
    "                        coupling_graph.add_and_support(path, method.get_path(), strength)\n",
    "                    def get_text(node):\n",
    "                        if node is None:\n",
    "                            pdb.set_trace(header=\"node is None, cannot get text!\")\n",
    "                        return file.node_text(node)\n",
    "                    couple_method_by_content(method, couple_method_to, get_text, context)\n",
    "                    \n",
    "                    assignments = assignment_query.captures(method.ts_node)\n",
    "                    for assignment in assignments:\n",
    "                        assigned_variable = file.node_text(assignment[0])\n",
    "                        if assigned_variable.startswith(\"this.\"):\n",
    "                            assigned_variable = assigned_variable[len(\"this.\"):]\n",
    "                        if assigned_variable[-1] == \"]\" and \"[\" in assigned_variable:  # remove array access brackets\n",
    "                            assigned_variable = assigned_variable[:assigned_variable.index(\"[\")]\n",
    "                        if \".\" in assigned_variable:\n",
    "                            # probably a static field of another class?\n",
    "                            # TODO it might be a nested attribute of a field of the current class?\n",
    "                            base_class = \".\".join(assigned_variable.split(\".\")[:-1])\n",
    "                            resolved_type = _resolve_compound_type(base_class, node, method.get_path())\n",
    "                            if resolved_type is not None:\n",
    "                                resolved_var_path = resolved_type + \"/\" + assigned_variable.split(\".\")[-1]\n",
    "                                if self.repo.get_tree().has_node(resolved_var_path):\n",
    "                                    coupling_graph.add_and_support(resolved_var_path, method.get_path(), STRENGTH_FIELD_USAGE)\n",
    "                                else:\n",
    "                                    print(\"Referenced Class does not have such a field!\", resolved_var_path, method.get_path())\n",
    "                            else:\n",
    "                                print(\"Cannot resolve compound variable:\", assigned_variable)\n",
    "                        else:\n",
    "                            if assigned_variable in field_by_name:\n",
    "                                coupling_graph.add_and_support(field_by_name[assigned_variable], method.get_path(), STRENGTH_FIELD_USAGE)\n",
    "                            else:\n",
    "                                print(\"Cannot resolve variable:\", assigned_variable)\n",
    "                            \n",
    "                    \n",
    "                        \n",
    "        print(\"Unresolved types:\", len(unresolvable_types), \" | examples:\")\n",
    "        print(list(unresolvable_types)[0:10], \"...\" if len(unresolvable_types) > 10 else \"\")\n",
    "\n",
    "        return coupling_graph\n",
    "    \n",
    "    \n",
    "    def calculate_linguistic_connections(self) -> WeightGraph:\n",
    "        \"\"\"\n",
    ",--.   ,--.,--.  ,--. ,----.   ,--. ,--.,--. ,---. ,--------.,--. ,-----.                                \n",
    "|  |   |  ||  ,'.|  |'  .-./   |  | |  ||  |'   .-''--.  .--'|  |'  .--./                                \n",
    "|  |   |  ||  |' '  ||  | .---.|  | |  ||  |`.  `-.   |  |   |  ||  |                                    \n",
    "|  '--.|  ||  | `   |'  '--'  |'  '-'  '|  |.-'    |  |  |   |  |'  '--'\\                                \n",
    "`-----'`--'`--'  `--' `------'  `-----' `--'`-----'   `--'   `--' `-----'              \n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        # TODO: https://pypi.org/project/biterm/\n",
    "        #\n",
    "        \n",
    "        # constants\n",
    "        MIN_WORD_LENGTH = 2\n",
    "        MAX_WORD_LENGTH = 50\n",
    "        MIN_WORD_USAGES = 2  # any word used less often will be ignored\n",
    "        MAX_DF = 0.95  # any terms that appear in a bigger proportion of the documents than this will be ignored (corpus-specific stop-words)\n",
    "        MAX_FEATURES = 1500  # the size of the LDA thesaurus - amount of words to consider for topic learning\n",
    "        TOPIC_COUNT = 40 # 40  # 100 according to paper\n",
    "        BTM_ITERATIONS = 100  # 100 according to docs?\n",
    "        # LDA_PASSES = 200  # how often to go through the corpus\n",
    "        LDA_RANDOM_SEED = 42\n",
    "        DOCUMENT_SIMILARITY_EXP = 8 # higher = lower equality values, lower = equality values are all closer to 1\n",
    "        DOCUMENT_SIMILARITY_CUTOFF = 0.1  # in range [0 .. 1]: everything below this is dropped\n",
    "        \n",
    "        \n",
    "        # keywords from python, TS and Java\n",
    "        custom_stop_words = [\"abstract\", \"and\", \"any\", \"as\", \"assert\", \"async\", \"await\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\", \"class\", \"const\", \"constructor\", \"continue\", \"debugger\", \"declare\", \"def\", \"default\", \"del\", \"delete\", \"do\", \"double\", \"elif\", \"else\", \"enum\", \"except\", \"export\", \"extends\", \"false\", \"False\", \"final\", \"finally\", \"float\", \"for\", \"from\", \"function\", \"get\", \"global\", \"goto\", \"if\", \"implements\", \"import\", \"in\", \"instanceof\", \"int\", \"interface\", \"is\", \"lambda\", \"let\", \"long\", \"module\", \"new\", \"None\", \"nonlocal\", \"not\", \"null\", \"number\", \"of\", \"or\", \"package\", \"pass\", \"private\", \"protected\", \"public\", \"raise\", \"require\", \"return\", \"set\", \"short\", \"static\", \"strictfp\", \"string\", \"super\", \"switch\", \"symbol\", \"synchronized\", \"this\", \"throw\", \"throws\", \"transient\", \"true\", \"True\", \"try\", \"type\", \"typeof\", \"var\", \"void\", \"volatile\", \"while\", \"with\", \"yield\"]\n",
    "        stop_words = set(list(get_stop_words('en')) + list(stopwords.words('english')) + custom_stop_words)\n",
    "        splitter = r\"(?:[\\W_]+|(?<![A-Z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z]))\"\n",
    "        lemma = WordNetLemmatizer()\n",
    "        printable_characters = set(string.printable)\n",
    "        \n",
    "        def _normalize_word(word):\n",
    "            return lemma.lemmatize(lemma.lemmatize(word.lower(), pos = \"n\"), pos = \"v\")\n",
    "        \n",
    "        def _get_text(content_string):\n",
    "            # https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "            # https://agailloty.rbind.io/en/project/nlp_clean-text/\n",
    "            content_string = ''.join(c for c in content_string if c in printable_characters)\n",
    "            words = re.split(splitter, content_string)\n",
    "            words = [word for word in words if not word in stop_words]\n",
    "            words = [_normalize_word(word) for word in words]\n",
    "            words = [word for word in words if len(word) >= MIN_WORD_LENGTH and len(word) <= MAX_WORD_LENGTH]\n",
    "            words = [word for word in words if not word in stop_words]\n",
    "            return words\n",
    "        \n",
    "        def array_similarity(a, b):\n",
    "            \"\"\"given two arrays of numbers, how equal are they?\"\"\"\n",
    "            return math.pow(1. - distance.cosine(a, b), DOCUMENT_SIMILARITY_EXP)  # TODO check for alternative distance metrics?\n",
    "        \n",
    "        \n",
    "        \n",
    "        # see https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "        freq_dist = FreqDist()\n",
    "        \n",
    "        files = self.repo.get_all_files()\n",
    "        \n",
    "        \n",
    "        node_words = []  # List of (RepoTree-Node,wordList) - tuples\n",
    "        for file in log_progress(files, desc=\"Extracting language corpus\", smoothing=0.1):\n",
    "            node = file.get_repo_tree_node()  # TODO unify with structural view code\n",
    "            if node is None:\n",
    "                continue  # TODO why / when does this happen?\n",
    "            \n",
    "            # TODO keep in sync with evolutionary and structural view as well as RepoFile class\n",
    "            classes = node.get_descendants_of_type(\"class\") + node.get_descendants_of_type(\"interface\") + node.get_descendants_of_type(\"enum\")\n",
    "            for class_node in classes:\n",
    "                fields = class_node.get_children_of_type(\"field\")\n",
    "                methods = class_node.get_children_of_type(\"method\") + class_node.get_children_of_type(\"constructor\")\n",
    "                # print(\"Class \" + class_node.name + \": \" + str(len(methods)) + \" methods and \" + str(len(fields)) + \" fields\")\n",
    "                \n",
    "                for member in fields + methods:\n",
    "                    text = member.get_comment_and_own_text(file)\n",
    "                    # words = list(_get_text(class_node.get_path() + \" \" + text))\n",
    "                    words = list(_get_text(class_node.name + \" \" + text))\n",
    "                    for word in words:\n",
    "                        freq_dist[word] += 1\n",
    "                    # TODO: handle the empty list?!?\n",
    "                    node_words.append((member, words))\n",
    "                    # print(\" \".join(words))\n",
    "            \n",
    "        # random.seed(LDA_RANDOM_SEED)\n",
    "        # random.shuffle(node_words)\n",
    "        \n",
    "        for word in freq_dist:\n",
    "            if freq_dist[word] < MIN_WORD_USAGES:\n",
    "                del freq_dist[word]\n",
    "                \n",
    "        print(\"Amount of documents: \" + str(len(node_words)))\n",
    "        print(\"Total Amount of words: \" + str(sum([len(b) for a, b in node_words])))\n",
    "        print(\"Vocab size: \" + str(len(freq_dist)))\n",
    "        \n",
    "        # https://pypi.org/project/biterm/\n",
    "        print(\"Vectorizing words...\")\n",
    "        texts = [\" \".join(words) for (node, words) in node_words]\n",
    "        vec = CountVectorizer(max_df=MAX_DF, max_features=MAX_FEATURES, stop_words=None)\n",
    "        X = vec.fit_transform(texts).toarray()\n",
    "        vocab = np.array(vec.get_feature_names())\n",
    "        \n",
    "        print(\"Instantiating BTM...\")\n",
    "        btm = oBTM(num_topics=TOPIC_COUNT, V=vocab)\n",
    "        biterms = vec_to_biterms(X)\n",
    "        print(\"Total Amount of biterms: \" + str(sum([len(x) for x in biterms])))\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        btm.fit(biterms, iterations=BTM_ITERATIONS)\n",
    "        topics = btm.transform(biterms)\n",
    "        \n",
    "        print(\"Generating topic output...\")\n",
    "        words_to_show_per_topic = 10\n",
    "        topic_summuary(btm.phi_wz.T, X, vocab, words_to_show_per_topic)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"Generating coupling graph...\")\n",
    "        # debug_list = [] # (sim, f1, f2)\n",
    "        coupling_graph = WeightGraph(\"linguistic\")\n",
    "        for n1 in log_progress(range(len(node_words)), desc=\"Generating coupling graph\", smoothing=0.1):\n",
    "            for n2 in range(len(node_words)):\n",
    "                if n1 >= n2:\n",
    "                    continue\n",
    "                t1 = topics[n1]\n",
    "                t2 = topics[n2]\n",
    "                if np.isnan(t1[0]) or np.isnan(t2[0]):\n",
    "                    continue  # TODO filter out those earlier (happen when a btm-document is empty)\n",
    "                similarity = array_similarity(t1, t2)\n",
    "                coupling_graph.add(node_words[n1][0].get_path(), node_words[n2][0].get_path(), similarity)\n",
    "                # debug_list.append((similarity, f1, f2))\n",
    "        \n",
    "        for node, words in log_progress(node_words, desc=\"Generating coupling graph step 2\", smoothing=0.1):\n",
    "            coupling_graph.add_support(node.get_path(), len(words))\n",
    "                \n",
    "        print(\"Trimming graph...\")\n",
    "        coupling_graph.cutoff_edges(DOCUMENT_SIMILARITY_CUTOFF)\n",
    "                \n",
    "        \n",
    "        # print(\"Most similar files:\")\n",
    "        # debug_list = sorted(debug_list, key = lambda x: -x[0])\n",
    "        # print([str(sim) + \": \" + files[f1].get_path() + \" <> \" + files[f2].get_path() for sim, f1, f2 in debug_list[0:10]])\n",
    "        \n",
    "        # print(\"Most dissimilar files:\")\n",
    "        # debug_list = sorted(debug_list, key = lambda x: x[0])\n",
    "        # print([str(sim) + \": \" + files[f1].get_path() + \" <> \" + files[f2].get_path() for sim, f1, f2 in debug_list[0:10]])\n",
    "        \n",
    "        return coupling_graph\n",
    "            \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricManager:\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear(repo, name):\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            os.remove(WeightGraph.pickle_path(repo.name, name))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get(repo, name) -> WeightGraph:\n",
    "        if MetricManager._data_present(repo.name, name):\n",
    "            print(\"Using precalculated \" + name + \" values\")\n",
    "            return WeightGraph.load(repo.name, name)\n",
    "        print(\"No precalculated \" + name + \" values found, starting calculations...\")\n",
    "        graph = getattr(MetricsGeneration(repo), \"calculate_\" + name + \"_connections\")()\n",
    "        graph.cleanup()\n",
    "        print(\"Calculated \" + name + \" values, saving them now...\")\n",
    "        graph.save(repo.name)\n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def _data_present(repo_name, name):\n",
    "        return os.path.isfile(WeightGraph.pickle_path(repo_name, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
