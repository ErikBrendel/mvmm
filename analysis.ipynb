{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "%matplotlib inline\n",
    "%run LocalRepo.ipynb\n",
    "%run repos.ipynb\n",
    "%run parsing.ipynb\n",
    "%run metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisGraph:\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        self.children = {}\n",
    "        for n in self.g.g.nodes:\n",
    "            while n is not None:\n",
    "                p = self.get_parent(n)\n",
    "                self.children.setdefault(p, set()).add(n)\n",
    "                n = p\n",
    "        self.total_relative_coupling_cache = {}\n",
    "        self.median_maximum_support_cache = None\n",
    "        \n",
    "    def get_node_set(self):\n",
    "        return set(self.g.g.nodes)\n",
    "        \n",
    "    def get_children(self, node):\n",
    "        if node in self.children:\n",
    "            return self.children[node]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    def get_parent(self, node):\n",
    "        if len(node) <= 1:\n",
    "            return None\n",
    "        return \"/\".join(node.split(\"/\")[:-1])\n",
    "    \n",
    "    def get_directly_coupled(self, node):\n",
    "        if node in self.g.g:\n",
    "            return [n for n in self.g.g[node]]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def get_siblings(self, node):\n",
    "        \"\"\"Return all the other children of this.parent (excluding this)\"\"\"\n",
    "        parent = self.get_parent(node)\n",
    "        if parent is None:\n",
    "            return []\n",
    "        return [c for c in self.get_children(parent) if c != node]\n",
    "    \n",
    "    def get_coupling_candidates(self, node, add_predecessors = False):\n",
    "        \"\"\"Return all the nodes with which the given one could have a non-zero relative coupling.\n",
    "         * := The direct coupling nodes of given+descendants, and all their predecessors\"\"\"\n",
    "        this_and_descendants = self.get_self_and_descendants(node)\n",
    "        \n",
    "        direct_coupling_candidates = []\n",
    "        for n in this_and_descendants:\n",
    "            for n2 in self.get_directly_coupled(n):\n",
    "                direct_coupling_candidates.append(n2)\n",
    "        \n",
    "        result = set()\n",
    "        result.add(\"\")  # root node, added to stop the predecessor iteration\n",
    "        for other in direct_coupling_candidates:\n",
    "            while result.add(other): # while not present yet\n",
    "                if not add_predecessors:\n",
    "                    break\n",
    "                other = self.get_parent(other)\n",
    "        result.remove(\"\")  # root node - removed, since not very interesting\n",
    "        return result\n",
    "    \n",
    "        \n",
    "    def get_self_and_descendants(self, node):\n",
    "        \"\"\"return the given node and all its descendants as a list\"\"\"\n",
    "        result = []\n",
    "        self.get_self_and_descendants_rec(node, result)\n",
    "        return result\n",
    "    \n",
    "    def get_self_and_descendants_rec(self, node, result):\n",
    "        \"\"\"add the given node and all its descendants into the provided list\"\"\"\n",
    "        result.append(node)\n",
    "        for child in self.get_children(node):\n",
    "            self.get_self_and_descendants_rec(child, result)\n",
    "            \n",
    "    \n",
    "    def get_direct_coupling(self, a, b):\n",
    "        \"\"\"direct coupling between a and b\"\"\"\n",
    "        return self.g.get(a, b)\n",
    "    \n",
    "    def get_direct_multi_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a and all b\"\"\"\n",
    "        return sum([self.get_direct_coupling(a, b) for b in others])\n",
    "    \n",
    "    def get_relative_coupling(self, a, b):\n",
    "        \"\"\"sum of direct coupling between a+descendants and b+descendants\"\"\"\n",
    "        others = self.get_self_and_descendants(b)\n",
    "        return self.get_relative_multi_direct_coupling(a, others)\n",
    "    \n",
    "    def get_relative_multi_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a+descendants and others+descendants\"\"\"\n",
    "        direct_others = []\n",
    "        for other in others:\n",
    "            self.get_self_and_descendants_rec(other, direct_others)\n",
    "        return self.get_relative_multi_direct_coupling(a, direct_others)\n",
    "    \n",
    "    def get_relative_multi_direct_coupling(self, a, others):\n",
    "        \"\"\"sum of direct coupling between a+descendants and others\"\"\"\n",
    "        if len(others) == 0:\n",
    "            return 0\n",
    "        result = self.get_direct_multi_coupling(a, others)\n",
    "        for child in self.get_children(a):\n",
    "            result += self.get_relative_multi_direct_coupling(child, others)\n",
    "        return result\n",
    "    \n",
    "    def get_total_relative_coupling(self, a):\n",
    "        \"\"\"the sum of direct couplings that a has, cached\"\"\"\n",
    "        if a in self.total_relative_coupling_cache:\n",
    "            return self.total_relative_coupling_cache[a]\n",
    "        \n",
    "        a_candidates = self.get_coupling_candidates(a, add_predecessors = False)\n",
    "        total_coupling = self.get_relative_multi_direct_coupling(a, a_candidates)\n",
    "        self.total_relative_coupling_cache[a] = total_coupling\n",
    "        return total_coupling\n",
    "    \n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        \"\"\"relative coupling between a and b, normalized by the sum of couplings that a has, in range [0, 1]\"\"\"\n",
    "        if a not in self.g.g or b not in self.g.g:\n",
    "            return 0\n",
    "        target_coupling = self.get_relative_coupling(a, b)\n",
    "        if target_coupling == 0:\n",
    "            return 0\n",
    "        total_coupling = self.get_total_relative_coupling(a)\n",
    "        return target_coupling / total_coupling\n",
    "    \n",
    "    \n",
    "    def get_normalized_support(self, node):\n",
    "        \"\"\"\n",
    "        on a scale of [0, 1], how much support do we have for coupling values with that node?\n",
    "        This should depend on how much data (including children) we have for this node, relative to how much data is normal in this graph.\n",
    "        It should also be outlier-stable, so that having median-much data maybe results in a support score of 0.5?\n",
    "        \"\"\"\n",
    "        abs_supp = self.get_absolute_support(node)\n",
    "        median, maximum = self.get_absolute_support_median_and_max()\n",
    "        if abs_supp <= median:\n",
    "            return 0.5 * abs_supp / median\n",
    "        else:\n",
    "            return 0.5 + (0.5 * (abs_supp - median) / (maximum - median))\n",
    "    \n",
    "    def get_absolute_support(self, node):\n",
    "        result = self.get_absolute_self_support(node)\n",
    "        for child in self.get_children(node):\n",
    "            result += self.get_absolute_support(child)\n",
    "        return result\n",
    "        \n",
    "    def get_absolute_self_support(self, node):\n",
    "        return self.g.get_support(node)\n",
    "    \n",
    "    def get_absolute_support_median_and_max(self):\n",
    "        if self.median_maximum_support_cache is None:\n",
    "            supports = list([self.get_absolute_support(node) for node in self.g.g.nodes])\n",
    "            median = np.median(supports)\n",
    "            maximum = max(supports)\n",
    "            self.median_maximum_support_cache = (median, maximum)\n",
    "        return self.median_maximum_support_cache\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public so other code can use it\n",
    "def path_module_distance(a, b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    steps_a = a.split(\"/\")\n",
    "    steps_b = b.split(\"/\")\n",
    "    min_len = min(len(steps_a), len(steps_b))\n",
    "    for i in range(min_len):\n",
    "        if steps_a[i] == steps_b[i]:\n",
    "            continue\n",
    "        # unequal: calc distance\n",
    "        return len(steps_a) + len(steps_b) - (i * 2)\n",
    "    return len(steps_a) + len(steps_b) - (min_len * 2)\n",
    "\n",
    "\n",
    "class ModuleDistanceAnalysisGraph:\n",
    "    \n",
    "    def get_node_set(self):\n",
    "        return None\n",
    "    \n",
    "    def get_normalized_support(self, node):\n",
    "        return 1\n",
    "    \n",
    "    def get_normalized_coupling(self, a, b):\n",
    "        dist = path_module_distance(a, b)\n",
    "        base = 1.1  # needs to be bigger than one. Lower values = stronger coupling across bigger distances. Higher values = faster decay of coupling across module distance\n",
    "        return math.pow(base, -dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_graph(r, metric_name):\n",
    "    if metric_name == \"module_distance\":\n",
    "        return ModuleDistanceAnalysisGraph()\n",
    "    else:\n",
    "        return AnalysisGraph(MetricManager.get(r, metric_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(coupling_values, pattern, support_values):\n",
    "    \"\"\"how good does this node-pair fit to the given pattern? Range: [0, 1]\"\"\"\n",
    "    error_sum = 0; values = 0; support = 1\n",
    "    for i, coupling_val in enumerate(coupling_values):\n",
    "        if pattern[i] is not None:\n",
    "            error = abs(pattern[i] - coupling_val)\n",
    "            error_sum += error * error\n",
    "            values += 1\n",
    "            support = min(support, support_values[i])\n",
    "    match_score = 1. - (error_sum / values)\n",
    "    return match_score, support\n",
    "\n",
    "\n",
    "# as seen in:\n",
    "# https://thelaziestprogrammer.com/python/a-multiprocessing-pool-pickle\n",
    "# https://thelaziestprogrammer.com/python/multiprocessing-pool-a-global-solution\n",
    "class StaticStuff:\n",
    "    analysis_graphs = None\n",
    "    target_patterns = None\n",
    "\n",
    "MIN_PATTERN_MATCH = 0.001  # how close the coupling values need to match the pattern to be a result\n",
    "MIN_SUPPORT = 0.001  # how much relative support a result needs to not be discarded\n",
    "def analyze_pair(pair): #, analysis_graphs, target_patterns):\n",
    "    # pdb.set_trace()\n",
    "    _a, _b = pair\n",
    "    if _a.startswith(_b) or _b.startswith(_a):  # ignore nodes that are in a parent-child relation\n",
    "        return None\n",
    "    # for each view: how much support do we have for this node pair (minimum of both node support values)\n",
    "    support_values = [min(supp_a, supp_b) for supp_a, supp_b in zip(*[\n",
    "        [g.get_normalized_support(node) for g in StaticStuff.analysis_graphs] for node in [_a, _b]\n",
    "    ])]\n",
    "    result = [[] for p in StaticStuff.target_patterns]\n",
    "    for a, b in [(_a, _b), (_b, _a)]:\n",
    "        normalized_coupling_values = list([g.get_normalized_coupling(a, b) for g in StaticStuff.analysis_graphs])\n",
    "        for i, pattern in enumerate(StaticStuff.target_patterns):\n",
    "            match_score, support = pattern_match(normalized_coupling_values, pattern, support_values)\n",
    "            if match_score >= MIN_PATTERN_MATCH and support >= MIN_SUPPORT:\n",
    "                result[i].append((a, b, match_score * support, normalized_coupling_values))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_disagreements(repo, views, target_patterns, node_filter_func = None, node_pair_filter_func = None):\n",
    "    \"\"\"\n",
    "    when views are [struct, evo, ling], the pattern [0, 1, None, \"comment\"] searches for nodes that are\n",
    "    strongly coupled evolutionary, loosely coupled structurally, and the language does not matter\n",
    "    \"\"\"\n",
    "    if len(views) <= 1:\n",
    "        return\n",
    "    if not all([len(p) >= len(views) for p in target_patterns]):\n",
    "        print(\"Patterns need at least one element per graph!\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    analysis_graphs = list([get_analysis_graph(repo, g) for g in views])\n",
    "    analysis_graph_nodes = [g.get_node_set() for g in analysis_graphs]\n",
    "    all_nodes = list(set.intersection(*[nodes for nodes in analysis_graph_nodes if nodes is not None]))\n",
    "    all_nodes = [n for n in all_nodes if repo.get_tree().has_node(n)]\n",
    "    print(\"Total node count:\", len(all_nodes))\n",
    "    print(\"Methods:\", sum(repo.get_tree().find_node(path).get_type() == \"method\" for path in all_nodes))\n",
    "    print(\"constructors:\", sum(repo.get_tree().find_node(path).get_type() == \"constructor\" for path in all_nodes))\n",
    "    print(\"fields:\", sum(repo.get_tree().find_node(path).get_type() == \"field\" for path in all_nodes))\n",
    "    print(\"classes:\", sum(repo.get_tree().find_node(path).get_type() == \"class\" for path in all_nodes))\n",
    "    print(\"interfaces:\", sum(repo.get_tree().find_node(path).get_type() == \"interface\" for path in all_nodes))\n",
    "    print(\"enums:\", sum(repo.get_tree().find_node(path).get_type() == \"enum\" for path in all_nodes))\n",
    "    print(\"without type:\", sum(repo.get_tree().find_node(path).get_type() is None for path in all_nodes))\n",
    "    if node_filter_func is not None:\n",
    "        all_nodes = [node for node in all_nodes if node_filter_func(node)]\n",
    "    print(\"all filtered nodes:\", len(all_nodes))\n",
    "    \n",
    "    all_node_pairs = list(all_pairs(all_nodes))\n",
    "    if node_pair_filter_func is not None:\n",
    "        prev_len = len(all_node_pairs)\n",
    "        all_node_pairs = [pair for pair in all_node_pairs if node_pair_filter_func(pair[0], pair[1])]\n",
    "        print(\"Amount of node pairs to check:\", len(all_node_pairs), \"of\", prev_len, \"(\" + str(len(all_node_pairs)/prev_len*100) + \"%)\")\n",
    "        \n",
    "        \n",
    "    print(\"Going parallel...\")\n",
    "    pattern_results = [[] for p in target_patterns]\n",
    "    def handle_results(pattern_results_part):\n",
    "        for i, part in enumerate(pattern_results_part):\n",
    "            pattern_results[i] += part\n",
    "    StaticStuff.analysis_graphs = analysis_graphs\n",
    "    StaticStuff.target_patterns = target_patterns\n",
    "    map_parallel(\n",
    "        all_node_pairs,\n",
    "        analyze_pair, # partial(analyze_pair, analysis_graphs=analysis_graphs, target_patterns=target_patterns),\n",
    "        handle_results,\n",
    "        \"Analyzing edges\",\n",
    "        force_non_parallel=True\n",
    "    )\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    for i, (pattern, results) in enumerate(zip(target_patterns, pattern_results)):\n",
    "        print(\"\\nPattern \" + str(i) + \" (\" + str(pattern) + \"):\")\n",
    "        sorted_results = sorted(results, key = lambda e: -e[2])\n",
    "        print(\"  Amount of disagreements:\", len(sorted_results), \"which is\", str(round(100*len(sorted_results)/(len(all_nodes)*len(all_nodes)), 2))+\"%\", \"of all edges\")\n",
    "\n",
    "\n",
    "        values = [v for a, b, v, x in sorted_results]\n",
    "        plt.hist(values, \"auto\", facecolor='g', alpha=0.75)\n",
    "        plt.axvline(np.array(values).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        # plt.xscale(\"log\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.xlabel('Pattern match')\n",
    "        plt.ylabel('Amount of results')\n",
    "        plt.title('Histogram of pattern matching strengths')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        strong_matches = [(a, b, v) for a, b, v, x in sorted_results if v <= 1]\n",
    "        print(\"  Strong nontrivial disagreements:\", len(strong_matches))\n",
    "        for a, b, v in  strong_matches[:10]:\n",
    "            print(\"  \", a, \"<>\", b, \" - \", v)\n",
    "            \n",
    "        def show_data(multi_sorted_results):\n",
    "            print(len(multi_sorted_results), \"results\")\n",
    "            for r in multi_sorted_results[:10]:\n",
    "                print(r)\n",
    "        def get_i(i):\n",
    "            def getter(d):\n",
    "                return abs(pattern[i] - d[3][i])\n",
    "            return getter\n",
    "        dimensions = [(name, get_i(i)) for i, name in enumerate(views) if pattern[i] is not None]\n",
    "        interactive_multi_sort(results, dimensions, show_data) \n",
    "        \n",
    "    \n",
    "    \n",
    "    # pdb.set_trace()\n",
    "    # TODO trim node set of nodes to those they have in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    r = LocalRepo(\"ErikBrendel/LudumDare\")\n",
    "    analyze_disagreements([MetricManager.get(r, view) for view in [\"structural\", \"evolutionary\"]], [[0, 1], [None, 1], [1, None]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
